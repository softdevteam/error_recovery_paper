\documentclass[acmsmall,small,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{booktabs}
\usepackage{listings}
\usepackage{multicol}
\usepackage{proof}
\usepackage{softdev}
\usepackage{subcaption}
\usepackage{xspace}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

\lstset{
    basicstyle=\tt\scriptsize,
    xleftmargin=0pt,
    numbersep=.8em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

\setcopyright{none}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}

% DOI
%\acmDOI{0000001.0000001}

% Paper history
%\received{February 2007}

\newcommand{\corchueloplus}{\textit{CPCT}$^+$\xspace}
\newcommand{\crarrow}{\rightarrow_{\textrm{\tiny CR}}}
\newcommand{\crarrowstar}{\rightarrow_{\textrm{\tiny CR}}^*}
\newcommand{\lrarrowstar}{\rightarrow_{\textrm{\tiny LR}}^*}
\newcommand{\lrarrow}{\rightarrow_{\textrm{\tiny LR}}}
\newcommand{\mf}{\textit{MF}\xspace}

\include{experimentstats}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title[Fast Multiple Repair Error Recovery]{Fast Multiple Repair Error Recovery}

\author{Lukas Diekmann}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}}
\author{Laurence Tratt}
\orcid{0000-0002-5258-3805}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}
}
\thanks{Authors' URLs: %
    L.~Diekmann~\url{http://diekmann.co.uk/},
    L.~Tratt~\url{http://tratt.net/laurie/}.
}


\begin{abstract}
Syntax errors are generally easy to fix for humans, but not for parsers: the
latter often fail to find an effective recovery, leading to a cascading chain of
errors that drown out the original. More advanced recovery techniques
suffer much less from this problem but have seen little practical use because
their typical performance was seen as poor and their worse case unbounded.
In this paper we show that not only can an advanced approach run in acceptable time
-- which we define as spending a maximum of 0.5s in error recovery per file -- but
that we can increase the complexity of the problem to finding \emph{all} minimal
cost recovery options, and still do so within acceptable time. We first extend
an existing algorithm, before introducing a new, faster, alternative \mf. We
validate our algorithms with a corpus of \corpussize real-world
syntactically invalid of Java programs: \mf is able to find recovery options for
\mfsuccessrate of these in acceptable time.
\end{abstract}

\keywords{Parsing, error recovery, programming languages}

\maketitle

\section{Introduction}

Programming is a humbling job, which requires acknowledging that we will make
untold errors in our quest to perfect a program. Most troubling are semantic
errors, where we intended the program to do one thing, but it does another. Less
troubling, but often no less irritating, are syntax errors, which are
(generally minor) deviances from the exacting syntax required by a compiler.
So common are syntax errors that the parsers in modern compilers expect us to make several
in a single input. Rather than stop on the first syntax error encountered, they attempt
to \emph{recover} from it. This allows them to report, and us to fix, all our
syntax errors in one go.

When error recovery works well, it is a useful productivity gain. Unfortunately,
widely used approaches are ad-hoc and simplistic, with
two weaknesses limiting their usefulness: only limited consideration is
given to the context of the error; and the only recoveries attempted are the
skipping of input until a pre-determined synchronisation
token is reached~\cite[p.~3]{degano95comparison} or the insertion of a
single synchronisation token. Inappropriate recoveries tend to cause a
cascade of spurious syntax errors (see
Figure~\ref{fig:javaerror} for an example): programmers quickly learn that
only the position of the first error -- not the reported repair, nor the position of
subsequent errors -- can really be relied upon to be accurate.

\begin{figure}[t]
\begin{minipage}{0.48\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:javaerror:input}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
class C {
  int x y;
}
\end{lstlisting}
\end{minipage}
\\
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{1}
\caption{}
\label{lst:javaerror:mf}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Error at line 2 col 9. Repairs found:
  Delete "y"
  Insert "COMMA"
  Insert "EQ"
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\vspace{-20.5pt}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{-2}
\caption{}
\label{lst:javaerror:javac}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
C.java:2: error: ';' expected
  int x y;
       ^
C.java:2: error: <identifier> expected
  int x y;
         ^
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%&
%\begin{minipage}[t]{0.47\textwidth}
%\vspace{-17pt}
\vspace{-10pt}
\caption{An example of a simple, common Java syntax error
(\subref{lst:javaerror:input}) and the problems traditional error recovery has in
dealing with it. \texttt{javac} (\subref{lst:javaerror:javac}) spots the error
when it encounters `\texttt{y}'. Its error recovery heuristic then
repairs the input by inserting a semicolon before `\texttt{y}' (i.e.~making
the input equivalent to `\texttt{int x; y;}'). This immediately leads to a spurious second parse error,
since `\texttt{y}' on its own is not a valid statement. The \corchueloplus and \mf
recovery algorithms (both produce the output shown in \subref{lst:javaerror:mf}) also spot the
error when it encounters `\texttt{y}', and then use the
Java grammar to find minimal cost repair sequences. These two algorithms find
and report to the user three separate repair sequences: one can delete `\texttt{y}'
entirely (`\texttt{int x;}'), or insert a comma
(`\texttt{int x, y;}'), or insert an equals sign (\texttt{`int x = y;'}). In
this case, all three repair sequences have the same rank, and are thus presented
in an arbitrary order. However, it is important to note that the first repair
sequence is then used to repair the input for subsequent parsing. Presenting all
three minimal cost repair sequences to the user gives a much greater chance that one
matches their original intention.}
\label{fig:javaerror}
%\end{minipage}
%\end{tabular}
\end{figure}

Most of us are so used to this state of affairs that
we assume it to be inevitable. However, there are more advanced algorithms which,
as well as being able to deal with any LR grammar, take into account the full context of the error, and have
several ways of recovering from errors. Probably the earliest such algorithm
is \citet{aho72minimum}, which, upon encountering an error, creates on-the-fly an
alternative (possibly ambiguous) grammar which allows the parser to recover.
The implementation complexity of this algorithm, and the challenge of explaining
to users what has been done, probably explain why it has fallen out of
favour in programming language circles. A simpler family of algorithms, which
trace their roots to \citet{fischer79locally}, instead try to find a single minimal cost
\emph{repair sequence} of token insertions and deletions which allow the parser to
recover. Algorithms in this family are good at recovering from errors and are
easily adapted to give human-friendly feedback. However, they
have seen little practical use because their typical
performance was seen as poor and their worse case unbounded \cite[p.~14]{mckenzie95error}.

In this paper we first test the following hypothesis:

\begin{description}
  \item[H1] Error recovery algorithms from the \citet{fischer79locally} family
    can repair nearly all errors in acceptable time.
\end{description}

We (somewhat arbitrarily) define `acceptable time' as 0.5s for error recovery
per file (i.e.~all errors in a file must be
recovered from within this time) since we think that even the most demanding user
will tolerate that delay in order to get better quality results. We
strongly validate this hypothesis. To a
reasonable extent our access to faster hardware made it more likely that we
can validate this hypothesis than past researchers. However, while existing
algorithms in the \citet{fischer79locally} family aim to find a single
minimal cost repair sequence, we also test a second, stronger, hypothesis:

\begin{description}
  \item[H2] Nearly all minimal cost repair sequences can be found in acceptable time.
\end{description}

We also strongly validate this hypothesis. In other words, we are able to tackle a
harder version of the problem than any previous algorithm, and do so in
acceptable time. This not only leads to higher quality, and more programmer
friendly, error messages (see Figure~\ref{fig:javaerror} for an example), but
allows us to reduce the cascading error problem still further.

We first use one of the more recent algorithms in this family -- that of
\citet{corchuelo02repairing} -- as a base, correcting and extending it
to form an algorithm \corchueloplus (Section~\ref{corchueloplus}). We
then show that an even newer algorithm which promises better performance -- that
of \citet{kimyi10astar} -- has problems which cause it to miss many
minimal cost repair sequences (Section~\ref{kimyi}). However, we are able to use
it as partial inspiration for an entirely new error recovery algorithm \mf
(Section~\ref{mf}), which is approximately \mfratioovercorchuelo faster than
\corchueloplus. We aim for both algorithms to be as simple as possible, so that
they are realistic targets for tool builders: \corchueloplus is somewhat
simpler than \mf, though the latter is still less than 1,000 lines of Rust code.

We then validate \corchueloplus and \mf on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}) -- approximately
15x bigger than the biggest previous experiment \cite[p.~84]{cerecke03phd}. Within the
time budget of 0.5s: \corchueloplus is able to find repairs for \corchuelosuccessrate
of files within this time, with a mean recovery time of \corchuelomeantime; and
\mf is able to find repairs for \mfsuccessrate of files within this time, with
a mean recovery time of \mfmeantime. We believe that this shows that such
approaches are ready for wider usage, either on their own, or as part of a multi-phase
recovery system such as \citet{deJonge12natural}.


\section{Background}

\begin{figure}[t]
\begin{lstlisting}
%start Expr
%%
Expr: Term "+" Expr
    | Term ;

Term: Factor "*" Term
    | Factor ;

Factor: "(" Expr ")"
      | "INT" ;
\end{lstlisting}
\vspace{-10pt}
\caption{An example grammar, its corresponding stategraph, and statetable.
  Each item within a state $[N \colon \alpha \bullet \beta]$ references one
  of the rule $N$'s productions; $\alpha$ and $\beta$ each
represent zero or more symbols; with the \emph{dot} ($\bullet$) representing
  how much of the production must have been matched ($\alpha$) if parsing has
  reached that state, and how much remains ($\beta$).
  \laurie{the stategraph and statetable are (obviously) missing}}
\label{fig:stategraphtable}
\label{fig:exprgrammar}
\end{figure}

We assume a high-level understanding of the mechanics of parsing in this paper,
but in this section we provide a handful of definitions, and a brief refresher
of relevant low-level details, needed to understand the rest of this paper.
Although the parsing tool we created for this paper is written in Rust, we
appreciate that this still an unfamiliar language to most readers: code examples
are therefore given in Python which, we hope, is familiar to most.

Although there are many flavours of parsing, the \citet{fischer79locally} family
of algorithms, including those presented in this paper, are based on LR
parsing~\cite{knuth65lllr}. As well as describing the largest practical set
of unambiguous grammars, LR parsing remains one of the most widely used parsing
approaches due to the ubiquity of Yacc~\cite{johnson75yacc} and its
descendants (which include the Rust parsing tool we created for this paper).
We use Yacc syntax throughout this paper so that
examples can easily be tested in Yacc-compatible parsing tools.

Yacc-like tools take in a Context-Free Grammar (CFG) and produce a parser from
it. The CFG has one or more \emph{rules}; each rule has a name and one or more
\emph{productions} (often called `alternatives'); each production contains one
or more \emph{symbols}; and a symbol is either a \emph{terminal} (i.e.~a token
type such as \texttt{INT}) or a \emph{nonterminal} (i.e.~a reference to another rule in
the grammar). One rule is designated the \emph{start rule}. The resulting parser
takes as input a stream of tokens, each of which has a \emph{type}
(e.g.~\texttt{INT}) and a \emph{value} (e.g.~\texttt{123}).\footnote{In practise, the
system we outline requires a \emph{lexer} which splits string inputs up into
tokens. In the interests of brevity, we assume the existence of a tool such as
Lex which performs this task.} Strictly speaking, parsing is the act of
determining whether a stream of tokens is correct with respect to the underlying
grammar. Since this is rarely useful on its own, Yacc-like tools allow grammars
to specify `semantic actions' which are executed when a production in the grammar is
successfully matched. In this paper, we assume that the semantic actions build
a \emph{parse tree}, ordering the tokens into a tree of nonterminals (nodes
which can have children) and terminals (nodes which cannot have children)
relative to the underlying grammar.

The beauty of LR parsing stems from the way a parser is constructed. The CFG is
first transformed into a \emph{stategraph}, which is a statemachine
where each node contains one or more \emph{items} (describing the valid
parse states at that point) and edges are labelled with terminals or
nonterminals. Since even on a modern machine, a canonical
(i.e.~unmerged) LR stategraph for a real-world grammar takes several seconds to
build, and a surprising amount of memory to store, we use the state merging
algorithm of \citet{pager77practical} to merge together compatible states. The
effect of this is significant, reducing the Java grammar we use later from
\laurie{8000ish} to \laurie{900ish} states. The stategraph is then transformed into a
\emph{statetable} with one row per state. Each row has a possibly empty \emph{action} (shift, reduce,
or accept) for each terminal and a possibly empty \emph{goto state} for each
nonterminal. Figure~\ref{fig:stategraphtable} shows an example grammar, its
stategraph, and statetable.

The statetable allows us to define a simple, efficient, parsing process. We
first define two functions relative to the statetable: \textsf{action}$(s, t)$
returns the action for the state $s$ and token $t$
or \emph{error} if no such action exists; and \textsf{goto}$(s, N)$
returns the goto state for the state $s$ and the nonterminal $N$. We then define
a reduction relation $\lrarrow$ for $(\textit{parsing stack}, \textit{token
list})$ pairs with two reduction rules as shown in Figure~\ref{fig:lrreduction}.
A full LR parse $\lrarrowstar$ repeatedly applies the two $\lrarrow$ reductions
until neither applies, which means that either \textsf{action}$(s_n, i)$ is:
$\textit{accept}$ (i.e.~the input has been fully parsed); or
$\textit{error}$ (i.e.~an error has been detected at position $i$). A
full parse of an empty file takes a starting pair of $([0], [t_0 \ldots t_n, \$])$,
where state $0$ is expected to represent the entry point into the stategraph, $t_0 \ldots t_n$ are
the sequence of input tokens, and $\$$ is the special End-Of-File (EOF) token.

\begin{figure}[t]
\small
\[
\infer[(\textsc{LR Shift})]
      {([s0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_n, s'], [t_1 \ldots t_n])}
      {\textsf{action}(s_n, t_n) = \textit{shift}\ s'}
\]
\vspace{-3pt}
\[
\infer[(\textsc{LR Reduce})]
      {([s0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'], [t_0 \ldots t_n])}
      {(\textsf{action}(s_n, t_n) = \textit{reduce}\ N \colon \alpha)
       \wedge (\textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s')}
\]
\vspace{-15pt}
\caption{Core reduction rules for $\lrarrow$. \textsc{LR Shift}
advances the input by one token and grows the parsing stack, while
\textsc{LR Reduce} unwinds the parsing stack when a production is complete
before moving to a new state.}
\label{fig:lrreduction}
\end{figure}


\section{\corchueloplus}
\label{corchueloplus}

The \citet{fischer79locally} family of error recovery algorithms has many
members --- far too many to cover them all in one paper (see
\citet{cerecke03phd} for a fairly comprehensive summary). We therefore start
with one of the more recent members -- \citet{corchuelo02repairing}. We first
explain the original algorithm
(Section~\ref{corchuelo:orig}), although we use different notation
than the original, fill in several missing details,
and provide a more formal definition of the overall algorithm. We then
make two correctness fixes that ensure that the algorithm can always
find minimal cost repair sequences (Section~\ref{corchuelo:kimyi}). Since the original description
gives few details as to how the algorithm might best be implemented, we then
explain the steps we took to make a performant implementation
(Section~\ref{corchuelo:implementation}). Finally, we show how the algorithm can
be extended to find all minimum cost repair sequences
(Section~\ref{corchuelo:allminimumcost}). It is this final algorithm that we
refer to as \corchueloplus and which forms part of our later evaluation.


\subsection{The original algorithm}
\label{corchuelo:orig}

As with all error recovery algorithms, \citet{corchuelo02repairing} is invoked
when an error is found in main parsing i.e.~when \textsf{action}$(s_n, i) =
\textit{error}$. Intuitively, the algorithm starts at the error state and tries
to find a minimum cost repair sequence consisting of: \textit{insert T}
(`insert a token of type T'), \textit{delete} (`delete the token at the current offset'),
or \textit{shift} (`skip over the token at the current offset without changing it'). The
algorithm completes successfully if it reaches an accept state or shifts
`enough' tokens ($N_\textit{shift}$; set at 3 in \citet{corchuelo02repairing}),
or unsuccessfully if it consumes `too many' tokens ($N_\textit{total}$; set at
10 in \citet{corchuelo02repairing}).

\begin{figure}[tb]
\small
\[
\infer[\textsc{CR Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow (S', [t_0 \ldots t_n], [\textit{insert}~t])}
      {\textsf{action}(s_n, t) \ne error \wedge t \ne \${}
       \wedge ([s_0 \ldots s_n], [t, t_0 \ldots t_n]) \lrarrowstar (S', [t_0 \ldots t_n])}
\]
\vspace{-6pt}
\[
\infer[\textsc{CR Delete}]
      {(S, [t_0 \ldots t_n]) \crarrow (S, [t_1 \ldots t_n], [\textit{delete}])}
      {t_0 \ne \$}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 1}]
      {(S, [t_0 \ldots t_n]) \crarrow ([s_0 \ldots s_n], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
(S, [t_0 \ldots t_n]) \lrarrowstar ([s_0 \ldots s_n], [t_j \ldots t_n])
 \wedge 0 < j \leq N_\textit{shifts}
\\
 j = N_\textit{shifts} \vee
 \textsf{action}(s_n, t_j) \in \{\textit{accept}, \textit{error}\} 
\end{array}}
\]
\vspace{-7pt}
\caption{The repair-creating reduction rules \cite{corchuelo02repairing}.
\textsc{CR Insert} finds all terminals reachable from the current state and
creates insert repairs for them (other than the EOF token `$\$$').
\textsc{CR Delete} creates deletion repairs if user defined input remains.
\textsc{CR Shift 1} parses at most $N_\textit{shift}$ tokens; if it reaches an accept or error
state, or parses exactly $N_\textit{shift}$ tokens, then a shift repair per
token shifted is created.}
\label{fig:corchuelo:reductions}
\vspace*{-10pt}
\end{figure}


\begin{figure}[tb]
\begin{multicols}{2}
\begin{lstlisting}[numbers=left]
def corchueloetal(pstack, toks):
  todo = [[(pstack, toks, [])]]
  cur_cst = 0
  while cur_cst < len(todo):
    if len(todo[cur_cst]) == 0:
      cur_cst += 1
      continue
    n = todo[cur_cst].pop()
    if action(n[0][-1], n[1]) == <!{\textrm{\textit{accept}}}!> \
       or ends_in_N_shifts(n[2]):
      return n
    elif len(n[1]) - len(toks) == N_total:
      continue
    for nbr in all_cr_star(n.0, n.1):
      if len(n[2]) > 0 and n[2][-1] == <!{\textrm{\textit{delete}}}!> \
         and nbr[2][-1] == <!{\textrm{\textit{insert}}}!>:
        continue
      cst = cur_cst + rprs_cst(nbr_rprs)
      for _ in range(len(todo), cst):
        todo.push([])
      todo[cst].append((nbr[0], nbr[1], \
                        n[2] + nbr[2]))
  return None

def rprs_cst(rprs):
  c = 0
  for r in rprs:
    if r == <!{\textrm{\textit{shift}}}!>: continue
    c += 1
  return c

def all_cr_star(pstack, toks):
  # Exhaustively apply the <!{$\crarrowstar$}!> relation to
  # (pstack, toks) and return the resulting
  # list of (pstack, toks, repair) triples.
\end{lstlisting}
\columnbreak
\vspace*{-18pt}
\caption{Our version of the \citet{corchuelo02repairing} algorithm. The main function
\texttt{corchueloetall} takes in a (\emph{parsing stack}, \emph{token list})
pair and returns: a (\emph{parsing stack}, \emph{token list}, \emph{repair
sequence}) triple where \emph{repair sequence} is guaranteed to be a minimal
cost repair sequence; or \texttt{None} if it failed to find a repair sequence.\\[9pt]
%
The algorithm maintains a todo list of lists (which we
model on \citet[p.~25]{cerecke03phd} rather than the suggestion in \citet{corchuelo02repairing}); the first sub-list contains
search nodes of cost 0, the second sub-list contains search nodes of cost 1 and
so on. The todo list is initialised with the error parsing stack, remaining
tokens, and an empty repair sequence (line 2). If there are todo items left, a
lowest cost node $n$ is picked (line 8). If $n$ represents an accept state (line
9) or if the last $N_\textit{shifts}$ repairs are shifts (line 10), then $n$
represents a minimal cost repair sequence and the algorithm terminates
successfully (line 11). If $n$ has already consumed $N_\textit{total}$ tokens,
then it is discarded (lines 12, 13). Otherwise, $n$'s neighbours
are gathered using the $\crarrow$ relation (lines 14, 32--35). To avoid
duplicate repairs, \textit{delete} repairs never follow \textit{insert} repairs
(lines 16--18). Each neighbour has its repairs costed and is then
assigned to the correct todo sub-list (lines 19--23).\\[9pt]
%
The \texttt{rprs\_cst} function returns the cost of a repair sequence. Inserts
and deletes cost 1, shifts 0.}
\label{fig:corchuelo:algorithm}
\end{multicols}
\vspace*{-15pt}
\end{figure}

As with the original, we explain the approach in two parts. First is a new reduction relation
$\crarrow$ which defines when individual repairs are created
(Figure~\ref{fig:corchuelo:reductions}). Second is an algorithm which
determines when to use the $\crarrow$ relation (Figure~\ref{fig:corchuelo:algorithm}).
As well as several changes for clarity, the biggest difference is that
Figure~\ref{fig:corchuelo:algorithm} captures semi-formally what
\citet{corchuelo02repairing} explain in prose (spread amongst
several topics over several pages) what we have captured as an algorithm in
Figure~\ref{fig:corchuelo:algorithm}: perhaps inevitably
we have had to fill in several missing details. For example,
\citet{corchuelo02repairing} do not define what the cost of repairs is: for
simplicities sake, we define the cost of \textit{insert} and \textit{delete} as
1, and \textit{shift} as 0.\footnote{It is trivial to extend this to variable
token costs if desired, and our implementation supports this. However, it is
unclear whether non-uniform token costs are useful in practise
\cite[p.96]{cerecke03phd}.}

\laurie{do we need an example here? probably...}


\subsection{Ensuring minimal cost repair sequences aren't missed}
\label{corchuelo:kimyi}

\textsc{CR Shift 1} has two flaws that mean that it is
unable to generate some minimal cost repair sequences.

\begin{figure}[tb]
\small
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 2}]
      {(S, [t_0 \ldots t_n]) \crarrow ([s_0 \ldots s_n], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
(S, [t_0 \ldots t_n]) \lrarrowstar ([s_0 \ldots s_n], [t_j \ldots t_n])
 \wedge 0 \leq j \leq N_\textit{shifts}
\\
 (j = 0 \wedge S \ne [s_0 \ldots s_n]) \vee j = N_\textit{shifts} \vee
 \textsf{action}(s_n, t_j) \in \{\textit{accept}, \textit{error}\}
\end{array}}
\]

\[
\infer[\textsc{CR Shift 3}]
      {(S, [t_0 \ldots t_n]) \crarrow (S', [t_j \ldots t_n], R)}
      {%
\begin{array}{c}
(S, [t_0 \ldots t_n]) \lrarrowstar (S', [t_j \ldots t_n])
 \wedge 0 \leq j \leq 1
\\
 (j = 0 \wedge S \ne S' \wedge R = [])
  \vee
  (j = 1 \wedge R = [\textit{shift}])
\end{array}
}
\]
\vspace{-10pt}
\caption{\textsc{CR Shift 2} fixes the problem that
\textsc{CR Shift} always forces a shift to occur, which misses repair sequences
whose final repair is a delete: it allows reductions to occur without
a corresponding shift. \textsc{CR Shift 3} fixes the problem that both
\textsc{CR Shift 1} and \textsc{CR Shift 2} generate multiple shift
repairs in one go, skipping `intermediate' repair positions. \textsc{CR
Shift 3} generates at most one shift, exploring all intermediate repair
positions.}
\label{fig:corchuelo:kimyi}
\vspace*{-8pt}
\end{figure}

\begin{figure}

\begin{minipage}[t]{0.4\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.38\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{fig:crshift2:orig}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Delete, Delete
Delete, Shift, Insert "PLUS"
Delete, Shift, Insert "MULT"
\end{lstlisting}
\end{minipage}
\\
\begin{subfigure}{0.02\textwidth}
\vspace{-5.5pt}
\caption{}
\label{fig:crshift2:fix1}
\end{subfigure}
&
\begin{minipage}[t]{0.38\textwidth}
\vspace{-10pt}
\begin{lstlisting}
Delete, Shift, Delete
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%
\begin{minipage}[t]{0.48\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\vspace{-6.25pt}
\caption{}
\label{fig:crshift2:fix2}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-10.25pt}
\begin{lstlisting}
Insert "INT", Shift, Delete
Insert "INT", Shift, Shift, Delete
Insert "INT", Shift, Shift, Insert "PLUS"
Insert "INT", Shift, Shift, Insert "MULT"
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}

\vspace{-10pt}
\caption{Repair sequences for the input `\texttt{+ 2 3}' (where an error is
found at the first token) relative to the grammar from
Figure~\ref{fig:exprgrammar}. (\subref{fig:crshift2:orig}) shows the three repair
sequences that the original \textsc{CR Shift 1} rule is capable of finding.
(\subref{fig:crshift2:fix1}) shows the additional repair sequence that is found by \textsc{CR Shift 2}.
(\subref{fig:crshift2:fix2}) shows the four additional repair sequences that the
\texttt{CR Shift 3} rule is capable of finding.}
\label{fig:crshift2:example}
\end{figure}

First, \textsc{CR Shift 1} requires at least one token to be shifted. However,
after a repair, all that may be required is one or more reductions in order to
reach an \textit{accept} state. \textsc{CR Shift 2} in Figure~\ref{fig:corchuelo:kimyi}
shows the two-phase fix which addresses this problem.
We first change the condition $0 < j \leq N_\textit{shifts}$ to $0 \leq j \leq
N_\textit{shifts}$ (i.e. we don't force the LR parser to consume any tokens).
However, this then opens the possibility of an infinite loop. We avoid this by
saying that, if the input is not advanced, the parsing stack must have changed
($j = 0 \wedge S \ne S'$). Put another way, in either case, we require progress
to be made, even if that progress does not require consuming any input.

Second, \textsc{CR Shift 1} and \textsc{CR Shift 2} generate multiple shifts
at a time. This cause them to miss intermediate
positions from which minimal cost repair sequences may be found
(this problem, and the basis of a fix, derive from \cite[p.~12]{kimyi10astar},
though their suggestion suffers from the problem fixed by \textsc{CR Shift 1}).
The solution is simple: at most one shift can be generated at any one
time, so that intermediate positions are not missed. \textsc{CR Shift 3} in
Figure \ref{fig:corchuelo:kimyi} (as well as incorporating the fix from
\textsc{CR Shift 2}) generates at most one shift repair at a time. Relative to
\textsc{CR Shift 1}, it is simpler, though it also inevitably slows down the
search for repair sequences, as more are tried.

Examples of the additional repair sequences that \textsc{CR Shift 2} and
\textsc{CR Shift 3} can produce are shown in Figure~\ref{fig:crshift2:example}. Although
in this case \textsc{CR Shift 1} produces some minimal cost repair
sequences, in some cases it finds none, while \textsc{CR Shift 2} or \texttt{CR
Shift 3} find some. There is also a subtle difference in the style of repair
sequences found by the various rules as shown in
Figure~\ref{fig:crshift2:example}: \textsc{CR Shift 3} is much more likely to
produce repair sequences that don't require deleting user input.


\subsection{Implementation considerations}
\label{corchuelo:implementation}

The definitions we have given thus far do not obviously lead to
an efficient implementation and \citet{corchuelo02repairing} give few useful
hints. We found that three techniques, of the many we tried, were both
effective at improving performance while being simple to implement.

First, rather than use a general queue data-structure (probably based on a
tree), we use a similar queue data-structure to
\citet[p.~25]{cerecke03phd}. This consists of one sub-list per cost (i.e.~ the
first sub-list contains search nodes
of cost 0, the second sub-list contains search nodes of cost 1 and so on).
Since we always know what cost we are currently investigating,
finding the next todo element requires only a single \texttt{pop}
(line 8 of Figure~\ref{fig:corchuelo:algorithm}). Similarly,
adding elements requires a simple \texttt{append} to the relevant sub-list
(lines 18, 21, 22). This
data-structure works well in our situation because costs never get very
big (getting into double digits is unusual for real-world
grammars); and each neighbour generated from a node with cost $c$ will
have a cost $\geq c$ (i.e.~we never have to search previous cost sub-lists).

Second, the algorithm creates many duplicates as it operates, because the
$\lrarrow$ relation reduces multiple seemingly distinct search nodes to the same
neighbour. We therefore modify the queue data-structure above to be a
list-of-ordered-sets (i.e.~a set which preserves insertion order). This has
near-identical \texttt{append} / \texttt{pop} performance to a normal list, but
allows us to identify duplicates with near-identical performance to a hashmap.
This allows us to only add unique triples to the relevant todo list. While this
clearly doesn't find all duplicates (if an entry $U$ has been removed from the
todo list, then an equivalent new node $U'$ will not be detected as a duplicate), it
finds enough to be a substantial speed-up, and avoids the impractical overhead of
remembering all past nodes. For example, even on the simple
example from Figure~\ref{fig:javaerror}, we observed 54\% nodes being detected as
duplicates (though this figure varies as the search is inherently
non-deterministic), with a commensurate effect on performance.

Third, we do not use lists to represent parsing stacks and repair sequences
as Figure~\ref{fig:corchuelo:algorithm} may suggest. We found
that this representation consumes noticeably more memory, and is slightly less
efficient, than using parent pointer trees (often called `cactuses').
Every node in such a tree has a reference to a single parent (or \texttt{null} for the
root node) but no references to child nodes. Since our implementation is written
in Rust -- a language without garbage collection -- we use simple reference
counting on nodes (i.e.~a parent is only freed when it is not in a todo list and
no children point to it). When the error recovery algorithm starts, it
converts the main parsing stack (a list) into a parent pointer tree; and
repair sequences start as empty parent pointer trees. The $\crarrow$ part
of our implementation thus operates exclusively on parent pointer trees.
To our surprise, this appears to be a natural fit
for modern \texttt{malloc} implementations, which we found struggled when we
used many variable sized objects, and more than makes up for the poorer cache
performance implied by the inevitable scattering of neighbouring nodes
throughout memory. However, it is quite possible that a different representation would be
better for a garbage collected language.


\subsection{Finding all minimum cost repair sequences}
\label{corchuelo:allminimumcost}


\section{\cite{kimyi10astar}}
\label{kimyi}


\section{\mf}
\label{mf}

\subsection{Ambiguity}
\label{disambiguation}


\section{Experiment}
\label{experiment}

\subsection{Methodolgy}

\subsection{Results}
\section{Related work}

Error recovery techniques have been explored for so long that there is not a
definitive reference, or overview of them. However, \citet{degano95comparison}
contains an overall historical analysis and \citet{cerecke03phd} an excellent
overview of many of the approaches which build on \citet{fischer79locally}. Both
must be supplemented with more recent works, such as we have cited in this
paper,

The biggest limitation of error recovery algorithms in the
\citet{fischer79locally} family is that they are local: they find repairs at the
point that an error is discovered, which may be later in the file than the cause
of the error. Thus even when they successfully recover from an error, the repair
sequence reported may be very different from the fix the user considers
appropriate. Perhaps the most common -- and without doubt the most frustrating
-- example of this is missing a \texttt{\}} character within the method of a
Java-like languages. \citet{santos18syntax} present an intriguing new approach
to what has previously been a computationally impractical problem. They use
machine learning to train a system on syntactically correct programs: when a
syntax error is encountered, they use their model to suggest appropriate global
fixes. Although they also use \laurie{the same blackbox data we do} their
experimental methodology is very different: they are looser in that they only
consider errors which can be fixed with by a single token (discarding 42\% of
the data \cite[p.~8]{santos18syntax}) whereas we attempt to fix errors which
span multiple tokens; they are much stricter in that they focus much more on
trying to find exactly the same type of repair as the human user actually
applied themselves. It is thus difficult to directly compare their results to
ours. However, by the high bar they have set themselves, they are able to repair
52\% of single-token errors (i.e.~about 30\% of all possible errors; for
reference, we repair \laurie{XXX\%}). It seems likely that future machine
learning approaches will improve upon this figure, although the size of the
problem space suggests it will be hard to get close to 100\%{}. It seems likely
that an ideal system will mix both deterministic approaches (such as ours) with
probabilistic approaches (such as \citet{santos18syntax}), possibly even with
multiple sub-approaches (such as \citet{deJonge12natural}).

Although one of our paper's aims is to find all minimal cost repair sequences,
it is unclear how best to present them to users leading to questions such as:
should they be simplified? should a subset be presented? and so on. There is a
body of work which has tried to understand how best to structure compiler error
messages (normally in the context of those learning to program). However, the
results are hard to interpret: some studies find that more complex error
messages are not useful \cite{nienaltowski08error}, while others suggest they
are \cite{prather17novices}.

While the programming language world has largely forgotten the approach of
\citet{aho72minimum}, there are a number of successor works, most recently that
of~\citet{rajasekaran16error}. These improve on the time complexity, though none
that we are aware of address the issue of how to present what has been done to
the user.

\cite{mckenzie95error}

\cite{deJonge12natural}

\cite{pottier16reachability}

\cite{gomezrodriguez10error}


\section{Future work}

Our current parsing tool uses the merging algorithm from
\citet{pager77practical}, but this can over-merge states when conflict
resolution is used \cite[p.~3]{denny10ielr}. Since our error recovery approach
is intended to be independent of the merging approach, it should be possible to
use the more sophisticated state merging approach of \cite{denny10ielr} without
problems.

\bibliography{bib}

\end{document}
