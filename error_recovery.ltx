\documentclass[a4paper,UKenglish,cleveref,autoref,thm-restate]{lipics-v2019}
%for anonymising the authors (e.g. for double-blind review), add "anonymous"
%\nolinenumbers %uncomment to disable line numbering
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{multicol}
\usepackage{natbib}
\usepackage[autolanguage]{numprint}
\usepackage{proof}
\usepackage{softdev}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xspace}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\lstset{
    basicstyle=\ttfamily\scriptsize,
    xleftmargin=0pt,
    numbersep=.8em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

% from https://tex.stackexchange.com/questions/264361/skipping-line-numbers-in-lstlisting#264373
\let\origthelstnumber\thelstnumber
\makeatletter
\newcommand*\Suppressnumber{%
  \lst@AddToHook{OnNewLine}{%
    \let\thelstnumber\relax%
     \advance\c@lstnumber-\@ne\relax%
    }%
}

\newcommand*\Reactivatenumber[1]{%
  \setcounter{lstnumber}{\numexpr#1-1\relax}
  \lst@AddToHook{OnNewLine}{%
   \let\thelstnumber\origthelstnumber%
   \refstepcounter{lstnumber}
  }%
}

\newcommand{\cpctplus}{\textrm{\textit{CPCT}}$^+$\xspace}
\newcommand{\cpctplusrev}{\textrm{\textit{CPCT$^+_\textrm{rev}$}}\xspace}
\newcommand{\crarrow}{\rightarrow_{\textrm{\tiny CR}}}
\newcommand{\crarrowstar}{\rightarrow_{\textrm{\tiny CR}}^*}
\newcommand{\kyarrow}{\rightarrow_{\textrm{\tiny KY}}}
\newcommand{\lrarrowstar}{\rightarrow_{\textrm{\tiny LR}}^*}
\newcommand{\lrarrow}{\rightarrow_{\textrm{\tiny LR}}}
\newcommand{\mf}{\textrm{\textit{MF}}\xspace}
\newcommand{\mfarrow}{\rightarrow_{\textrm{\tiny MF}}}
\newcommand{\mfarrowstar}{\rightarrow_{\textrm{\tiny MF}}^*}
\newcommand{\panic}{\textrm{\textit{Panic}}\xspace}

\newcommand{\aho}{Aho and Peterson\xspace}
\newcommand{\corchuelo}{Corchuelo \emph{et al.}\xspace}
\newcommand{\fischer}{Fischer \emph{et al.}\xspace}
\newcommand{\holub}{Holub\xspace}

\include{experimentstats}

\title{Don't Panic! Better, Fewer, Syntax Errors for LR Parsers}

\author{Lukas Diekmann}{Software Development Team, King's College London, United Kingdom \and \url{https://lukasdiekmann.com/}}{lukas.diekmann@gmail.com}{}{}
\author{Laurence Tratt}{Software Development Team, King's College London, United Kingdom \and \url{https://tratt.net/laurie/}}{laurie@tratt.net}{https://orcid.org/0000-0002-5258-3805}{}
\authorrunning{L.\,Diekmann and L.\,Tratt}
\Copyright{Lukas Diekmann and Laurence Tratt}
\keywords{Parsing, error recovery, programming languages}
\ccsdesc{Theory of computation~Parsing}
\ccsdesc{Software and its engineering~Compilers}

% Document starts
\begin{document}

\maketitle

\begin{abstract}
Syntax errors are generally easy to fix for humans, but not for parsers, in general,
and LR parsers, in particular. Traditional `panic mode' error recovery, though
easy to implement and applicable to any grammar,
often leads to a cascading chain of
errors that drown out the original. More advanced error recovery techniques
suffer less from this problem but have seen little practical use because
their typical performance was seen as poor, their worst case unbounded, and the
repairs they reported arbitrary. In this paper we introduce an algorithm
and implementation that addresses these issues. First, we
report the complete set of minimum cost repair sequences for a given location, allowing
programmers to select the one that best fits their intention. Second, on a
corpus of \corpussize real-world syntactically invalid Java programs, we are
able to repair \cpctplussuccessrate of files within
a cut-off of 0.5s. Finally, we use the existence of the complete set of
minimum cost repair sequences to reduce one of the most frustrating consequences of
error reporting: the cascading error problem. Across our corpus, we report
\cpctpluserrorlocs error locations to the user, while the panic
mode algorithm reports \panicerrorlocs error locations: in other words, we
reduce the cascading error problem by well over half.
\end{abstract}

\section{Introduction}

Programming is a humbling job which requires acknowledging that we will make
untold errors in our quest to perfect a program. Most troubling are semantic
errors, where we intended the program to do one thing, but it does another. Less
troubling, but often no less irritating, are syntax errors, which are
(generally minor) deviances from the exacting syntax required by a compiler.
So common are syntax errors that parsers in modern compilers are designed
to cope with us making several: rather than stop on the first syntax error, they attempt
to \emph{recover} from it. This allows them to report, and us to fix, all our
syntax errors in one go.

When error recovery works well, it is a useful productivity gain. Unfortunately,
most current error recovery approaches are simplistic. The most common
grammar-neutral approach to error recovery are those algorithms described as
`panic mode' algorithms (e.g.~\cite[p.~348]{holub90compilerdesign}) which skip
input until the parser finds something it is able to parse. A more
grammar-specific variation of this idea is to skip input until a pre-determined
synchronisation token (e.g. `;' in Java) is
reached~\cite[p.~3]{degano95comparison}, or to try inserting a single
synchronisation token. Such strategies are often unsuccessful,
leading to a
cascade of spurious syntax errors (see
Figure~\ref{fig:javaerror} for an example). Programmers quickly learn that
only the location of the first error in a file -- not the reported repair, nor the position of
subsequent errors -- can be relied upon to be accurate.

\begin{figure}[t]
\begin{minipage}{0.49\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:javaerror:input}
\end{subfigure}
&
\begin{minipage}[t]{0.86\textwidth}
\vspace{-13.5pt}
\begin{lstlisting}[language=Java]
class C {
  int x y;
}
\end{lstlisting}
\end{minipage}
\\
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{1}
\caption{}
\label{lst:javaerror:cpctplus}
\end{subfigure}
&
\begin{minipage}[t]{0.86\textwidth}
\vspace{-13.5pt}
\begin{lstlisting}
Parsing error at line 2 col 9. Repair
sequences found:
  1: Delete y
  2: Insert ,
  3: Insert =
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%
\begin{minipage}{0.49\textwidth}
\vspace{-27pt}
\begin{tabular}{p{0.02\textwidth}p{0.48\textwidth}}
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{-2}
\caption{}
\label{lst:javaerror:javac}
\end{subfigure}
&
\begin{minipage}[t]{0.9\textwidth}
\vspace{-13.5pt}
\begin{lstlisting}
C.java:2: error: ';' expected
  int x y;
       ^
C.java:2: error: <identifier> expected
  int x y;
         ^
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
\vspace{-10pt}
\caption{An example of a simple, common Java syntax error
(\subref{lst:javaerror:input}) and the problems traditional error recovery has in
dealing with it. \texttt{javac} (\subref{lst:javaerror:javac}) spots the error
when it encounters `\texttt{y}'. Its error recovery heuristic then
repairs the input by inserting a semicolon before `\texttt{y}' (i.e.~making
the input equivalent to `\texttt{int x; y;}'). This causes a (spurious) cascading
parsing error, since `\texttt{y}' on its own is not a valid statement. The \cpctplus
error recovery algorithm we introduce in this paper produces
the output shown in (\subref{lst:javaerror:cpctplus}): after spotting an error
when parsing encounters `\texttt{y}', it uses the Java grammar to find the
complete set of minimum cost repair sequences (unlike previous approaches which
non-deterministically find one minimum cost repair sequence). In this case three
repair sequences are reported to the user: one can delete `\texttt{y}'
entirely (`\texttt{int x;}'), or insert a comma
(`\texttt{int x, y;}'), or insert an equals sign (\texttt{`int x = y;'}).}
\label{fig:javaerror}
\end{figure}

A handful of parsers contain hand-written error recovery algorithms
for specific languages.
These generally allow better recovery from errors, but are challenging
to create. For example, the Java error recovery approach in the Eclipse IDE is 5KLoC long,
making it only slightly smaller than a modern version of Berkeley Yacc --- a
complete parsing system! Unsurprisingly, few real-world parsers contain
effective hand-written error recovery algorithms.

Most of us are so used to these trade-offs (cheap generic algorithms and poor
recovery vs.~expensive hand-written algorithms and reasonable recovery) that we
assume them to be inevitable. However, there is a long line of work on
more advanced generic error recovery algorithms. Probably the earliest such algorithm
is \aho~\cite{aho72minimum}, which, upon encountering an error, creates on-the-fly an
alternative (possibly ambiguous) grammar which allows the parser to recover.
This algorithm has fallen out of favour in programming language
circles, probably because of its implementation complexity and the difficulty of
explaining to users what recovery has been used. A simpler family of algorithms, which
trace their roots to \fischer~\cite{fischer79locally}, instead try to find a single minimum cost
\emph{repair sequence} of token insertions and deletions which allow the parser to
recover. Algorithms in this family are much better at recovering from errors
than naive approaches and can communicate the repairs they find in a way that
humans can easily replicate. However, such algorithms
have seen little practical use because their typical
performance is seen as poor and their worst case unbounded \cite[p.~14]{mckenzie95error}.
We add a further complaint to this mix: such approaches only report a single
repair sequence to users. In general -- and especially in syntactically rich
languages -- there are multiple reasonable repair sequences for a given error
location, and the algorithm has no way of knowing which best matches the user's intentions.

In this paper we introduce a new error recovery algorithm in the \fischer
family, \cpctplus. This takes the approach of \corchuelo~\cite{corchuelo02repairing}
as a base, corrects it, and then substantially expands it. \cpctplus is simple
to implement (under 500 lines of Rust code), is able to repair
nearly all errors in reasonable time, and reports the
complete set of minimum cost repair sequences to users.

We validate \cpctplus on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}). \cpctplus is
able to recover \cpctplussuccessrate of files within a 0.5s timeout and does
so while reporting well under
half as many error locations to the user as a traditional panic mode algorithm:
in other words, \cpctplus substantially reduces the cascading error problem. Finally,
we show -- for, as far as we know, the first time -- that advanced error
recovery can be fairly easily added to a Yacc-esque system, allowing users
to concisely make fine-grained decisions about what to do when error
recovery has altered an input (Section~\ref{sec:api}). We believe that this
shows that algorithms such as \cpctplus are ready for wider
usage, either on their own, or as part of a multi-phase recovery system.


\subsection{Defining the problem}

Formally speaking, we first test the following hypothesis:

\begin{description}
  \item[H1] The complete set of minimum cost repair sequences can
    be found in acceptable time.
\end{description}

The only work we are aware of with a similar concept of `acceptable time'
is~\cite{deJonge12natural}, who define it as the total time spent in error
recovery per file, with a threshold of 1s. Since many compilers are able to
fully execute in less time than this, we felt that a tighter threshold is more
appropriate: we use 0.5s since we think that even the most
demanding user will tolerate such a delay. We strongly validate this hypothesis.
Relative to previous approaches, we are the clear beneficiaries of faster, modern
hardware, which undoubtedly makes it easier to validate this hypothesis.
However, it is important to note that we have
stated a much stronger hypothesis than previous approaches: where
they have aimed to find only a single minimum cost repair sequence, we
find the complete set of minimum cost repair sequences, a much more challenging
task.

The complete set of minimum cost repair sequences makes it much more likely that
the programmer will see a repair sequence that best matches their original
intention (see Figure~\ref{fig:javaerror} for an example; Appendix~\ref{app:examples}
contains further examples in Java, Lua, and PHP). It also opens up a
new opportunity for error recovery algorithms. Previous error recovery
algorithms find a single repair sequence, apply that to the input, and then
continue parsing. While that repair sequence may have been a reasonable local
choice, it may cause cascading errors later. Since we have the complete set of
minimum cost repair sequences available, we can select from them a repair sequence
which causes fewer cascading errors. We thus
rank repair sequences by how far they allow parsing to continue successfully
(up to a threshold --- parsing the whole file would, in general, be too costly),
and choose from the subset that gets furthest (note that the time required to do
this is included in the 0.5s timeout). We thus also test a second hypothesis:

\begin{description}
  \item[H2] The cascading error problem can be significantly reduced by ranking
the complete set of minimum cost repair sequences and choosing from those which
allow parsing to continue the furthest.
\end{description}

We also strongly validate this hypothesis. We do this by comparing `normal' \cpctplus
with a simple variant \cpctplusrev which reverses the ranking process, always selecting
from amongst the worst performing minimum cost repair
sequence. \cpctplusrev models the worst case of previous approaches in the
\fischer family, which non-deterministically select a single
minimum cost repair sequence. \cpctplusrev leads to \cpctplusreverrorlocsratioovercpctplus more
errors being reported (i.e.~it substantially worsens the cascading error problem).

This paper is structured as follows. We describe the \corchuelo algorithm
(Section~\ref{corchuelo}), filling in missing details from the original description
and correcting its definition. We then expand the algorithm into \cpctplus
(Section~\ref{corchueloplus}). We then validate \cpctplus on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}). To
emphasise that our algorithms are grammar-neutral, we show examples of
error recovery on different grammars in Appendix~\ref{app:examples}.


\section{Background}

We assume a high-level understanding of the mechanics of parsing in this paper,
but in this section we provide a handful of definitions, and a brief refresher
of relevant low-level details, needed to understand the rest of this paper.
Although the parsing tool we created for this paper is written in Rust, we
appreciate that this is still an unfamiliar language to most readers: algorithms
are therefore given in Python which, we hope, is familiar to most.

Although there are many flavours of parsing, the \fischer family
of error recovery algorithms are designed to be used with LR($k$)
parsers~\cite{knuth65lllr}. LR parsing remains one of the most widely used parsing
approaches due to the ubiquity of Yacc~\cite{johnson75yacc} and its
descendants (which include the Rust parsing tool we created for this paper).
We use Yacc syntax throughout this paper so that
examples can easily be tested in Yacc-compatible parsing tools.

Yacc-like tools take in a Context-Free Grammar (CFG) and produce a parser from
it. The CFG has one or more \emph{rules}; each rule has a name and one or more
\emph{productions} (often called `alternatives'); each production contains one
or more \emph{symbols}; and a symbol references either a \emph{token type}
or a grammar rule. One rule is designated the \emph{start rule}. The resulting parser
takes as input a stream of tokens, each of which has a \emph{type}
(e.g.~\texttt{INT}) and a \emph{value} (e.g.~\texttt{123}).\footnote{In practise, the
system we outline requires a \emph{lexer} which splits string inputs up into
tokens. In the interests of brevity, we assume the existence of a tool such as
Lex which performs this task.} Strictly speaking, parsing is the act of
determining whether a stream of tokens is correct with respect to the underlying
grammar. Since this is rarely useful on its own, Yacc-like tools allow grammars
to specify `semantic actions' which are executed when a production in the grammar is
successfully matched. Except where stated otherwise, we assume that the semantic actions build
a \emph{parse tree}, ordering the tokens into a tree of nonterminal nodes
(which can have children) and terminal nodes (which cannot have children)
relative to the underlying grammar.

\begin{figure}[t]
\includegraphics{examplegrammar}
\vspace{-23pt}
\caption{An example grammar (top left), its corresponding stategraph (right), and statetable
(split into separate action and goto tables; bottom left). Productions in
the grammar are labelled \texttt{(I)} to \texttt{(VI)}. In the stategraph: S($x$)
means `shift to state $x$'; R($x$) means `reduce production $x$ from the
grammar' (e.g.~\textit{action(3, `+')} returns R(IV) which references
the production `\texttt{Term: Factor;}').
  Each item within a state $[N \colon \alpha \bullet \beta]$ references one
  of rule $N$'s productions; $\alpha$ and $\beta$ each
represent zero or more symbols; with the \emph{dot} ($\bullet$) representing
  how much of the production must have been matched ($\alpha$) if parsing has
  reached that state, and how much remains ($\beta$).}
\label{fig:stategraphtable}
\label{fig:exprgrammar}
\end{figure}

The CFG is first transformed into a \emph{stategraph}, a statemachine
where each node contains one or more \emph{items} (describing the valid
parse states at that point) and edges are labelled with terminals or
nonterminals. Since even on a modern machine, a canonical
(i.e.~unmerged) LR stategraph for a real-world grammar takes several seconds to
build, and a surprising amount of memory to store, we use the state merging
algorithm of \cite{pager77practical} to merge together compatible
states.\footnote{Unfortunately \cite{pager77practical} can over-merge states when conflict
resolution is used \cite[p.~3]{denny10ielr} (i.e.~when Yacc uses its
precedence rules to turn an ambiguous input into an unambiguous LR parser).
Since our error recovery approach operates purely on the statetable,
it should work correctly with other merging approaches such as that of~\cite{denny10ielr}.} The
effect of this is significant, reducing the Java grammar we use later from
8908 to 1148 states. The stategraph is then transformed into a
\emph{statetable} with one row per state. Each row has a possibly empty \emph{action} (shift, reduce,
or accept) for each terminal and a possibly empty \emph{goto state} for each
nonterminal. Figure~\ref{fig:stategraphtable} shows an example grammar, its
stategraph, and statetable.

The statetable allows us to define a simple, efficient, parsing process. We
first define two functions relative to the statetable: \textsf{action}$(s, t)$
returns the action for the state $s$ and token $t$
or \emph{error} if no such action exists; and \textsf{goto}$(s, N)$
returns the goto state for the state $s$ and the nonterminal $N$ or \emph{error}
if no such goto state exists. We then define
a reduction relation $\lrarrow$ for $(\textit{parsing stack}, \textit{token
list})$ pairs with two reduction rules as shown in Figure~\ref{fig:lrreduction}.
A full LR parse $\lrarrowstar$ repeatedly applies the two $\lrarrow$ rules
until neither applies, which means that \textsf{action}$(s_n, t_0)$ is either:
$\textit{accept}$ (i.e.~the input has been fully parsed); or
$\textit{error}$ (i.e.~an error has been detected at the terminal $t_0$). A
full parse takes a starting pair of $([0], [t_0 \ldots t_n, \$])$,
where state $0$ is expected to represent the entry point into the stategraph, $t_0 \ldots t_n$
is the sequence of input tokens, and `$\$$' is the special End-Of-File (EOF) token.

\begin{figure}[t]
\centering
\begin{minipage}{0.63\textwidth}
\small
\[
\infer[\textsc{LR Shift}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_n, s'], [t_1 \ldots t_n])}
      {\textsf{action}(s_n, t_0) = \textit{shift}\ s'}
\]
\end{minipage}
\vspace{-0pt}
\begin{minipage}{0.83\textwidth}
\[
\infer[\textsc{LR Reduce}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'], [t_0 \ldots t_n])}
      {(\textsf{action}(s_n, t_0) = \textit{reduce}\ N \colon \alpha)
       \wedge (\textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s')}
\]
\end{minipage}
\vspace{-0pt}
\caption{Reduction rules for $\lrarrow$, which operate on $(\textit{parsing stack},
\textit{token list})$ pairs. \textsc{LR Shift}
advances the input by one token and grows the parsing stack, while
\textsc{LR Reduce} unwinds (`reduces') the parsing stack when a production is
complete before moving to a new (`goto') state.}
\label{fig:lrreduction}
\end{figure}


\section{Panic mode}
\label{sec:panic mode}

Error recovery algorithms are invoked by a parser when it has yet to finish but
there is no apparent way to continue parsing (i.e.~when \textsf{action}$(s_n,
t_0) = \textit{error}$). Error recovery algorithms are thus called with a parsing
stack and a sequence of remaining input (which, for simplicities sake, we represent
as a list of tokens): they can modify either or both of the
parsing stack and the input in their quest to get parsing back on track. The differences
between algorithms are thus in what modifications they can carry out
(e.g.~altering the parse stack; deleting input; inserting input), and how they
carry such modifications out.

The simplest grammar-neutral error recovery algorithms are called `panic mode'
algorithms. The precise origin of this family of algorithms seems lost in time;
there are also more members of this family for LL parsing than there are for
LR parsing. Indeed, for LR parsing, there is only one fundamental way of creating
a grammar-neutral panic mode algorithm: we take our formulation from
\holub~\cite[p.~348]{holub90compilerdesign}.\footnote{Note that step 2 in
\holub causes valid repairs to be missed: while it is
safe to ignore the top element of the parsing stack on the first iteration of
the algorithm, as soon as one token is skipped, one must check all elements of
the parsing stack. Our description simply drops step 2 entirely.} It works by
taking the parsing stack and popping elements to
see if an earlier part of the stack is able to parse the next input symbol. If
no element in the stack is capable of parsing the next input symbol, the input
symbol is skipped, the stack restored, and the process repeated. At worst,
this algorithm guarantees to find a match at the EOF token.
Figure~\ref{fig:holub:algorithm} shows a more formal version of this algorithm.

\begin{figure}[tb]
\begin{adjustbox}{valign=t,minipage=.53\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def holub(pstack, toks):
  while len(toks) > 0:
    npstack = pstack.copy()
    while len(npstack) > 0:
      if action(npstack[-1], toks[0]) != <!{\textrm{\textit{error}}}!>:
        return (npstack, toks)
      npstack.pop()
    del toks[0]
  return None
\end{lstlisting}
\end{adjustbox}
\hspace{5pt}
\begin{adjustbox}{valign=t,minipage=.45\textwidth}
\vspace*{-0pt}
\caption{Our version of the \holub~\cite{holub90compilerdesign} algorithm. This
panic mode algorithm takes in a (\emph{parsing stack}, \emph{token list})
pair and returns: a (\emph{parsing stack}, \emph{token list}) pair if
it managed to recover; or \texttt{None} if it failed to recover.
The algorithm tries to find an element in the stack that has a non-error action
for the next token in the input (lines 4--7). If it fails to find such an
element, the input is advanced by one element (line 8) and the stack restored
(line 3).}
\label{fig:holub:algorithm}
\end{adjustbox}
\vspace{-15pt}
\end{figure}

The advantage of this algorithm is its simplicity and speed. For example,
consider the grammar from Figure~\ref{fig:exprgrammar} and the input `\texttt{2
+ + 3}'. The parser encounters an error on the second `+' token, leaving it with
a parsing stack of [0, 2, 7] and the input `\texttt{+ 3}' remaining. The error
recovery algorithm now starts. It first tries \textsf{action}(7, `+') which
(by definition, since it is the place the parser encountered an error) returns
\textit{error}; it then pops the top element from the parsing stack and tries
\textsf{action}(2, `+'), which returns \textit{shift}. This is enough for the
error algorithm to complete, and parsing resumes with a stack [0, 2].

The fundamental problem with error recovery can be seen from the above example:
the adjustment made to the parsing stack is not one that the user can
replicate. Looked at another way, error recovery is a \emph{Deus ex
machina}: while panic mode managed to recover from the error, the only
general way to report what was done is to show the parsing stack
before and after recovery: this is challenging to interpret for small grammars
like that of Figure~\ref{fig:exprgrammar} and completely impractical for
anything larger. There is an important corollary to this: since the recoveries
made often don't match anything the user could have passed as input, they
are often of poor quality, leading to a cascade of further
parsing errors (as we will see later in Section~\ref{results}).


\section{\corchuelo}
\label{corchuelo}

There have been many attempts to create better LR error recovery algorithms than
panic mode. Most numerous are those error recovery algorithms in what we call the
\fischer family. Indeed, there are far too many
members of this family of algorithms to cover in one paper. We therefore start
with one of the more recent -- \corchuelo~\cite{corchuelo02repairing}. We first
explain the original algorithm (Section~\ref{corchuelo:orig}), although we use
different notation than the original, fill in several missing details, and
provide a more formal definition. We then
make two correctness fixes to ensure that the algorithm always
finds minimum cost repair sequences (Section~\ref{corchuelo:kimyi}). Since the original description
gives few details as to how the algorithm might best be implemented, we then
explain the steps we took to make a performant implementation
(Section~\ref{corchuelo:implementation}).


\subsection{The original algorithm}
\label{corchuelo:orig}

Intuitively, the \corchuelo algorithm starts at the error state and tries
to find a minimum cost repair sequence consisting of: \textit{insert T}
(`insert a token of type T'), \textit{delete} (`delete the token at the current offset'),
or \textit{shift} (`parse the token at the current offset'). The
algorithm completes: successfully if it reaches an accept state or shifts
`enough' tokens ($N_\textit{shifts}$, set at 3 in \corchuelo);
or unsuccessfully if it deletes and inserts `too many' tokens ($N_\textit{total}$, set at 10
in \corchuelo). Repair sequences are reported back to users
with trailing \emph{shift} repairs pruned i.e.~[\emph{insert x, shift y, delete
z, shift a, shift b, shift c}] is reported as [\emph{insert x, shift y, delete
z}].

In order to find repair sequences, the algorithm keeps a queue of
\emph{configurations}, each of which represents a different search state;
configurations are searched for their neighbours until a successful configuration
is found. The cost of a configuration is the cumulative cost of the repairs
in its repair sequence. By definition, a configuration's neighbours have the
same, or greater, cost to it.

\begin{figure}[tb]
\small
\centering
\begin{minipage}{0.9\textwidth}
\[
\infer[\textsc{CR Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_0 \ldots t_n], [\textit{insert}~t])}
      {\textsf{action}(s_n, t) \ne error \wedge t \ne \${}
       \wedge ([s_0 \ldots s_n], [t, t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_0 \ldots t_n])}
\]
\end{minipage}
\vspace{-0pt}
\begin{minipage}{0.7\textwidth}
\[
\infer[\textsc{CR Delete}]
      {([s_0 \ldots s_n], [t_0, t_1 \ldots t_n])
       \crarrow
       ([s_0 \ldots s_n], [t_1 \ldots t_n], [\textit{delete}])}
      {t_0 \ne \$}
\]
\end{minipage}
\vspace{-0pt}
\begin{minipage}{0.75\textwidth}
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 1}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n])
       \crarrow
       ([s'_0 \ldots s'_m], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_j \ldots t_n])
 \wedge 0 < j \leq N_\textit{shifts}
\\
 j = N_\textit{shifts} \vee
 \textsf{action}(s'_m, t_j) \in \{\textit{accept}, \textit{error}\} 
\end{array}}
\]
\end{minipage}
\vspace{-3pt}
\caption{The repair-creating reduction rules \cite{corchuelo02repairing}.
\textsc{CR Insert} finds all terminals reachable from the current state and
creates insert repairs for them (other than the EOF token `$\$$').
\textsc{CR Delete} creates deletion repairs if user defined input remains.
\textsc{CR Shift 1} parses at least 1 and at most $N_\textit{shifts}$ tokens; if it reaches an accept or error
state, or parses exactly $N_\textit{shifts}$ tokens, then a shift repair per
token shifted is created.}
\label{fig:corchuelo:reductions}
\vspace*{-8pt}
\end{figure}


\begin{figure}[tb]
\begin{adjustbox}{valign=t,minipage=.52\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def corchueloetal(pstack, toks):
  todo = [[(pstack, toks, [])]]
  cur_cst = 0
  while cur_cst < len(todo):
    if len(todo[cur_cst]) == 0:
      cur_cst += 1
      continue
    n = todo[cur_cst].pop()
    if action(n[0][-1], n[1][0]) == <!{\textrm{\textit{accept}}}!> \
       or ends_in_N_shifts(n[2]):
      return n
    elif len(n[1]) - len(toks) == N_total:
      continue
    for nbr in all_cr_star(n[0], n[1]):
      if len(n[2]) > 0 and n[2][-1] == <!{\textrm{\textit{delete}}}!> \
         and nbr[2][-1] == <!{\textrm{\textit{insert}}}!>:
        continue
      cst = cur_cst + rprs_cst(nbr[2])
      for _ in range(len(todo), cst):
        todo.push([])
      todo[cst].append((nbr[0], nbr[1], \
                        n[2] + nbr[2]))
  return None

def rprs_cst(rprs):
  c = 0
  for r in rprs:
    if r == <!{\textrm{\textit{shift}}}!>: continue
    c += 1
  return c

def all_cr_star(pstack, toks):
  # Exhaustively apply the <!{$\crarrowstar$}!> relation to
  # (pstack, toks) and return the resulting
  # list of (pstack, toks, repair) triples.
\end{lstlisting}
\end{adjustbox}
\hspace{5pt}
\begin{adjustbox}{valign=t,minipage=.46\textwidth}
\vspace*{-0pt}
\caption{Our version of the \corchuelo algorithm. The main function
\texttt{corchueloetal} takes in a (\emph{parsing stack}, \emph{token list})
pair and returns: a (\emph{parsing stack}, \emph{token list}, \emph{repair
sequence}) triple where \emph{repair sequence} is guaranteed to be a minimum
cost repair sequence; or \texttt{None} if it failed to find a repair sequence.\\[9pt]
%
The algorithm maintains a todo list of lists: the first sub-list contains
configurations of cost 0, the second sub-list configurations of cost 1, and
so on. The todo list is initialised with the error parsing stack, remaining
tokens, and an empty repair sequence (line 2). If there are todo items left, a
lowest cost configuration $n$ is picked (lines 4--8). If $n$ represents an accept state (line
9) or if the last $N_\textit{shifts}$ repairs are shifts (line 10), then $n$
represents a minimum cost repair sequence and the algorithm terminates
successfully (line 11). If $n$ has already consumed $N_\textit{total}$ tokens,
then it is discarded (lines 12, 13). Otherwise, $n$'s neighbours
are gathered using the $\crarrow$ relation (lines 14, 32--35). To avoid
duplicate repairs, \textit{delete} repairs never follow \textit{insert} repairs
(lines 15--17). Each neighbour has its repairs costed (line 18) and is then
assigned to the correct todo sub-list (lines 21--22).\\[9pt]
%
The \texttt{rprs\_cst} function returns the cost of a repair sequence. Inserts
and deletes cost 1, shifts 0.}
\label{fig:corchuelo:algorithm}
\end{adjustbox}
\vspace{-15pt}
\end{figure}

As with the original, we explain the approach in two parts.
First is a new reduction relation $\crarrow$ which defines a configuration's
neighbours (Figure~\ref{fig:corchuelo:reductions}). Second is an algorithm which
makes use of the $\crarrow$ relation to generate neighbours, and determines when
a successful configuration has been found or if error recovery has failed
(Figure~\ref{fig:corchuelo:algorithm}).
As well as several changes for clarity, the biggest difference is that
Figure~\ref{fig:corchuelo:algorithm} captures semi-formally what
\corchuelo explain in prose (spread amongst
several topics over several pages): perhaps inevitably
we have had to fill in several missing details. For example,
\corchuelo do not define what the cost of repairs is: for
simplicities sake, we define the cost of \textit{insert} and \textit{delete} as
1, and \textit{shift} as 0.\footnote{It is trivial to extend this to variable
token costs if desired, and our implementation supports this. However, it is
unclear whether non-uniform token costs are useful in practise
\cite[p.96]{cerecke03phd}.}


\subsection{Ensuring that minimum cost repair sequences aren't missed}
\label{corchuelo:kimyi}

\textsc{CR Shift 1} has two flaws which prevent it from generating
all possible minimum cost repair sequences.

\begin{figure}[tb]
\small
\[
\hspace{-5pt}
\infer[\textsc{CR Shift 2}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_j \ldots t_n])
 \wedge 0 \leq j \leq N_\textit{shifts}
\\
 (j = 0 \wedge [s_0 \ldots s_n] \ne [s'_0 \ldots s'_m]) \vee j = N_\textit{shifts} \vee
 \textsf{action}(s'_m, t_j) \in \{\textit{accept}, \textit{error}\}
\end{array}}
\]

\[
\hspace{20pt}
\infer[\textsc{CR Shift 3}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_j \ldots t_n], R)}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s_m], [t_j \ldots t_n])
 \wedge 0 \leq j \leq 1
\\
 (j = 0 \wedge [s_0 \ldots s_n] \ne [s'_0 \ldots s_m] \wedge R = [])
  \vee
  (j = 1 \wedge R = [\textit{shift}])
\end{array}
}
\]
\vspace{-15pt}
\caption{\textsc{CR Shift 1} always consumes input, when sometimes performing
one or more reduction/gotos without consuming input would be better. \textsc{CR
Shift 2} addresses this issue. Both \textsc{CR Shift 1} and \textsc{CR Shift 2}
generate multiple shift repairs in one go, which causes them to skip
`intermediate' (and sometimes important) configurations. \textsc{CR Shift 3}
generates at most one shift, exploring all intermediate configurations.}
\label{fig:corchuelo:kimyi}
\vspace*{-0pt}
\end{figure}


\begin{figure}[t]
\hspace*{0pt}
\begin{tabular}{llll}
\begin{subfigure}{0.012\textwidth}
\vspace{-22pt}
\caption{}
\label{lst:crshift2:crshift2}
\end{subfigure}
&
\begin{minipage}{190pt}
\begin{lstlisting}
Delete 3, Delete +
Delete 3, Shift +, Insert Int
Insert +, Shift 3, Shift +, Insert Int
Insert *, Shift 3, Shift +, Insert Int
\end{lstlisting}
\end{minipage}
&
\begin{subfigure}{0.012\textwidth}
\vspace{-22pt}
\caption{}
\label{lst:crshift2:crshift3}
\end{subfigure}
&
\begin{minipage}{140pt}
\vspace{-16pt}
\begin{lstlisting}
Insert *, Shift 3, Delete +
Insert +, Shift 3, Delete +
\end{lstlisting}
\end{minipage}
\end{tabular}
\vspace{-5pt}
\caption{Given the input `\texttt{2 3 +}' and the grammar from
Figure~\ref{fig:exprgrammar}, \textsc{CR Shift 1} is unable to find any repair
sequences because it does not perform the reductions/gotos necessary after
the final \textit{insert} or \textit{delete} repairs to reach an accept state.
(\subref{lst:crshift2:crshift2}) \textsc{CR Shift 2} can find 4 minimum cost repair sequences.
(\subref{lst:crshift2:crshift3}) \textsc{CR Shift 3} can find a further 2
minimum cost repair sequences on top those found by \textsc{CR Shift 2} (i.e.~6 in total).}
\label{fig:crshift2:example}
\end{figure}

First, \textsc{CR Shift 1} requires at least one token to be shifted. However,
after a non-shift repair, all that may be needed to reach a useful next configuration,
or an \textit{accept} state, is one or more reductions/gotos via
\textsc{LR Reduce}. \textsc{CR Shift 2} in Figure~\ref{fig:corchuelo:kimyi}
shows the two-phase fix which addresses this problem.
We first change the condition $0 < j \leq N_\textit{shifts}$ to $0 \leq j \leq
N_\textit{shifts}$ (i.e.~we don't force the LR parser to consume any tokens).
However, this then opens the possibility of an infinite loop. We avoid this by
saying that, if the input is not advanced, the parsing stack must have changed.
Put another way, in either case we require progress
to be made, even if that progress does not require consuming any input.

Second, \textsc{CR Shift 1} and \textsc{CR Shift 2} generate multiple shifts
at a time. This causes them to skip intermediate
configurations from which minimum cost repair sequences may be found.
The solution\footnote{The problem, and the basis of a fix, derive from
\cite[p.~12]{kimyi10astar}, though their suggestion suffers from the same
problem as \textsc{CR Shift 1}.} is simple: at most one shift can be generated at any one
time. \textsc{CR Shift 3} in
Figure \ref{fig:corchuelo:kimyi} (as well as incorporating the fix from
\textsc{CR Shift 2}) generates at most one shift repair at a time. Relative to
\textsc{CR Shift 1}, it is simpler, though it also inevitably slows down the
search, as more configurations are generated.

The problems with \textsc{CR Shift 1}, in particular, can be severe.
Figure~\ref{fig:crshift2:example} shows an example input where \textsc{CR Shift
1} is unable to find any repair sequences, \textsc{CR Shift 2} some, and
\textsc{CR Shift 3} all minimum cost repair sequences.


\subsection{Implementation considerations}
\label{corchuelo:implementation}

The definitions we have given thus far do not obviously lead to
an efficient implementation and \corchuelo give few useful
hints. We found that two techniques were both
effective at improving performance while being simple to implement.

First, although \corchuelo do not refer to it as such,
it was clear to us that the most natural way to model the search is as an instance of
Dijkstra's algorithm. However, rather than use a general queue data-structure (probably based on a
tree) to discover which element to search next, we use a similar queue data-structure to
\cite[p.~25]{cerecke03phd}. This consists of one sub-list per cost (i.e.~the
first sub-list contains configurations
of cost 0, the second sub-list configurations of cost 1 and so on).
Since we always know what cost we are currently investigating,
finding the next todo element requires only a single \texttt{pop}
(line 8 of Figure~\ref{fig:corchuelo:algorithm}). Similarly,
adding elements requires only an \texttt{append} to the relevant sub-list
(lines 18, 21, 22). This
data-structure is a good fit because costs in our setting are always small
(double digits is unusual for real-world
grammars) and each neighbour generated from a configuration with cost $c$ has
a cost $\geq c$.

\label{cactusconfigurations}
Second, we do not use lists to represent parsing stacks and repair sequences
as Figure~\ref{fig:corchuelo:algorithm} may suggest. We found
that this representation consumes noticeably more memory, and is slightly less
efficient, than using parent pointer trees (often called `cactuses').
Every node in such a tree has a reference to a single parent (or \texttt{null} for the
root node) but no references to child nodes. Since our implementation is written
in Rust -- a language without garbage collection -- nodes
are reference counted (i.e.~a parent is only freed when it is not in a todo list and
no children point to it). When the error recovery algorithm starts, it
converts the main parsing stack (a list) into a parent pointer tree; and
repair sequences start as empty parent pointer trees. The $\crarrow$ part
of our implementation thus operates exclusively on parent pointer trees.
Although this does mean that neighbouring configurations are scattered throughout
memory, the memory sharing involved seems to more than compensate for
poor cache behaviour; it also seems to be a good
fit with modern \texttt{malloc} implementations, which are particularly
efficient when allocating and freeing objects of the same size. While we
suspect this representation is always likely to be a reasonable choice, it is
difficult to generalise from our experience whether it will always be the best
in other contexts, in particular for garbage collected languages.

One seemingly obvious further improvement is to split the search into parallel threads. However,
we found that the nature of the problem means that parallelisation is more
tricky, and less productive, than might be expected. There are two related
problems: we cannot tell in advance if a given
configuration will have huge numbers of successors or none at all; and
configurations are, in general, searched for successors extremely quickly. Thus
if we attempt to seed threads with initial sets of configurations, some threads
quickly run out of work whilst others have ever growing queues. If,
alternatively, we have a single global queue then significant amounts of time
can be spent adding or removing configurations in a thread-safe manner. This
suggests that the right approach is likely to be a combination of the two
approaches: threads would have a local queue which, if it gets too full,
would be partly emptied into a global queue, from which otherwise idle threads
can find new work. As we shall see in Section~\ref{experiment}, \cpctplus runs
fast enough that the additional complexity of such an approach is not, in our
opinion, justified.


\section{\cpctplus}
\label{corchueloplus}

In this section, we extend the \corchuelo algorithm to become what we call
\cpctplus. First we extend the algorithm to find
the complete set of minimum cost repair sequences
(Section~\ref{corchuelo:allminimumcost}). Since this significantly slows down
the search, we introduce a significant optimisation in the form
of merging compatible configurations (Section~\ref{merging}). The complete set of
minimum cost repair sequences allows us to make an
algorithm less susceptible to the cascading error problem
(Section~\ref{rankingrepairs}). We then change the criteria for terminating
error recovery (Section~\ref{timeout}).


\subsection{Finding the complete set of minimum cost repair sequences}
\label{corchuelo:allminimumcost}

The basic \corchuelo algorithm non-deterministically completes as soon
as it has found a single minimum cost repair sequence. This is confusing
in two different ways: the successful repair sequence found can vary from run
to run; and the successful repair sequence might not match the user's intention.

We therefore introduce the idea of the complete set of repair sequences: that
is all equivalently good repair sequences. Although we will refine the concept
of `equivalently good' in Section~\ref{rankingrepairs}, at this stage we
consider all successful repair sequences with the minimum cost $c$ to be
equivalently good. In other words, as soon as we find the first successful repair
sequence, its cost $c$ defines the minimum cost.

An algorithm to generate this set is then simple:
when a repair sequence of cost $c$ is found to
be successful, we discard all repair sequences with cost $> c$, and continue
exploring configurations in cost $c$ (including, transitively, all neighbours that are
also of cost $c$; those with cost $> c$ are immediately discarded). Each
successful configuration is recorded and, when all configurations
in $c$ have been explored, the set of successful configurations is returned.
One of these successful configurations is then non-deterministically chosen,
applied to the input, and parsing continued.


\subsection{Merging compatible configurations}
\label{merging}

Relative to finding a single solution, finding the complete set of repair
sequences can be extremely expensive because there may
be many remaining configurations in $c$, which may, transitively, have many neighbours.
Our solution to this performance problem is to merge together \emph{compatible}
configurations on-the-fly, preserving their distinct repair sequences while
still reducing the search space. Two configurations are compatible if:

\begin{enumerate}
\item their
parsing stacks are identical,
\item they both have an identical amount of input remaining,
\item  and their repair sequences are compatible.
\end{enumerate}

\noindent Two repair sequences are compatible:

\begin{enumerate}
   \item if they both end in the same number ($n \ge 0$) of shifts,
   \item and, if one repair sequence ends in a delete, the other repair sequence also
ends in a delete.
\end{enumerate}

\noindent The first of these conditions is a direct consequence of the fact that a configuration
is deemed successful if it ends in $N_\textit{shifts}$ shift
repairs. When we merge configurations, one part of the merge is `dominant'
(i.e.~checked for $N_\textit{shifts}$) and the other `subsumed'. Thus we have to
maintain symmetry between the dominant and subsumed parts to prevent the
dominant part accidentally preventing the subsumed part from being recorded as
successful. In other words, if the dominant part of the merge had fewer shifts
at the end of its repair sequence than the subsumed part, then the
$N_\textit{shifts}$ check (line 10, Figure~\ref{fig:corchuelo:algorithm}) would
fail, even though reversing the dominant and subsumed
parts may have lead to success. It is therefore only safe to merge repair sequences
which end in the same number of shifts.

The second condition relates to the weak form of compatible merging inherited
from \cite[p.~8]{corchuelo02repairing}: delete repairs are never followed by an
insert (see Figure~\ref{fig:corchuelo:algorithm}) since [\textit{delete},
\textit{insert x}] always leads to the same configuration as [\textit{insert x},
\textit{delete}]. Although we get much of the same effect through
compatible configuration merging, we keep it as a separate optimisation because: it is
such a frequent case; our
use of the todo list means that we would not catch every case; the
duplicate repair sequences are uninteresting from a user perspective, so we
would have to filter them out later anyway; and each additional merge costs
memory. We thus have to make sure that merged repair sequences don't accidentally
suppress insert repairs because one part of the repair sequence ends in a delete
while the other does not. The simplest way of solving this problem is thus to
forbid merging repair sequences if one sequence ends in a delete and the other does not.

Fortunately, implementing compatible configuration merging is simple. We
first modify the todo data-structure to be a
list-of-ordered-hashsets\footnote{An ordered hashset preserves insertion order,
and thus allows list-like integer indexing as well as hash-based lookups.}. This has
near-identical \texttt{append} / \texttt{pop} performance to a normal list, but
filters out duplicates with near-identical performance to an unordered hashset.
We then make use of a simply property of hashsets: an object's hash behaviour
need only be a non-strict subset of its equality behaviour. In other words,
while we need to ensure that two objects that compare equal always map to the
same hash, we can allow two objects that do not compare equal to map to the
same hash. In our context, this allows us to quickly find potentially compatible
nodes using hashing, checking for definitely compatible configurations
using equality. We therefore hash configurations based solely on their parsing stack
and remaining input whereas configuration equality is based on a configurations'
parsing stacks, remaining input, and repair sequences.

Conceptually, merging two configurations together is simple: each configuration
needs to store a set of repair sequences, each of which is updated as further
repairs are found. However, this is an extremely inefficient representation as
the sets involved need to be copied and extended as each new repair is found.
Instead, we reuse the idea of graph-structured stacks from GLR
parsing~\cite[p.~4]{tomita87efficient} which allows us to avoid copying whenever
possible. The basic idea is that configurations no longer reference a parent pointer tree of
repairs directly, but instead a parent pointer tree of \emph{repair merges}. A
repair merge is a pair (\textit{repair}, \textit{merged}) where
\textit{repair} is a plain repair and \textit{merged} is a (possibly null) set of
repair merge sequences. This structure has two advantages. First, the
$N_\textit{shifts}$ check can be performed solely using the first element of
repair merge pairs. Second, we avoid allocating memory for configurations which
have not yet been subject to a merge. The small
downside to this scheme is that expanding configurations into repair sequences requires
recursively expanding both the normal parent pointer tree of the first
element as well as the merged parent pointer trees of the second element.

Compatible configuration merging is a powerful optimisation even though
it can only merge configurations in the todo list (i.e.~we cannot detect
all possible compatible merges). An example of compatible configuration merging
can be seen in Figure~\ref{fig:cpctplus:full}.


\subsection{Ranking repair sequences}
\label{rankingrepairs}

\begin{figure}[t]
\includegraphics[width=1.0\textwidth]{cpctplus}
\caption{An elided visualisation of a real run of \cpctplus with the input
`2 3 +' and the grammar from Figure~\ref{fig:exprgrammar}. The left hand side of
the tree shows the `normal' parser at work, which hits an error as soon as it
has shifted the token `\texttt{2}': at this point, \cpctplus starts operating.
As this shows, the search encounters various dead ends, as well as successful
routes. As shown in Figure~\ref{fig:crshift2:example}, this input has 6 minimum
cost repair sequences, but the search only has 5 success configurations, because
two configurations were merged together.}
\label{fig:cpctplus:full}
\end{figure}

\begin{figure}[t]
\hspace{5pt}
\begin{tabular}{p{0.02\textwidth}p{0.35\textwidth}p{0.02\textwidth}p{0.55\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:ranking:java}
\end{subfigure}
&
\begin{minipage}[t]{0.35\textwidth}
\vspace{-14pt}
\begin{lstlisting}[language=Java]
class C {
    T x = 2 +
    T y = 3;
}
\end{lstlisting}
\end{minipage}
&
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:ranking:output}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-14pt}
\begin{lstlisting}
Parsing Error at line 3 col 7. Repair
sequences found:
  1: Insert ,
\end{lstlisting}
\end{minipage}
\end{tabular}
\vspace{-10pt}
\caption{An example showing how the ranking of repair sequences can lessen the
cascading error problem. The Java example (\subref{lst:ranking:java}) leads
to a parsing error on line 3 at `\texttt{y}', with three minimum cost repair
sequences found: [\textit{insert} \texttt{,}], [\textit{insert} \texttt{?}], and
[\textit{insert} \texttt{(}]. These repair sequences are then ranked by how far
they allow parsing to continue successfully. [\textit{insert} \texttt{,}] leads
to the rest of the file being parsed without further error. [\textit{insert}
\texttt{?}] causes a cascading error at `\texttt{;}' which must then be resolved
by completing the ternary expression started by `?' (e.g.~changing line 3 to
`\texttt{T ? y : this;}'). Similarly, [\textit{insert} \texttt{(}] causes a
cascading error at `\texttt{;}' which must then be resolved by inserting a
`\texttt{)}'. Since [\textit{insert} \texttt{,}] is ranked more highly than the
other repair sequences, the latter are discarded, leading to the parsing output shown
in (\subref{lst:ranking:output}). javac in contrast attempts to insert
`\texttt{;}' before `\texttt{y}' causing a cascading error on the next token.}
\label{fig:ranking}
\end{figure}

In nearly all cases, members of the complete set of minimum cost repair
sequences end with $N_\textit{shifts}$ (the only exception being if an error
location is found less than $N_\textit{shifts}$ from the end of an input). Thus
while the repair sequences we find are all equivalently good within the range
of $N_\textit{shifts}$, some, but not others, may perform poorly beyond that
range. This problem is exacerbated by the fact that $N_\textit{shifts}$ has
to be a fairly small integer (we use 3, the value suggested by \corchuelo)
since each additional token searched exponentially increases the
search space. From a user perspective this can mean that some members
of the complete set of minimum cost repair sequences can appear to be of much
lower quality than others.

In order to lessen this problem, we rank configurations which represent the
complete set of minimum cost repair sequences by how far they allow parsing to continue, up to a limit of
$N_\textit{try}$ tokens (which we somewhat arbitrarily set at 250). Taking
the furthest-parse point as our top rank, we then discard all configurations
which parsed less input than this. The reason
why we rank the configurations, and not the repair sequences, is that we only
need to rank one repair sequence for each merged configuration, a small but useful
optimisation. We then expand the top ranked configurations into repair
sequences and remove shifts from the end of those repair sequences. Since the
earlier merging of compatible configurations is imprecise (it misses
configurations that have already been processed), there can be some remaining
duplicate repair sequences: we thus perform a final purge of duplicate repair
sequences. Figure~\ref{fig:cpctplus:full} shows a visualisation of \cpctplus in
action.

Particularly on real-world grammars, selecting the top-ranked repair sequences
substantially decreases cascading errors (see Figure~\ref{fig:ranking} for an example).
It also does so for very little additional computational cost, as the complete set of
minimum cost repair sequences is much smaller than the number of configurations
searched. However, it cannot entirely reduce the cascading error problem. Since,
from our perspective, each member of the top-ranked set is equivalently good, we
non-deterministically select one of its members to repair the input and allow
parsing to continue. This can mean that we select a repair sequence which
performs less well beyond $N_\textit{try}$ tokens than other repair sequences in
the top-ranked set.


\subsection{Timeout}
\label{timeout}

The final part of \cpctplus relates to the use of $N_\textit{total}$ in
\corchuelo. As with all members of the
\fischer family, \cpctplus is not only unbounded in
time~\cite[p.~14]{mckenzie95error}, but
also unbounded in memory. $N_\textit{total}$ is an attempt to stop the
algorithm from running unacceptably long by limiting how much input the
algorithm will consider for modification. Unfortunately it is impossible to find
a good value for this, as `too long' is entirely dependent on the grammar and
erroneous input: Java's grammar, for example, is large with a commensurately
large search space while Lua's grammar is small with a commensurately small
search space.

This problem can be easily seen on inputs with unbalanced brackets
(e.g.~expressions such as `\texttt{x = f(();}'): each additional unmatched
bracket exponentially increases the search space. On a modern machine
with a Java 7 grammar, \cpctplus takes about 0.3s to find
the complete set of minimum cost repair sequences for 3 unmatched brackets, 3s
for 4 unmatched brackets, and 6 unmatched brackets caused our 32GiB test machine
to run out of RAM.

Attempting to work around this problem by reducing $N_\textit{total}$
leads to some simple, but spread-out syntax errors, no longer being repaired.
The only sensible alternative is a timeout: up to several seconds is safe in our
experience. We thus remove $N_\textit{total}$ from \cpctplus and rely
entirely on a timeout which, in this paper, is defined to be 0.5s.


\section{Experiment}
\label{experiment}

In order to understand the performance of \cpctplus, we conducted a large
experiment on real-world Java code. In this section we outline our methodology
(Section~\ref{methodology}) and results (Section~\ref{results}). Our experiment
is fully repeatable and downloadable from
\url{https://archive.org/download/error_recovery_experiment/0.3/}. The results
from our particular run of the experiment can also be downloaded from the same
location.


\subsection{Methodology}
\label{methodology}

In order to evaluate error recovery implementations, we need a concrete implementation. We
implemented a new Yacc-compatible parsing system \emph{grmtools} in Rust which
we use for our experiments. Including
associated libraries for LR table generation and so on, \emph{grmtools} is around
13KLoC. Although intended as a production library, it has accidentally
played a part as a flexible test bed for experimenting with, and understanding,
error recovery algorithms. We added a simple front-end \emph{nimbleparse} which produces the output
seen in e.g.~Figure~\ref{fig:javaerror}.

There are two standard problems when evaluating error recovery algorithms: how
to determine if a good job has been done on an individual example; and obtaining
sufficient examples to get a wide perspective on an algorithm's performance. To
some extent, solutions to these problems are mutually exclusive: for
real-world inputs, the only way to guarantee that a good job has been done is to
manually evaluate it, which means that it is only practical to use a small set
of input programs. Most papers we are aware of use at most 200 source files
(e.g.~\cite{corchuelo02repairing}), with one using a single source file with minor
variants (\cite{kimyi10astar}). \cite{cerecke03phd} was the first to use a
large-scale corpus of approximately 60,000 Java source files. Early in the
development of our methodology, we performed some rough experiments which
suggested that statistics only start to stabilise once a corpus exceeds 10,000
source files. We therefore prefer to use a much larger corpus than most previous
studies. We are fortunate to have access to the Blackbox
project~\cite{brown14blackbox}, an opt-in data collection facility for the BlueJ
editor, which records major editing events (e.g.~compiling a file) and sends
them to a central repository. Crucially, one can see the source code associated
with each event. What makes Blackbox most appealing as a data source is its
scale and diversity: it has hundreds of thousands of users, and a huge
collection of source code.

We first obtained a Java 1.5 Yacc grammar and updated it to support Java
1.7.\footnote{Unfortunately, changes to the method calling syntax in Java 1.8
mean that it is an awkward, though not impossible, fit for an LR(1) formalism
such as Yacc, requiring substantial changes to the current Java Yacc grammar. We
consider the work involved beyond that useful for this paper.} We then
randomly selected source files from Blackbox's database (following the lead of
\cite{santos18syntax}, we selected data from Blackbox's beginning until the end
of 2017-12-31). We then ran such source files through our Java 1.7 lexer. We immediately
rejected files which didn't lex, since such files cannot be considered
for parsing.\footnote{Happily, this also excludes outputs which can't
possibly be Java source code. Some odd things are pasted into text
editors.} We then parsed candidate files with our Java grammar and rejected any
which did parse successfully, since there is little point running an error
recovery algorithm on correct input. The final corpus consists of \corpussize source files
(collectively a total of \corpussizemb{}MiB). Since Blackbox, quite reasonably,
requires each person with access to the source files to register with them, we
cannot distribute the source files directly; instead, we distribute the
(inherently anonymised) identifiers necessary to extract the source
files for those who register with Blackbox.

The size of our corpus means that we cannot manually evaluate repairs for
quality. Instead, to evaluate this paper's most important metric (the cascading
error problem), we report the number of error locations found in the
corpus for different algorithms. We know, by definition, that the corpus
contains at least \corpussize manually created errors (i.e.~at least one per
file). Since it is likely that some files have more than one manually created
error, the minimum possible number of error locations is likely to be
bigger than this, but we have no way of knowing the true number. However, we can
compare different algorithms: the fewer error locations an algorithm
reports, the fewer cascading errors it has caused. This comes with an
important caveat: a nefarious error recovery algorithm could simply skip all
input after the first error encountered, thus reporting `only' \corpussize
error locations (i.e.~one per file). Since, for other reasons, we also record the proportion of input skipped,
we can confirm that this is not a significant factor in any of the algorithms
we report on.

In order to test hypothesis H1 we ran each error recovery algorithm against
the entire Java corpus, collecting for each file: the time spent in recovery (in seconds);
whether error recovery on the file was successful (true or false); the
number of error locations; the cost of repair sequences at each
error location (only if error recovery was successful on the file as a whole);
and the number of tokens skipped by error recovery (i.e.~how many \emph{delete}
repairs were applied). Note that parsing fails if the timeout is exceeded or
if the algorithm runs out of plausible candidate repair sequences.
We measure the time spent in error recovery with a monotonic wall-clock timer,
covering the time from when the main parser first invokes
error recovery until an updated parsing stack and parsing index are returned
along with minimum cost repair sequences. The timer is suspended when normal parsing
restarts and resumed if error recovery is needed again (i.e.~the timeout applies
to the file as a whole).

In order to test hypothesis H2, we created a variant of \cpctplus called \cpctplusrev,
collecting the same data as for the other error recovery algorithms.
Instead of selecting from the minimum cost repair sequences which allow
parsing to continue furthest, \cpctplusrev selects from those which allow parsing to
continue the least far. This models the worst case for other members of the
\fischer family which non-deterministically select a single minimum
cost repair sequence. In other words, it allows us to understand how many
more errors could be reported to users of other members of the
\fischer family compared to \cpctplus.

In order to understand the accuracy of the numbers we report, we provide 99\%
confidence intervals. We bootstrapped~\cite{efron79bootstrap} our results
\numbootstrap times to produce confidence intervals. However, since, as
Figure~\ref{fig:results:cpctplus_histogram} shows, our distribution is heavy-tailed,
we cannot bootstrap naively. Instead, we ran each error recovery algorithm
\numruns times on each source file; when bootstrapping we randomly sample one of the
\numruns values collected (i.e.~our bootstrapped data contains an entry for every
file in the experiment; that entry is one of the \numruns values collected for
that file). The only subtlety is when bootstrapping the mean cost
size: this value only makes sense if the file was successfully recovered from,
so we do not sample from runs where error recovery failed.

All experiments were run on an otherwise unloaded Intel Xeon E3-1240 v6 with
32GiB RAM running Debian 10. We disabled hyperthreading and turbo boost and ran experiments
serially. Our experiments take approximately 4 days to complete. We used Rust
1.40.0 to compiler \emph{grmtools} (the
\texttt{Cargo.lock} file necessary to reproduce the build is included in our
experimental repository).


\subsection{Results}
\label{results}

\begin{figure}[t]
\begin{tabular}{lcccccc}
\toprule
  & Mean     & Median   & Cost      & Failure   & Tokens      & Error \\
  & time (s) & time (s) & size (\#) & rate (\%) & skipped (\%) & locations (\#) \\
\midrule
\input{table.tex}
\bottomrule
\end{tabular}
\caption{Summary statistics from running our error recovery algorithms over
a corpus of \corpussize Java files (for all measures, lower values are better).
Mean and median times
report how long was spent in error recovery per file: both figures
include files which exceeded the recovery timeout, so they represent the `real'
times that users would experience, whether or not all errors are repaired or
not. Cost size reports the mean cost (i.e.~the number of insert and delete
repairs) of each error location repaired (this number is meaningless for \panic,
which does not have a concept of costable repairs).
The failure rate is the percentage of files which could not be fully
repaired within the timeout (this number is semi-meaningless for \panic, which,
at worst, is always able to find a repair at the EOF token). Tokens skipped is
the percentage of input
skipped (because of a delete repair).}
\label{fig:results:summary}
\end{figure}

\begin{figure}[t]
\vspace{-10pt}
\includegraphics[scale=.7]{cpctplus_histogram.pdf}
\caption{A histogram of the time spent in error recovery by \cpctplus for files in our
corpus. The $x$ axis shows time (up to the timeout of 0.5s) and the $y$ axis is
a logarithmic scale for the number of files. Error bars represent 99\% confidence
intervals. As this clearly shows, most files
are repaired extremely quickly. There is then a continual decrease
until the timeout of 0.5s, where the files that were unable to be repaired
in the timeout cause a small, but pronounced, peak.}
\label{fig:results:cpctplus_histogram}
\end{figure}

Figure~\ref{fig:results:summary} shows a summary of the results of our
experiment. The overall conclusions are fairly clear. \cpctplus is
able to repair nearly all input files within the 0.5s timeout; and while
panic mode is able to repair every file within the 0.5s timeout, it
reports well over twice as many error locations as \cpctplus (i.e.~panic
mode substantially worsens the cascading error problem).
The fact that the median recovery time for \cpctplus is two orders of
magnitude lower than the mean
recovery time suggests that only a small number of outliers cause error recovery to
take long enough to be perceptible to humans; this is confirmed by the
histogram in Figure~\ref{fig:results:cpctplus_histogram}. These results strongly
validate Hypothesis H1.

\begin{figure}[t]
\includegraphics[scale=.7]{cpctplus_cpctplusrev_error_locs_histogram_zoomed.pdf}
\caption{A histogram of the number of files with 0--50 errors (this removes a
handful of outliers which distort the full histogram, which can be found in
Figure~\ref{fig:results:cpctplus_cpctplusrev_histogram_full} in the Appendix)
for \cpctplus and \cpctplusrev. The $x$ axis shows the number of error
locations in a file and the $y$ axis is a logarithmic scale for the number of
files. Error bars represent 99\% confidence intervals.  As this clearly
shows, the entire distribution is skewed
slightly rightwards by \cpctplusrev, showing that \cpctplusrev makes error recovery
slightly worse in a number of files (rather than making error recovery in a small
number of files a lot worse).}
\label{fig:results:cpctplus_cpctplusrev_histogram}
\end{figure}

\cpctplus ranks the complete set of minimum cost repair sequences by how
far parsing can continue and chooses from those which allow parsing to
continue furthest. \cpctplusrev, in contrast, selects from those which allow parsing
to continue the least far. \cpctplusrev shows that the ranking technique used in \cpctplus substantially
reduces the potential for cascading errors: \cpctplusrev leads to
\cpctplusreverrorlocsratioovercpctplus more error locations being reported to users
relative to \cpctplus. As the histogram in Figure~\ref{fig:results:cpctplus_cpctplusrev_histogram} shows,
the distribution of error locations in \cpctplus and \cpctplusrev is
similar, with the latter simply shifted slightly to the right.
In other words, \cpctplusrev
makes error recovery slightly worse in a number of files
(rather than making error
recovery in a small number of files a lot worse). This strongly validates
Hypothesis H2.

Interestingly, \cpctplusrev has a noticeably higher mean cost of repair sequences relative to
\cpctplus. In other words, \cpctplusrev not only causes more error locations to be reported,
but the repair sequences at the additional error locations have higher numbers
of insert and delete repairs. This suggests that there is a double whammy from
cascading errors: not only are more error locations reported, but the poorer
quality repair sequences chosen make subsequent error locations
disproportionately harder for the error recovery algorithm to recover from.


\subsection{The impact of skipping input}

As well as the much higher failure rate, the number of error locations reported
by panic mode is well over twice that of \cpctplus. This led us to make
an additional hypothesis:

\begin{description}
  \item[H3] The more of the user's input that is skipped, the greater the number
of cascading parsing errors.
\end{description}

The intuition underlying this hypothesis is that, in general, the user's input
is very close to being correct: thus the more of the input that one skips, the
less likely one is to get back to a successful parse. We thus added the ability
to record how much of the user's input is skipped as the result of \emph{delete}
repairs during error recovery. The figures surprised us: \cpctplus skips
very little of the user's input; \cpctplusrev skips a little more; and
panic mode skips an order of magnitude more. Although we do
not have enough data points to make a definitive statement, our data seem to
validate Hypothesis H3.


\section{Using error recovery in practice}
\label{sec:api}

Although several members of the \fischer family were implemented in parsing
tools of the day, to the best of our knowledge none of those implementations
have survived. Equally, previous approaches in the \fischer family make no
mention of how error recovery should be used or, indeed, if it has any
implications for users at all.

We are therefore forced to treat the following
as an open question: can one
sensibly use error recovery in the \fischer family in practice? In
particular, given that the most common way to use LR grammars
is to execute semantic actions as each production is reduced, what should semantic actions do
when parts of the input have been altered by error recovery? This latter
question is important for real-world systems (e.g.~compilers) which can
still perform useful computations (e.g.~running a type checker) in the
face of syntax errors.

While different languages are likely to require different solutions, in
this section we show that \emph{grmtools} allows sensible integration
of error recovery in a Rust context. Readers who prefer to avoid
Rust-specific details may wish to move immediately to Section~\ref{sec:threats}.


\subsection{A basic solution}

\begin{figure}[!t]
\begin{adjustbox}{valign=t,minipage=.52\textwidth}
\begin{lstlisting}[numbers=left]
%start Expr
%%
Expr -> u64:
    Term "+" Expr { $1 + $3 }
  | Term { $1 }
  ;

Term -> u64:
    Factor "*" Term { $1 * $3 }
  | Factor { $1 }
  ;

Factor -> u64:
    "(" Expr ")" { $2 }
  | "INT"
    {
      let n = $lexer.lexeme_str(&$1.unwrap());
      match s.parse::<u64>() {
        Ok(val) => val as u64,
        Err(_) => panic!(
          "{} cannot be represented as a u64",
          s)
      }
    }
  ;
\end{lstlisting}
\end{adjustbox}
\hspace{3pt}
\begin{adjustbox}{valign=t,minipage=.46\textwidth}
\vspace*{0pt}
\caption{A naive version of the calculator grammar with semantic actions on each
production. The traditional Yacc \texttt{\%union} declaration is unwieldy in
Rust. Instead, \emph{grmtools} allows each rule to have a Rust return type
associated with it (between `\texttt{->}' and `\texttt{:}'): the actions of each
production in that rule must return values of that type. \texttt{\$\emph{n}}
variables reference the \emph{nth} symbol in a production (numbered from one
i.e. \texttt{\$1} references the first symbol). If that symbol is a reference to
a rule, a value of that rule's Rust type will be stored in that variable. If
that symbol is a token then the user's input can be obtained by
\texttt{\$lexer.lexeme\_str(\&\$1.unwrap())} (line 17). Note that while this
grammar's semantic actions work as expected for inputs such as `2 + 3 * 4', they will
panic if too large a number is passed (lines 20--23), or if an integer is
inserted by error recovery. Figure~\ref{fig:api:resulttype} shows how to
avoid both problems.}
\label{fig:api:naive}
\end{adjustbox}
\end{figure}

Figure~\ref{fig:api:naive} shows a naive \emph{grmtools} version of the grammar
from Figure~\ref{fig:exprgrammar} that can evaluate numeric results as
parsing occurs (i.e.~given the input $2 + 3 * 4$ it returns $14$). This
grammar should mostly be familiar to Yacc users: each production has a \emph{semantic
action} (i.e.~Rust code that is executed when the production is reduced); and
symbols in the production are available to the semantic action
as pseudo-variables named \texttt{\$\emph{n}} (a production of
$n$ symbols has $n$ pseudo-variables with the first symbol connected to
\texttt{\$1} and so on). A minor difference from traditional Yacc is that
\emph{grmtools} allows rules to specify a different return type, an approach
shared with other modern parsers such as ANTLR~\cite{parr13definitive}.

A more significant difference is in the contents of the \texttt{\$\emph{n}}
pseudo-variables and how user input is extracted from them. If a pseudo-variable
references a rule $R$, then that pseudo-variable's static type is $R$'s return type.
However, if a pseudo-variable references a token $T$, then that
pseudo-variable's static type is (slightly simplified) \texttt{Result<Lexeme,
Lexeme>}. We will explain the reasons for this shortly, but at this stage it
suffices to note that we can extract tokens matching the user's input by
calling \texttt{\$1.unwrap()}, and obtain the actual string the user passed by
using the globally available \texttt{\$lexer.lexeme\_str} function.


\subsection{Can semantic action execution continue in the face of error recovery?}

\begin{figure}[!t]
\begin{adjustbox}{valign=t,minipage=.60\textwidth}
\begin{lstlisting}[numbers=left]
%start Expr
%avoid_insert "INT"
%%
Expr -> Result<u64, ()>:
   Term "+" Expr { Ok($1? + $3?) }
 | Term { $1 }
 ;

Term -> Result<u64, ()>:
   Factor "*" Term { Ok($1? * $3?) }
 | Factor { $1 }
 ;

Factor -> Result<u64, ()>:
  '(' Expr ')' { $2 }
 | 'INT'
   {
     let n = $lexer.lexeme_str(&$1.map_err(|_| ())?);
     match s.parse::<u64>() {
       Ok(val) => Ok(val as u64),
       Err(_) => eprintln!(
         "{} cannot be represented as a u64", s
         );
         Err(())
     }
   }
 ;
\end{lstlisting}
\end{adjustbox}
\hspace{3pt}
\begin{adjustbox}{valign=t,minipage=.38\textwidth}
\vspace*{0pt}
\caption{A more sophisticated version of the grammar from
Figure~\ref{fig:api:naive}. Each rule now returns a \texttt{Result} type. If an
integer is inserted by error recovery, the \texttt{Factor} rule returns
\texttt{Err(())} (line 18). All other rules simply percolate such errors upwards
using the `\texttt{?}' operator (which, if the \texttt{Result}-returning
expression it is attached to evaluates to an \texttt{Err}, immediately returns
that error; otherwise it unwraps the \texttt{Ok}). Note that other token types
are unaffected: if error recovery inserts a bracket, for example, evaluation of
the expression continues.}
\label{fig:api:resulttype}
\end{adjustbox}
\end{figure}

In Yacc, semantic actions can assume that each symbol in the
production has `normal' data attached to it (either a rule's value or the
string matching a token; Yacc's error recovery is implicitly expected to
maintain this guarantee). This assumption is unsafe in our setting, if we apply
a repair sequence with an insert repair: the inserted token will have a type
but no value. Given the input `(2 + 3', the inserted close bracket is not
hugely important, and our calculator returns the value 5. However,
given the input `2 +',
\cpctplus finds a single repair sequence \emph{[Insert Int]}: what should
a calculator do with an inserted integer? Our naive calculator simply \texttt{panic}s
(which is roughly equivalent to `raises an exception and then exits') in such a
situation (the \texttt{unwrap} in Figure \ref{fig:api:naive} on line 17).
However, there are two alternatives to this rather extreme outcome:
the semantic action can assume a default value or stop further execution
of semantic values while allowing parsing to continue. Determining which is the
right action in the face of inserted tokens is inherently situation specific.
We therefore need to find a pragmatic way for users to control what happens in
such cases.

The approach we take is to allow users to easily differentiate normal vs.~inserted
tokens in a semantic action. Pseudo-variables that reference tokens have
(slightly simplified) the Rust type \texttt{Result<Lexeme, Lexeme>}. Rust's \texttt{Result}
type\footnote{Equivalents are found in several other languages: Haskell's
\texttt{Either}; O'Caml's \texttt{result}; or Scala's \texttt{Either}.} is a sum
type which represents success (\texttt{Ok($\ldots$)}) or error
(\texttt{Err($\ldots$)}) conditions. We use the \texttt{Ok} case
to represent `normal' tokens created from user input and the \texttt{Err} case
to represent tokens inserted by error recovery. Since the \texttt{Result} type
is widely used in Rust code, users can avail themselves of standard idioms.

For example, we can then alter our calculator grammar to continue
parsing, but stop executing meaningful semantic action code, when an inserted integer
is encountered. We change grammar rules from returning
type \texttt{T} to \texttt{Result<T, ()>} (where `()' is Rust's unit type). When a
rule cannot produce a value, for whatever reason, it simply returns
\texttt{Err(())}. It is then, deliberately, fairly easy to use with the
\texttt{Result<Lexeme, Lexeme>} type: for tokens whose value we absolutely
require, we map the \texttt{Err(Lexeme)} case to \texttt{Err(())} with the
(standard, if mildly clunky) idiom `\texttt{\$n.map\_err(|\_| ())?}'. In essence,
this first says `if \$n is an \texttt{Err(Lexeme)}, convert it to
\texttt{Err(())}' (with \texttt{map\_err}) and then `if we have
\texttt{Err(...)} percolate it upwards, otherwise unwrap the \texttt{Ok} case'
(the `\texttt{?}' operator). While slightly verbose, this idiom is easily
understood by Rust programmers. Figure~\ref{fig:api:resulttype} shows a
version that changing the grammar to make use of this idiom requires relatively
little extra code.


\subsection{Avoiding insert repairs when possible}

Although we now have a reasonable mechanism for dealing with inserted tokens,
there are cases where we can bypass them entirely. For
example, consider the input `2 + + 3', which has two repair sequences
\emph{[Delete +], [Insert Int]}: evaluation of the expression can continue
with the former repair sequence, but not the latter. However, as
presented thus far, these repair sequences are ranked equally and one
non-deterministically selected.

We therefore added an optional declaration
\texttt{\%avoid\_insert} to \emph{grmtools} which allows users to specify
those tokens which, if inserted by error recovery, are likely to prevent
semantic actions from continuing execution. In practise, this is synonymous with those
tokens whose values (and not just their types) are important. In the calculator
grammar only the \texttt{INT} token satisfies this
criteria, so we add \texttt{\%avoid\_insert "INT"} to the grammar. We then
make a simple change to the repair sequence ranking of
Section~\ref{rankingrepairs} such that the final list of repair sequences is
sorted with inserts of such tokens at the bottom of the list. In our case, this
means that we deterministically always select \emph{Delete +} as the repair
sequence to apply to the input `2 + + 3' (though note that we still present the
\emph{Insert Int} repair sequence to the user, simply ranking it consistently as
the second option).


\section{Threats to validity}
\label{sec:threats}

Although it might not be obvious at first, \cpctplus is
non-deterministic, which can lead to surprising
different results from one run to the next. The root cause of this problem is
that multiple repair sequences may have identical effects up to
$N_\textit{try}$ tokens, but cause different effects after that value.
By running each file through each
error recovery multiple times and reporting confidence intervals, we are able
to give a good -- though inevitably imperfect -- sense of the likely variance
induced by this non-determinism.

Blackbox contains an astonishingly large amount of source code but has two
inherent limitations. First, it only contains Java source code. This means that
our main experiment is limited to one grammar: it is possible that our
techniques do not generalise beyond the Java grammar (though, as
Appendix~\ref{app:examples} suggests, our techniques do appear to work well
on other grammars). Although \cite[p.~109]{cerecke03phd}
suggests that different grammars make relatively little difference
to the performance of such error recovery algorithms, we are not aware
of an equivalent repository for other language's source code. One solution is
to mutate correct source files (e.g.~randomly deleting tokens), thus
obtaining incorrect inputs which we can later test: however, it is difficult
to uncover and then emulate the numerous, sometimes surprising, ways that
humans make syntax errors, particularly as some are language specific
(though there is some early work in this area~\cite{dejonge12automated}).
Second, Blackbox's data comes largely from students,
who are more likely than average to be somewhat novice programmers. It is clear
that novice programmers make some different syntax errors -- and, probably, make
some syntax errors more often -- relative to advanced programmers. For example,
many of the files with the greatest number of syntax errors are caused by
erroneous fragments repeated with variants (i.e.~it is likely that the
programmer wrote a line of code, copy and pasted it, edited it, and repeated
that multiple times before deciding to test the syntactic validity). It is
thus possible that a corpus consisting solely of programs from advanced programmers
would lead to slightly different results. We consider this a minor worry,
partly because a good error recovery algorithm should
aim to perform well with inputs from users of different experience levels.

Our corpus was parsed using a Java 1.7 grammar, but some members of the corpus
were almost certainly written using Java 1.8 or later features. Many -- though not all -- post-1.7 Java
features require a new keyword: such candidate source files would thus have
failed our initial lexing test and not been included in our corpus. However,
some Java 1.8 files will have made it through our checks. Arguably these are still a valid
test of our error recovery algorithms. It is even likely that they may be a little
more challenging on average, since they are likely to be further away from being valid
syntax than files intended for Java 1.7.


\section{Related work}

Error recovery techniques are so numerous that there is no
definitive reference or overview of them. However, \cite{degano95comparison}
contains an overall historical analysis and \cite{cerecke03phd} an excellent
overview of many members of the \fischer family. Both
must be supplemented with more recent works.

The biggest limitation of error recovery algorithms in the
\fischer family (including \cpctplus) is that they find repairs at the
point that an error is discovered, which may be later in the file than the cause
of the error. Thus even when they successfully recover from an error, the repair
sequence reported may be very different from the fix the user considers
appropriate (note that this is distinct from the cascading error problem,
which our ranking of repair sequences in Section~\ref{rankingrepairs} partly
addresses). A common, frustrating, example of this is a missing `\texttt{\}}' character in
C/Java-like languages. Some approaches are able to backtrack from the source of
the error in order to try and find more appropriate repairs. However, there are
two challenges to this: first, the cost of maintaining the necessary state to
backtrack slows down normal parsing (e.g.~\cite{deJonge12natural} only stores the
relevant state at each line encountered to reduce this cost), whereas we add no
overhead at all to normal parsing; second, the search-space is so hugely
increased that it can be harder to find any repairs at
all~\cite{degano95comparison}.

One approach to global error recovery is to use machine
learning to train a system on syntactically correct programs~\cite{santos18syntax}: when a
syntax error is encountered, the resulting model is used to suggest appropriate global
fixes. Although \cite{santos18syntax} also use data from Blackbox, their
experimental methodology is both stricter -- aiming to find exactly the same
repair as the human user applied -- and looser -- they only
consider errors which can be fixed by a single token, discarding 42\% of
the data~\cite[p.~8]{santos18syntax}) whereas we attempt to fix errors which
span multiple tokens. It is thus difficult to directly compare their results to
ours. However, by the high bar they have set themselves, they are able to repair
52\% of single-token errors.

As \cpctplusrev shows, choosing an inappropriate repair sequence during
error recovery leads to cascading errors. The noncorrecting error recovery approach
proposed by~\cite{richter85noncorrecting} explicitly addresses this weakness,
eschewing repairs entirely. When a syntax error is discovered, noncorrecting
error recovery attempts to discover all further syntax errors by checking
whether the remaining input (after the point an error is detected) is a valid suffix in the
language. This is achieved by creating a recogniser that can identify all valid
suffixes in the language. Any errors identified in the suffix parse are
guaranteed to be genuine syntax errors because they are uninfluenced by
errors in the (discarded) prefix (though this does mean that some
genuine syntax errors are missed that would not have been valid
suffixes at that point in the user's input had the original syntax error not
been present). There seem to be two
main reasons why noncorrecting error recovery has
not been adopted. First, building an appropriate recogniser is surprisingly
tricky and we are not currently aware of one that can handle the full class of
LR grammars (though the full class of LL grammars has been
tackled~\cite{deudekom93initial}), though we doubt that this problem is
insoluble. Second, as soon as a syntax error is encountered, noncorrecting error
recovery is unable to execute semantic actions, since it lacks the execution context
they need.

Although one of our paper's aims is to find the complete set of minimum cost repair sequences,
it is unclear how best to present them to users, leading to questions such as:
should they be simplified? should a subset be presented? and so on. Although
rare, there are some surprising edge cases. For example,
the (incorrect) Java 1.7 expression `\texttt{x = f(""a""b);}' leads to 23,067 minimum
cost repair sequences being found, due to the large number of Java keywords that are
valid in several parts of this expression leading to a combinatorial explosion: even the most
diligent user is unlikely to find such a volume of information valuable. In
a different vein, the success condition of `reached an accept' state is
encountered rarely enough that users sometimes forget it exists at all: they
can then be surprised by an apparently unexplained difference in the repair sequences reported for
some syntax errors in the middle of a file versus its end. There is a
body of work which has tried to understand how best to structure compiler error
messages (normally in the context of those learning to program). However, the
results are hard to interpret: some studies find that more complex error
messages are not useful~\cite{nienaltowski08error}, while others suggest they
are~\cite{prather17novices}. It is unclear to us what the right approach might be,
or how it could be applied in our context.

The approach of \cite{mckenzie95error} is similar to
\corchuelo, although the former cannot incorporate shift
repairs. It tries harder than \cpctplus to prune out pointless search
configurations~\cite[p.~12]{mckenzie95error}, such as cycles in the parsing stack,
although this leads to some minimum cost repairs being
skipped~\cite{bertsch99failure}. A number of interlocking, sophisticated pruning
mechanisms which build on this are described in~\cite{cerecke03phd}. These are
significantly more complex than our merging of compatible configurations: since this
gives us acceptable performance in practise, we have not investigated other
pruning mechanisms.

The most radical member of the \fischer family is that
of~\cite{kimyi10astar}\footnote{In an earlier online draft of this paper we
stated that this algorithm has a fundamental flaw. We now believe this was
due to us incorrectly assuming that the `delete' optimisation of \corchuelo
applied to \cite{kimyi10astar}. We apologise to the authors for this
mistake.}. This uses the A* algorithm, and a precomputed distance table to
quickly generate repair sequences in the vein of \corchuelo.
\cite{kimyi10astar} works exclusively on the stategraph, assuming that it is
unambiguous. However, Yacc systems allow ambiguous stategraphs and provide a means for
resolving those ambiguities when creating the statetable. Many real-world
grammars (e.g.~Lua, PHP) make use of ambiguity resolution. In
an earlier online draft, we created \mf, an algorithm which
extends \cpctplus with ideas from \cite{kimyi10astar} at the cost of
significant additional complexity. With the benefit of hindsight, we do not
consider \mf's relatively small benefits (e.g.~reducing the failure rate by
approximately an additional 0.5\%) to be worth that
additional complexity.

\cpctplus takes only the grammar and token types into account. However,
it is possible to use additional information, such as nesting (e.g.~taking into
account curly brackets) and indentation when recovering from errors. This
has two aims: reducing the size of the search space (i.e.~speeding up error
recovery); and making it more likely that the repairs reported matched
the user's intentions. The most sophisticated
approach in this vein we are aware of is that of~\cite{deJonge12natural}. At
its core, this approach uses GLR parsing: after a grammar is suitably
annotated by the user, it is then transformed into a `permissive' grammar which
can parse likely erroneous inputs; strings which match the permissive parts of
the grammar can then be transformed into a non-permissive counterpart. In
all practical cases, the transformed grammar will be ambiguous, hence the
need for generalised parsing. There is an intriguing similarity between
our approach and that of~\cite{deJonge12natural}: our use of graph-structured
stacks in configuration merging (see Section~\ref{corchuelo:allminimumcost})
gives that part of our algorithm a similar feel to GLR parsing (even though
we never generate ambiguous strings). However, there are also major
differences: LR parsers are significantly simpler to implement than GLR parsers;
and the \fischer family of algorithms do not require manually
annotating, or statically increasing the size of, the grammar.

The approach we have taken in this paper can only repair errors on files which
are fully lexed. Since many users are unaware of the distinction between these
two stages, this can cause confusion: a minor lexing error does not lead to any
parsing errors. Looked at another way, fixing a single lexing error can,
surprisingly, lead to a slew of parsing errors being reported. Scannerless
parsing~\cite{salomon89scannerless} is one solution to this problem, since there
is no distinction between lexing and parsing. However, scannerless parsing
introduces new trade-offs: it is inherently ambiguous (e.g.~is `if' an
identifier or a keyword?); ambiguity is, in general, undecidable
and even the best ambiguity heuristics
fail to find all ambiguous parts of a grammar~\cite{vasudevan13detecting};
and resolving those ambiguities can make the parser context
sensitive~\cite{eijck__lets_accept_rejects}. Other possibilities are to
intermingle parsing and lexing (see e.g.~\cite{wyk07context}) or to allow
`fuzzy' or partial matching of tokens (see~\cite[p.~8]{vanter00displaying}).

A different approach to error recovery is that taken by~\cite{pottier16reachability}: rather
than try and recover from errors directly, it reports in natural language
how the user's input caused the parser to reach an error state (e.g.~``I
read an open bracket followed by an expression, so I was expecting a close
bracket here''), and
possible routes out of the error (e.g.~``A function or variable declaration is
valid here''). This involves significant manual work, as every parser state
(1148 in the Java grammar we use) in which an error can occur needs to be
manually marked up, though the approach has
various techniques to lessen the problem of maintaining messages as a grammar
evolves.

Many compilers and editors have hand-written parsers with hand-written error
recovery. Though generally ad-hoc in their approach, it is possible, with
sufficient effort, to make them perform well. However, this comes at
a cost. For example, the hand-written error recovery routines in the Eclipse IDE
are approximately 5KLoC and are solely for use with Java code: \cpctplus
is approximately 500LoC and can be applied to any LR grammar.

Although error recovery approaches have, historically, been mostly LR based,
there are several non-LR approaches. A full overview is impractical, though a
few pointers are useful. When LL approaches encounter an error, they generally
skip input until a token in the follow set is encountered (an early example
is~\cite{turner77error}). Although this outperforms the simple panic mode of
Section~\ref{sec:panic mode}, it will, in general, clearly skip more input than
\cpctplus, which is undesirable. LL parsers do, however, make it
somewhat easier to express grammar-specific error recovery rules.
The most advanced LL approach that we are aware of is
IntelliJ's Grammar-Kit, which allows user to annotate their grammars for error
recovery. Perhaps the most interesting annotation is that certain rules can be
considered as fully matched even if only a prefix is matched (e.g.~a partially
completed function is parsed as if it was complete). It might be possible to add
similar ideas to a successor of \cpctplus, though this is more
awkward to express in an LR approach. Error recovery for PEG grammars is much more
challenging, because the non-LL parts of the grammar mean that there is not
always a clearly defined point at which an error is determined to have
occurred. PEG error recovery has thus traditionally required extensive manual
annotations in order to achieve good quality recovery. \cite{medeiros19syntax}
is the most advanced work we are aware of that tackles the problem of lessening
the need for manual annotations for PEG error recovery. It does this by
automatically adding many (though not necessarily all) of the annotations
needed for good error recovery. However, deciding when to add, and when not to
add, annotations is a difficult task and the two algorithms presented have
different trade-offs: the \emph{Standard} algorithm adds more annotations,
leading to better quality error recovery, but is more likely to change
the input language accepted; the \emph{Unique}
algorithm adds fewer annotations, leading to poorer quality error recovery, but
does not affect the language accepted. The quality of error recovery of the
Unique algorithm, in particular, is heavily dependent on the input grammar: it works
well on some (e.g.~Pascal) but less well on others (e.g.~Java). In cases
where it performs less well, it can lead to parsers which skip large portions
(sometimes the remainder) of the input.

While the programming language world has largely forgotten the approach of
\cite{aho72minimum}, there are a number of successor works, most recently that
of~\cite{rajasekaran16error}. These improve on the time complexity, though none
that we are aware of address the issue of how to present what has been done to
the user.

We are not aware of any error recovery algorithms that are formally verified.
Indeed, as shown in this paper, some have serious flaws. We are only aware of
two works which have begun to consider what correctness for such algorithms might
mean:~\cite{zaytsev14formal} provides a brief philosophical justification of the need
and~\cite{gomezrodriguez10error} provides an outline of an approach. Until
such time as someone verifies a full error recovery algorithm, it is difficult
to estimate the effort involved, or what issues may be uncovered.


\section{Conclusions}

In this paper we have shown that error recovery algorithms in the
\fischer family can run fast enough to be usable in the real
world, and that they produce significantly better results than traditional
panic mode. Furthermore, extending such algorithms to produce the complete set of
minimum cost repair sequences allows parsers to provide better feedback to
users, as well as significantly reducing the cascading error problem.
The \cpctplus algorithm is simple to implement (less than 500LoC in our
Rust system) and still has good performance.

Looking to the future, we (perhaps immodestly) suggest that \cpctplus might be `good enough'
to serve as a common representative of the \fischer family.
However, we do not think that it is the perfect solution. We suspect
that, in the future, multi-phase solutions will be developed. For example, one
may use noncorrecting error recovery (e.g.~\cite{richter85noncorrecting})
to identify syntax errors, and then use a combination of machine-learning
(e.g.~\cite{santos18syntax}) and \cpctplus to discover repair sequences
that allow parsing to only error at those places.

\textbf{Acknowledgements:}
We are grateful to the Blackbox developers for allowing us access
to their data, and particularly to Neil Brown for help in extracting a relevant
corpus. We thank Edd Barrett for helping to set up our benchmarking
machine and for comments on the paper. We also thank Carl Friedrich
Bolz-Tereick, Sérgio Queiroz de Medeiros, Sarah Mount, François Pottier, Christian Urban,
and Naveneetha Vasudevan for comments on the paper.
This research was funded by the EPSRC Lecture (EP/L02344X/1) fellowship.

\bibliographystyle{plain}
\bibliography{bib}

\appendix

\section{Curated examples}
\label{app:examples}

In this section we show several examples of error recovery using \cpctplus in
different languages, to give some idea of what error recovery looks like in
practise, and to emphasise that the algorithms in this paper are grammar
neutral.

\subsection{Java 7}

\noindent Example 1 input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex1.java}

\noindent Example 1 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex1.out}

\vspace{12pt}

\noindent Example 2 input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex2.java}

\noindent Example 2 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex2.out}

\vspace{12pt}

\noindent Example 3 (taken from \cite[p.~10]{deJonge12natural}) input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex3.java}

\noindent Example 3 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex3.out}

\vspace{12pt}

\noindent Example 4 (taken from \cite[p.~16]{deJonge12natural}) input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex4.java}

\noindent Example 4 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex4.out}


\vspace{12pt}

\noindent Example 5 (taken from \cite[p.~2]{medeiros18syntax}):
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex5.java}

\noindent Example 5:
\lstinputlisting[xleftmargin=2em]{examples/java_ex5.out}


\vspace{12pt}

\noindent Example 6:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex6.java}

\noindent Example 6 output, showing the timeout being exceeded and error recovery
unable to complete:
\lstinputlisting[xleftmargin=2em]{examples/java_ex6.out}



\subsection{Lua 5.3}

\noindent Example 1 input:
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex1.lua}

\noindent Example 1 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex1.out}

\vspace{12pt}

\noindent Example 2 input. Note that `\texttt{=}' in Lua is the assignment
operator, which is not valid in conditionals; and that if/then/else blocks must
be terminated by `\texttt{end}'.
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex2.lua}

\noindent Example 2 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex2.out}

\vspace{12pt}

\noindent Examples 3 and 4 (both derived from the Lua 5.3 reference manual) shows
that \cpctplus naturally deals with an inherent ambiguity in Lua's Yacc
grammar involving function calls and assignments (which, following the Lua
specification, is resolved by Yacc in favour of function calls). This
example shows the `unambiguous' case (i.e.~if Lua forced users to use `;'
everywhere, the grammar would have no ambiguities):

\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex3.lua}

\noindent Example 3 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex3.out}


\vspace{12pt}

\noindent Example 4 shows what happens in the `ambiguous' case (which Lua's
grammar resolves in favour of viewing the code below as a function call to
\texttt{c}):

\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex4.lua}

\noindent Example 4 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex4.out}


\vspace{12pt}

\noindent Example 5 (taken from \cite[p.~7]{medeiros18syntax}):

\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex5.lua}

\noindent Example 5 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex5.out}


\subsection{PHP 7.3}

\noindent Example 1 input:
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/php_ex1.php}

\noindent Example 1 output:
\lstinputlisting[xleftmargin=2em]{examples/php_ex1.out}

\vspace{12pt}

\noindent Example 2 input:
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/php_ex2.php}

\noindent Example 2 output:
\lstinputlisting[xleftmargin=2em]{examples/php_ex2.out}

\vspace{12pt}

\noindent Example 3 input:
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/php_ex3.php}

\noindent Example 3 output:
\lstinputlisting[xleftmargin=2em]{examples/php_ex3.out}

\begin{figure}[t]
\includegraphics[scale=.7]{cpctplus_cpctplusrev_error_locs_histogram_full.pdf}
\caption{The full histogram of the number of error locations. The small number
of outliers obscures the main bulk of the data -- see
Figure~\ref{fig:results:cpctplus_cpctplusrev_histogram} for the truncated
version.}
\label{fig:results:cpctplus_cpctplusrev_histogram_full}
\end{figure}

\end{document}
