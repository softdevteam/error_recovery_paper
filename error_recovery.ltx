\documentclass[acmsmall,small,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{multicol}
\usepackage[autolanguage]{numprint}
\usepackage{proof}
\usepackage{softdev}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xspace}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

\lstset{
    basicstyle=\ttfamily\scriptsize,
    xleftmargin=0pt,
    numbersep=.8em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

% from https://tex.stackexchange.com/questions/264361/skipping-line-numbers-in-lstlisting#264373
\let\origthelstnumber\thelstnumber
\makeatletter
\newcommand*\Suppressnumber{%
  \lst@AddToHook{OnNewLine}{%
    \let\thelstnumber\relax%
     \advance\c@lstnumber-\@ne\relax%
    }%
}

\newcommand*\Reactivatenumber[1]{%
  \setcounter{lstnumber}{\numexpr#1-1\relax}
  \lst@AddToHook{OnNewLine}{%
   \let\thelstnumber\origthelstnumber%
   \refstepcounter{lstnumber}
  }%
}

\setcopyright{none}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}

% DOI
%\acmDOI{0000001.0000001}

% Paper history
%\received{February 2007}

\newcommand{\cpctplus}{\textrm{\textit{CPCT}}$^+$\xspace}
\newcommand{\crarrow}{\rightarrow_{\textrm{\tiny CR}}}
\newcommand{\crarrowstar}{\rightarrow_{\textrm{\tiny CR}}^*}
\newcommand{\kyarrow}{\rightarrow_{\textrm{\tiny KY}}}
\newcommand{\lrarrowstar}{\rightarrow_{\textrm{\tiny LR}}^*}
\newcommand{\lrarrow}{\rightarrow_{\textrm{\tiny LR}}}
\newcommand{\mf}{\textrm{\textit{MF}}\xspace}
\newcommand{\mfrev}{\textrm{\textit{MF$_\textrm{rev}$}}\xspace}
\newcommand{\mfarrow}{\rightarrow_{\textrm{\tiny MF}}}
\newcommand{\mfarrowstar}{\rightarrow_{\textrm{\tiny MF}}^*}
\newcommand{\panic}{\textrm{\textit{Panic}}\xspace}

\include{experimentstats}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title{Don't Panic! Reducing Cascading Syntax Errors in LR Parsers}

\author{Lukas Diekmann}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}}
\author{Laurence Tratt}
\orcid{0000-0002-5258-3805}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}
}
\thanks{Authors' URLs: %
    L.~Diekmann~\url{http://diekmann.co.uk/},
    L.~Tratt~\url{http://tratt.net/laurie/}.
}


\begin{abstract}
Syntax errors are generally easy to fix for humans, but not for parsers, in general,
and LR parsers, in particular. Traditional `panic mode' error recovery, though
easy to implement and applicable to any grammar,
often leads to a cascading chain of
errors that drown out the original. More advanced error recovery techniques
suffer less from this problem but have seen little practical use because
their typical performance was seen as poor, their worst case unbounded, and the
repairs they reported arbitrary. In this paper we show two generic error
recovery algorithms that fix all three problems. First, our algorithms report
the complete set of possible repair sequences for a given location, allowing
programmers to select the one that best fits their intention. Second, on a
corpus of \corpussize real-world syntactically invalid Java programs, we show
that our best performing algorithm is able to repair \mfsuccessrate files within
a cut-off of 0.5s. Furthermore, we are also able to
substantially reduce the cascading error problem, with our best performing
algorithm reporting \mferrorlocs error locations in the corpus to the user, while the panic
mode algorithm reports \panicerrorlocs error locations: in other words, our
algorithms reduce the cascading error problem by well over half.
\end{abstract}

\keywords{Parsing, error recovery, programming languages}

\maketitle

\section{Introduction}

Programming is a humbling job, which requires acknowledging that we will make
untold errors in our quest to perfect a program. Most troubling are semantic
errors, where we intended the program to do one thing, but it does another. Less
troubling, but often no less irritating, are syntax errors, which are
(generally minor) deviances from the exacting syntax required by a compiler.
So common are syntax errors that parsers in modern compilers expect us to make several
in a single input. Rather than stop on the first syntax error encountered, they attempt
to \emph{recover} from it. This allows them to report, and us to fix, all our
syntax errors in one go.

When error recovery works well, it is a useful productivity gain. Unfortunately,
most current error recovery approaches are simplistic. The most common
grammar-neutral approach to error recovery are those algorithms described as
`panic mode' algorithms (e.g.~\cite[p.~348]{holub90compilerdesign}) which skip
input until the parser finds something it is able to parse. A more
grammar-specific variation of this idea is to skip input until a pre-determined
synchronisation token (e.g. `;' in Java) is
reached~\cite[p.~3]{degano95comparison}, or to try inserting a single
synchronisation token. Such strategies are often unsuccessful,
leading to a
cascade of spurious syntax errors (see
Figure~\ref{fig:javaerror} for an example). Programmers quickly learn that
only the location of the first error in a file -- not the reported repair, nor the position of
subsequent errors -- can be relied upon to be accurate.

\begin{figure}[t]
\begin{minipage}{0.48\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:javaerror:input}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}[language=Java]
class C {
  int x y;
}
\end{lstlisting}
\end{minipage}
\\
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{1}
\caption{}
\label{lst:javaerror:mf}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Error at line 2 col 9. Repairs found:
  Delete "y"
  Insert "COMMA"
  Insert "EQ"
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\vspace{-20.5pt}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{-2}
\caption{}
\label{lst:javaerror:javac}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
C.java:2: error: ';' expected
  int x y;
       ^
C.java:2: error: <identifier> expected
  int x y;
         ^
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
\vspace{-10pt}
\caption{An example of a simple, common Java syntax error
(\subref{lst:javaerror:input}) and the problems traditional error recovery has in
dealing with it. \texttt{javac} (\subref{lst:javaerror:javac}) spots the error
when it encounters `\texttt{y}'. Its error recovery heuristic then
repairs the input by inserting a semicolon before `\texttt{y}' (i.e.~making
the input equivalent to `\texttt{int x; y;}'). This causes a (spurious) cascading
parsing error, since `\texttt{y}' on its own is not a valid statement. The two new error
recovery algorithms (\cpctplus and \mf) we introduce in this paper both
produce the output shown in (\subref{lst:javaerror:mf}): after spotting an error
when parsing encounters `\texttt{y}', they then use the Java grammar to find the
complete set of minimum cost repair sequences (unlike previous approaches which
non-deterministically find one minimum cost repair sequence). In this case three
repair sequences are reported to the user: one can delete `\texttt{y}'
entirely (`\texttt{int x;}'), or insert a comma
(`\texttt{int x, y;}'), or insert an equals sign (\texttt{`int x = y;'}).}
\label{fig:javaerror}
\end{figure}

A handful of parsers contain hand-written error recovery algorithms
for specific languages.
These generally allow better recovery from errors, but are difficult and
expensive to create. For example, the Java error recovery approach in the Eclipse IDE is 5KLoC long,
making it only slightly smaller than a modern version of Berkeley Yacc --- a
complete parsing system! Unsurprisingly, few real-world parsers contain
effective hand-written error recovery algorithms.

Most of us are so used to these trade-offs (cheap generic algorithms and poor
recovery vs.~expensive hand-written algorithms and reasonable recovery) that we
assume them to be inevitable. However, there is a long line of work on
more advanced generic error recovery algorithms. Probably the earliest such algorithm
is \citet{aho72minimum}, which, upon encountering an error, creates on-the-fly an
alternative (possibly ambiguous) grammar which allows the parser to recover.
However, this algorithm has fallen out of favour in programming language
circles, we suspect for two reasons: its implementation complexity; and the difficulty of
explaining to users what recovery has been used. A simpler family of algorithms, which
trace their roots to \citet{fischer79locally}, instead try to find a single minimum cost
\emph{repair sequence} of token insertions and deletions which allow the parser to
recover. Algorithms in this family are much better at recovering from errors
than naive approaches and can communicate the repairs they find in a way that
humans can easily replicate. However, such algorithms
have seen little practical use because their typical
performance is seen as poor and their worst case unbounded \cite[p.~14]{mckenzie95error}.
We add a further complaint to this mix: such approaches only report a single
repair sequence to users. In general -- and especially in syntactically rich
languages -- there are multiple reasonable repair sequences for a given error
location, and the algorithm has no way of knowing which best matches the user's intentions.

In this paper we introduce two new error recovery algorithms (in the
\citet{fischer79locally} mould) that run fast
enough to be usable, are able to repair nearly all errors, and which report the
complete set of repair sequences to users. The first algorithm, \cpctplus, takes
the approach of \citet{corchuelo02repairing} as a base, correcting and
substantially extending it. The second algorithm, \mf, takes partial inspiration
from the flawed algorithm of \citet{kimyi10astar} but is essentially a new
algorithm. Both algorithms have been designed to be as simple as possible, so
that they are realistic targets for tool builders: \cpctplus is the simpler of
the two, though \mf is still less than 1,000 lines of Rust code.

We then validate \cpctplus and \mf on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}). \cpctplus is
able to recover \cpctplussuccessrate of files within a 0.5s timeout, and \mf is
able to recover \mfsuccessrate. Significantly, both algorithms report well under
half as many error locations to the user as a traditional panic mode algorithm:
in other words, they substantially reduce the cascading error problem. We believe
that this shows that such approaches are ready for wider usage, either on their
own, or as part of a multi-phase recovery system.


\subsection{Defining the problem}

Formally speaking, we first test the following hypothesis:

\begin{description}
  \item[H1] The complete set of minimum cost repair sequences can
    be found in acceptable time.
\end{description}

The only work we are aware of with a similar concept of `acceptable time'
is~\citet{deJonge12natural}, who define it as the total time in spent in error
recovery per file, with a threshold of 1s. Since many compilers are able to
fully execute in less time than this, we felt that a tighter threshold is more
appropriate: we use 0.5s since we think that even the most
demanding user will tolerate such a delay. We strongly validate this hypothesis
with both \cpctplus (which successfully recovers \cpctplussuccessrate of files) and \mf (\mfsuccessrate). Relative
to previous approaches, we are the clear beneficiaries of faster modern
hardware, which undoubtedly make it easier to validate this hypothesis.
However, it it is important to note that we have
stated a much stronger hypothesis than previous approaches: where
they have aimed to find only a single minimum cost repair sequence, we
find the complete set of minimum cost repair sequences, a much more challenging
task.

The complete set of minimum cost repair sequences makes it much more likely that
the programmer will see a repair sequence that best matches their original
intention (see Figure~\ref{fig:javaerror} for an example; Appendix~\ref{app:examples}
contains further examples). It also opens up a
new opportunity for error recovery algorithms. Previous error recovery
algorithms find a single repair sequence, apply that to the input, and then
continue parsing. While that repair sequence may have been a reasonable local
choice, it may cause cascading errors later. Since we have the complete set of
minimum cost repair sequences available, we can instead choose a repair
sequence that we can show causes fewer cascading errors. We do this by
ranking repair sequences by how far they allow parsing to continue successfully
(up to a threshold --- parsing the whole file would, in general, be too costly),
and choose from the subset that gets furthest (note that the time required to do
this is included in the 0.5s timeout). We thus also test a second hypothesis:

\begin{description}
  \item[H2] The cascading error problem can be significantly reduced by ranking
the complete set of minimum cost repair sequences and choosing from those which
allow parsing to continue the furthest.
\end{description}

We also strongly validate this hypothesis. We do this by comparing `normal' \mf
with a simple variant \mfrev which reverses the ranking process, always selecting
from amongst the worst performing minimum cost repair
sequence. \mfrev models the worst case of previous approaches in the
\citet{fischer79locally} family, which non-deterministically select a single
minimum cost repair sequence. \mfrev leads to \mfreverrorlocsratioovermf more
errors being raised (i.e.~it substantially worsens the cascading error problem).

This paper is structured as follows.
We first use one of the more recent algorithms in this family -- that of
\citet{corchuelo02repairing} -- as a base, correcting and substantially extending it
to form a new algorithm \cpctplus (Section~\ref{corchueloplus}). We
then show that an even newer algorithm which promises better performance -- that
of \citet{kimyi10astar} -- has problems which cause it to miss many
minimum cost repair sequences (Section~\ref{kimyi}). However, we are able to use
it as partial inspiration for an entirely new error recovery algorithm \mf
(Section~\ref{mf}). We then validate \cpctplus and \mf on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}). To
emphasise that our algorithms are grammar-neutral, we show examples of
error recovery on different grammars in Appendix~\ref{app:examples}.


\section{Background}

We assume a high-level understanding of the mechanics of parsing in this paper,
but in this section we provide a handful of definitions, and a brief refresher
of relevant low-level details, needed to understand the rest of this paper.
Although the parsing tool we created for this paper is written in Rust, we
appreciate that this is still an unfamiliar language to most readers: code examples
are therefore given in Python which, we hope, is familiar to most.

Although there are many flavours of parsing, the \citet{fischer79locally} family
of error recovery algorithms are designed to be used with LR($k$)
parsers~\cite{knuth65lllr}. LR parsing remains one of the most widely used parsing
approaches due to the ubiquity of Yacc~\cite{johnson75yacc} and its
descendants (which include the Rust parsing tool we created for this paper).
We use Yacc syntax throughout this paper so that
examples can easily be tested in Yacc-compatible parsing tools.

Yacc-like tools take in a Context-Free Grammar (CFG) and produce a parser from
it. The CFG has one or more \emph{rules}; each rule has a name and one or more
\emph{productions} (often called `alternatives'); each production contains one
or more \emph{symbols}; and a symbol references either a \emph{token type}
or a grammar rule. One rule is designated the \emph{start rule}. The resulting parser
takes as input a stream of tokens, each of which has a \emph{type}
(e.g.~\texttt{INT}) and a \emph{value} (e.g.~\texttt{123}).\footnote{In practise, the
system we outline requires a \emph{lexer} which splits string inputs up into
tokens. In the interests of brevity, we assume the existence of a tool such as
Lex which performs this task.} Strictly speaking, parsing is the act of
determining whether a stream of tokens is correct with respect to the underlying
grammar. Since this is rarely useful on its own, Yacc-like tools allow grammars
to specify `semantic actions' which are executed when a production in the grammar is
successfully matched. In this paper, we assume that the semantic actions build
a \emph{parse tree}, ordering the tokens into a tree of nonterminal nodes
(which can have children) and terminal nodes (which cannot have children)
relative to the underlying grammar.

\begin{figure}[t]
\begin{minipage}{0.44\textwidth}
\begin{subfigure}{1.0\textwidth}
\begin{lstlisting}
%start Expr
%%
Expr: Term "+" Expr      // (I)
    | Term ;             // (II)

Term: Factor "*" Term    // (III)
    | Factor ;           // (IV)

Factor: "(" Expr ")"     // (V)
      | "INT" ;          // (VI)
\end{lstlisting}
\end{subfigure}
\begin{subfigure}{1.0\textwidth}
\vspace{10pt}
\scriptsize
\begin{tabular}{c c c c c c c}
\toprule
      & \multicolumn{6}{c}{Actions}\\
        \cmidrule{2-7}
$s$ & INT  & \texttt{+}    & \texttt{*}    & \texttt{(} & \texttt{)}    & \$   \\
\midrule
0     & S(4) &       &      & S(1) &        &      \\
1     & S(4) &       &      & S(1) &        &      \\
2     &      & S(7)  &      &      & R(II)  & R(II) \\
3     &      & R(IV) & S(8) &      & R(IV)  & R(IV) \\
4     &      & R(VI) & R(VI)&      & R(VI)  & R(VI) \\
5     &      &       &      &      &        & Accept \\
6     &      &       &      &      & S(9)   &      \\
7     & S(4) &       &      & S(1) &        &      \\
8     & S(4) &       &      & S(1) &        &      \\
9     &      & R(V)  & R(V) &      & R(V)   & R(V) \\
10    &      &       &      &      & R(I)   & R(I) \\
11    &      & R(III)&      &      & R(III) & R(III) \\
\bottomrule
\end{tabular}
\begin{subfigure}{1.0\textwidth}
\vspace{15pt}
\begin{tabular}{c c c c}
\toprule
      & \multicolumn{3}{c}{Gotos}\\
        \cmidrule{2-4}
$s$ & Term & Factor & Expr\\
\midrule
0     &  2  &   3    &  5  \\
1     &  2  &   3    &  6  \\
7     &  2  &   3    & 10  \\
8     & 11  &   3    &     \\
\bottomrule
\end{tabular}
\end{subfigure}
\end{subfigure}
\end{minipage}
\begin{minipage}{0.55\textwidth}
\begin{subfigure}{1.0\textwidth}
\vspace{-7pt}
\includegraphics[width=1.0\textwidth]{graph}
\end{subfigure}
\end{minipage}
\vspace{-5pt}
\caption{An example grammar (top left), its corresponding stategraph (right), and statetable
(split into separate action and goto tables; bottom left). Productions in
the grammar are labelled \texttt{(I)} to \texttt{(VI)}. In the stategraph: S($x$)
means `shift to state $x$'; R($x$) means `reduce production $x$ from the
grammar' (e.g.~\textit{action(3, `+')} returns R(IV) which references
the production `\texttt{Term: Factor;}').
  Each item within a state $[N \colon \alpha \bullet \beta]$ references one
  of rule $N$'s productions; $\alpha$ and $\beta$ each
represent zero or more symbols; with the \emph{dot} ($\bullet$) representing
  how much of the production must have been matched ($\alpha$) if parsing has
  reached that state, and how much remains ($\beta$).}
\label{fig:stategraphtable}
\label{fig:exprgrammar}
\end{figure}

The CFG is first transformed into a \emph{stategraph}, a statemachine
where each node contains one or more \emph{items} (describing the valid
parse states at that point) and edges are labelled with terminals or
nonterminals. Since even on a modern machine, a canonical
(i.e.~unmerged) LR stategraph for a real-world grammar takes several seconds to
build, and a surprising amount of memory to store, we use the state merging
algorithm of \citet{pager77practical} to merge together compatible
states.\footnote{Unfortunately \citet{pager77practical} can over-merge states when conflict
resolution is used \cite[p.~3]{denny10ielr} (i.e.~when Yacc uses its
precedence rules to turn an ambiguous input into an unambiguous LR parser).
Since our error recovery approach is intended to be independent of the merging
approach, it should be possible to use the more sophisticated state merging
approach of \cite{denny10ielr} without problems.} The
effect of this is significant, reducing the Java grammar we use later from
8908 to 1148 states. The stategraph is then transformed into a
\emph{statetable} with one row per state. Each row has a possibly empty \emph{action} (shift, reduce,
or accept) for each terminal and a possibly empty \emph{goto state} for each
nonterminal. Figure~\ref{fig:stategraphtable} shows an example grammar, its
stategraph, and statetable.

The statetable allows us to define a simple, efficient, parsing process. We
first define two functions relative to the statetable: \textsf{action}$(s, t)$
returns the action for the state $s$ and token $t$
or \emph{error} if no such action exists; and \textsf{goto}$(s, N)$
returns the goto state for the state $s$ and the nonterminal $N$ or \emph{error}
if no such goto state exists. We then define
a reduction relation $\lrarrow$ for $(\textit{parsing stack}, \textit{token
list})$ pairs with two reduction rules as shown in Figure~\ref{fig:lrreduction}.
A full LR parse $\lrarrowstar$ repeatedly applies the two $\lrarrow$ rules
until neither applies, which means that \textsf{action}$(s_n, t_0)$ is either:
$\textit{accept}$ (i.e.~the input has been fully parsed); or
$\textit{error}$ (i.e.~an error has been detected at the terminal $t_0$). A
full parse takes a starting pair of $([0], [t_0 \ldots t_n, \$])$,
where state $0$ is expected to represent the entry point into the stategraph, $t_0 \ldots t_n$
is the sequence of input tokens, and `$\$$' is the special End-Of-File (EOF) token.

\begin{figure}[t]
\small
\[
\infer[\textsc{LR Shift}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_n, s'], [t_1 \ldots t_n])}
      {\textsf{action}(s_n, t_0) = \textit{shift}\ s'}
\]
\vspace{-3pt}
\[
\infer[\textsc{LR Reduce}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'], [t_0 \ldots t_n])}
      {(\textsf{action}(s_n, t_0) = \textit{reduce}\ N \colon \alpha)
       \wedge (\textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s')}
\]
\vspace{-15pt}
\caption{Reduction rules for $\lrarrow$, which operate on $(\textit{parsing stack},
\textit{token list})$ pairs. \textsc{LR Shift}
advances the input by one token and grows the parsing stack, while
\textsc{LR Reduce} unwinds (`reduces') the parsing stack when a production is
complete before moving to a new (`goto') state.}
\label{fig:lrreduction}
\end{figure}


\section{Panic mode}
\label{sec:panic mode}

Error recovery algorithms are invoked by a parser when it has yet to finish but
there is no apparent way to continue parsing (i.e.~when \textsf{action}$(s_n,
t_0) = \textit{error}$). Error recovery algorithms are thus called with a parsing
stack and sequence of remaining input (which, for simplicities sake, we represent
as a list of tokens): they can modify either or both of the
stack and the input in their quest to get parsing back on track. The differences
between algorithms are thus in what modifications they can carry out
(e.g.~altering the parse stack; deleting input; inserting input), and how they
carry such modifications out.

The simplest grammar-neutral error recovery algorithms are called `panic mode'
algorithms. The precise origin of this family of algorithms seems lost in time;
there are also more members of this family for LL parsing than there are for
LR parsing. Indeed, for LR parsing, there appears to be only way of creating
a grammar-neutral panic mode algorithm: we take our formulation from
\citet[p.~348]{holub90compilerdesign}.\footnote{Note that step 2 in
\citet{holub90compilerdesign} causes valid repairs to be missed: while it is
safe to ignore the top element of the parsing stack on the first iteration of
the algorithm, as soon as one token is skipped, one must check all elements of
the parsing stack. Our description simply drops step 2 entirely.} It works by
taking the parsing stack and popping elements to
see if an earlier part of the stack is able to parse the next input symbol. If
no element in the stack is capable of parsing the next input symbol, the input
symbol is skipped, the stack restored, and the process repeated. At worst,
this algorithm guarantees to find a match at the EOF token.
Figure~\ref{fig:holub:algorithm} shows a more formal version of this algorithm.

\begin{figure}[tb]
\begin{adjustbox}{valign=t,minipage=.51\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def holub(pstack, toks):
  while len(toks) > 0:
    npstack = pstack[:]
    while len(npstack) > 0:
      if action(npstack[-1], toks[0]) != <!{\textrm{\textit{error}}}!>:
        return (npstack, toks)
      npstack.pop()
    del toks[0]
  return None
\end{lstlisting}
\end{adjustbox}
\begin{adjustbox}{valign=t,minipage=.47\textwidth}
\vspace*{-10pt}
\caption{Our version of the \citet{holub90compilerdesign} algorithm. The error
recovery algorithm takes in a (\emph{parsing stack}, \emph{token list})
pair and returns: a (\emph{parsing stack}, \emph{token list}) pair if
it managed to recover; or \texttt{None} if it failed to recover.
The algorithm tries to find an element in the stack that has a non-error action
for the next token in the input (lines 4--7). If it fails to find such an
element, the input is advanced by one element (line 8) and the stack restored
(line 3).}
\label{fig:holub:algorithm}
\end{adjustbox}
\end{figure}

The advantage of this algorithm is its simplicity and speed. For example,
consider the grammar from Figure~\ref{fig:exprgrammar} and the input `\texttt{2
+ + 3}'. The parser encounters an error on the second `+' token, leaving it with
a parsing stack of [0, 2, 7] and the input `\texttt{+ 3}' remaining. The error
recovery algorithm now starts. It first tries \textsf{action}(7, `+') which
(by definition, since it is the place the parser encountered an error) returns
\textit{error}; it then pops the top element from the parsing stack and tries
\textsf{action}(2, `+'), which returns \textit{shift}. This is enough for the
error algorithm to complete, and parsing resumes with a stack [0, 2].

The fundamental problem with error recovery can be seen from the above example:
the adjustment made to the parsing stack is not one that the user can
replicate. Looked at another way, error recovery is \emph{Deus ex
machina}: while panic mode managed to recover from the error, the only
general way to report what was done is to show the parsing stack
before and after recovery: this is challenging to interpret for small grammars
like that of Figure~\ref{fig:exprgrammar} and completely impractical for
anything larger. There is an important corollary to this: since the recoveries
made often don't match anything the user could have passed as input, the
recoveries made are often of poor quality, leading to a cascade of further
parsing errors (as we will see later in Section~\ref{results}).


\section{\cpctplus}
\label{corchueloplus}

There have been many attempts to create better LR error recovery algorithms than
panic mode. Most numerous are those error recovery algorithms in what we call the
\citet{fischer79locally} family. Indeed, there are far too many
members of this family of algorithms to cover in one paper. We therefore start
with one of the most recent -- \citet{corchuelo02repairing}. We first
explain the original algorithm (Section~\ref{corchuelo:orig}), although we use
different notation than the original, fill in several missing details, and
provide a more formal definition. We then
make two correctness fixes to ensure that the algorithm always
finds minimum cost repair sequences (Section~\ref{corchuelo:kimyi}). Since the original description
gives few details as to how the algorithm might best be implemented, we then
explain the steps we took to make a performant implementation
(Section~\ref{corchuelo:implementation}). We then show how the algorithm can
be extended to efficiently find the complete set of minimum cost repair sequences
(Section~\ref{corchuelo:allminimumcost}). This allows
us to make an algorithm less susceptible to the cascading error problem
(Section~\ref{cpctplus}): we refer to this final algorithm
as \cpctplus.


\subsection{The original algorithm}
\label{corchuelo:orig}

Intuitively, the \citet{corchuelo02repairing} algorithm starts at the error state and tries
to find a minimum cost repair sequence consisting of: \textit{insert T}
(`insert a token of type T'), \textit{delete} (`delete the token at the current offset'),
or \textit{shift} (`parse the token at the current offset'). The
algorithm completes: successfully if it reaches an accept state or shifts
`enough' tokens ($N_\textit{shifts}$, set at 3 in \citet{corchuelo02repairing});
or unsuccessfully if it deletes and inserts `too many' tokens ($N_\textit{total}$, set at 10
in \citet{corchuelo02repairing}). The algorithm keeps a queue of
\emph{configurations}, each of which represents a different search state;
configurations are searched for their neighbours until a successful configuration
is found. The cost of a configuration is the cumulative cost of the repairs
in its repair sequence.

\begin{figure}[tb]
\small
\[
\infer[\textsc{CR Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_0 \ldots t_n], [\textit{insert}~t])}
      {\textsf{action}(s_n, t) \ne error \wedge t \ne \${}
       \wedge ([s_0 \ldots s_n], [t, t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_0 \ldots t_n])}
\]
\vspace{-6pt}
\[
\infer[\textsc{CR Delete}]
      {([s_0 \ldots s_n], [t_0, t_1 \ldots t_n])
       \crarrow
       ([s_0 \ldots s_n], [t_1 \ldots t_n], [\textit{delete}])}
      {t_0 \ne \$}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 1}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n])
       \crarrow
       ([s'_0 \ldots s'_m], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_j \ldots t_n])
 \wedge 0 < j \leq N_\textit{shifts}
\\
 j = N_\textit{shifts} \vee
 \textsf{action}(s'_m, t_j) \in \{\textit{accept}, \textit{error}\} 
\end{array}}
\]
\vspace{-9pt}
\caption{The repair-creating reduction rules \cite{corchuelo02repairing}.
\textsc{CR Insert} finds all terminals reachable from the current state and
creates insert repairs for them (other than the EOF token `$\$$').
\textsc{CR Delete} creates deletion repairs if user defined input remains.
\textsc{CR Shift 1} parses at least 1 and at most $N_\textit{shifts}$ tokens; if it reaches an accept or error
state, or parses exactly $N_\textit{shifts}$ tokens, then a shift repair per
token shifted is created.}
\label{fig:corchuelo:reductions}
\vspace*{-8pt}
\end{figure}


\begin{figure}[tb]
\begin{adjustbox}{valign=t,minipage=.51\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def corchueloetal(pstack, toks):
  todo = [[(pstack, toks, [])]]
  cur_cst = 0
  while cur_cst < len(todo):
    if len(todo[cur_cst]) == 0:
      cur_cst += 1
      continue
    n = todo[cur_cst].pop()
    if action(n[0][-1], n[1][0]) == <!{\textrm{\textit{accept}}}!> \
       or ends_in_N_shifts(n[2]):
      return n
    elif len(n[1]) - len(toks) == N_total:
      continue
    for nbr in all_cr_star(n[0], n[1]):
      if len(n[2]) > 0 and n[2][-1] == <!{\textrm{\textit{delete}}}!> \
         and nbr[2][-1] == <!{\textrm{\textit{insert}}}!>:
        continue
      cst = cur_cst + rprs_cst(nbr[2])
      for _ in range(len(todo), cst):
        todo.push([])
      todo[cst].append((nbr[0], nbr[1], \
                        n[2] + nbr[2]))
  return None

def rprs_cst(rprs):
  c = 0
  for r in rprs:
    if r == <!{\textrm{\textit{shift}}}!>: continue
    c += 1
  return c

def all_cr_star(pstack, toks):
  # Exhaustively apply the <!{$\crarrowstar$}!> relation to
  # (pstack, toks) and return the resulting
  # list of (pstack, toks, repair) triples.
\end{lstlisting}
\end{adjustbox}
\begin{adjustbox}{valign=t,minipage=.47\textwidth}
\vspace*{-10pt}
\caption{Our version of the \citet{corchuelo02repairing} algorithm. The main function
\texttt{corchueloetal} takes in a (\emph{parsing stack}, \emph{token list})
pair and returns: a (\emph{parsing stack}, \emph{token list}, \emph{repair
sequence}) triple where \emph{repair sequence} is guaranteed to be a minimum
cost repair sequence; or \texttt{None} if it failed to find a repair sequence.\\[9pt]
%
The algorithm maintains a todo list of lists: the first sub-list contains
configurations of cost 0, the second sub-list configurations of cost 1, and
so on. The todo list is initialised with the error parsing stack, remaining
tokens, and an empty repair sequence (line 2). If there are todo items left, a
lowest cost configuration $n$ is picked (line 4--8). If $n$ represents an accept state (line
9) or if the last $N_\textit{shifts}$ repairs are shifts (line 10), then $n$
represents a minimum cost repair sequence and the algorithm terminates
successfully (line 11). If $n$ has already consumed $N_\textit{total}$ tokens,
then it is discarded (lines 12, 13). Otherwise, $n$'s neighbours
are gathered using the $\crarrow$ relation (lines 14, 32--35). To avoid
duplicate repairs, \textit{delete} repairs never follow \textit{insert} repairs
(lines 15--17). Each neighbour has its repairs costed (line 18) and is then
assigned to the correct todo sub-list (lines 21--22).\\[9pt]
%
The \texttt{rprs\_cst} function returns the cost of a repair sequence. Inserts
and deletes cost 1, shifts 0.}
\label{fig:corchuelo:algorithm}
\end{adjustbox}
\end{figure}

As with the original, we explain the approach in two parts. First is a new reduction relation
$\crarrow$ which defines a configuration's neighbours. Second is an algorithm which
makes use of the $\crarrow$ relation to generate neighbours, and determines when
a successful configuration has been found or if error recovery has failed.
As well as several changes for clarity, the biggest difference is that
Figure~\ref{fig:corchuelo:algorithm} captures semi-formally what
\citet{corchuelo02repairing} explain in prose (spread amongst
several topics over several pages): perhaps inevitably
we have had to fill in several missing details. For example,
\citet{corchuelo02repairing} do not define what the cost of repairs is: for
simplicities sake, we define the cost of \textit{insert} and \textit{delete} as
1, and \textit{shift} as 0.\footnote{It is trivial to extend this to variable
token costs if desired, and our implementation supports this. However, it is
unclear whether non-uniform token costs are useful in practise
\cite[p.96]{cerecke03phd}.}


\subsection{Ensuring that minimum cost repair sequences aren't missed}
\label{corchuelo:kimyi}

\textsc{CR Shift 1} has two flaws which prevent it from generating
all possible minimum cost repair sequences.

\begin{figure}[tb]
\small
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 2}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_j \ldots t_n])
 \wedge 0 \leq j \leq N_\textit{shifts}
\\
 (j = 0 \wedge [s_0 \ldots s_n] \ne [s'_0 \ldots s'_m]) \vee j = N_\textit{shifts} \vee
 \textsf{action}(s'_m, t_j) \in \{\textit{accept}, \textit{error}\}
\end{array}}
\]

\[
\infer[\textsc{CR Shift 3}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_j \ldots t_n], R)}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s_m], [t_j \ldots t_n])
 \wedge 0 \leq j \leq 1
\\
 (j = 0 \wedge [s_0 \ldots s_n] \ne [s'_0 \ldots s_m] \wedge R = [])
  \vee
  (j = 1 \wedge R = [\textit{shift}])
\end{array}
}
\]
\vspace{-16pt}
\caption{\textsc{CR Shift 1} always consumes input, when sometimes performing
one or more reduction/gotos without consuming input would be better. \textsc{CR
Shift 2} addresses this issue. Both \textsc{CR Shift 1} and \textsc{CR Shift 2}
generate multiple shift repairs in one go, which causes them to skip
`intermediate' (and sometimes important) configurations. \textsc{CR Shift 3}
generates at most one shift, exploring all intermediate configurations.}
\label{fig:corchuelo:kimyi}
\vspace*{-6pt}
\end{figure}


\begin{figure}[t]
\hspace*{-7pt}
\begin{tabular}{llll}
\begin{subfigure}{0.012\textwidth}
\vspace{-31.5pt}
\caption{}
\label{lst:crshift2:crshift2}
\end{subfigure}
&
\begin{lstlisting}
Delete "3", Delete "+"
Delete "3", Shift "+", Insert "INT"
Insert "PLUS", Shift "3", Shift "+", Insert "INT"
Insert "MULT", Shift "3", Shift "+", Insert "INT"
\end{lstlisting}
&
\begin{subfigure}{0.012\textwidth}
\vspace{-31.5pt}
\caption{}
\label{lst:crshift2:crshift3}
\end{subfigure}
&
\begin{lstlisting}
<! \vspace{-27pt} !>
Insert "MULT", Shift "3", Delete "+"
Insert "PLUS", Shift "3", Delete "+"
\end{lstlisting}
\end{tabular}
\vspace{-8pt}
\caption{Given the input `\texttt{2 3 +}' and the grammar from
Figure~\ref{fig:exprgrammar}, \textsc{CR Shift 1} is unable to find any repair
sequences because it does not perform the reductions/gotos necessary after
the final \textit{insert} or \textit{delete} repairs to reach an accept state.
\textsc{CR Shift 2} can find 4 minimum cost repair sequences
(\subref{lst:crshift2:crshift2}). \textsc{CR Shift 3} can find a further 2
minimum cost repair sequences on top of these (i.e.~6 in total)
(\subref{lst:crshift2:crshift3}).}
\label{fig:crshift2:example}
\end{figure}

First, \textsc{CR Shift 1} requires at least one token to be shifted. However,
after a non-shift repair, all that may be needed to reach a useful next configuration,
or an \textit{accept} state, is one or more reductions/gotos via
\textsc{LR Reduce}. \textsc{CR Shift 2} in Figure~\ref{fig:corchuelo:kimyi}
shows the two-phase fix which addresses this problem.
We first change the condition $0 < j \leq N_\textit{shifts}$ to $0 \leq j \leq
N_\textit{shifts}$ (i.e.~we don't force the LR parser to consume any tokens).
However, this then opens the possibility of an infinite loop. We avoid this by
saying that, if the input is not advanced, the parsing stack must have changed.
Put another way, in either case we require progress
to be made, even if that progress does not require consuming any input.

Second, \textsc{CR Shift 1} and \textsc{CR Shift 2} generate multiple shifts
at a time. This causes them to skip intermediate
configurations from which minimum cost repair sequences may be found.
The solution\footnote{The problem, and the basis of a fix, derive from
\cite[p.~12]{kimyi10astar}, though their suggestion suffers from the same
problem as \textsc{CR Shift 1}.} is simple: at most one shift can be generated at any one
time. \textsc{CR Shift 3} in
Figure \ref{fig:corchuelo:kimyi} (as well as incorporating the fix from
\textsc{CR Shift 2}) generates at most one shift repair at a time. Relative to
\textsc{CR Shift 1}, it is simpler, though it also inevitably slows down the
search, as more configurations are tried.

The problems with \textsc{CR Shift 1}, in particular, can be severe.
Figure~\ref{fig:crshift2:example} shows an example input where \textsc{CR Shift
1} is unable to find any repair sequences, \textsc{CR Shift 2} some, and
\textsc{CR Shift 3} all minimum cost repair sequences.


\subsection{Implementation considerations}
\label{corchuelo:implementation}

The definitions we have given thus far do not obviously lead to
an efficient implementation and \citet{corchuelo02repairing} give few useful
hints. We found that two techniques were both
effective at improving performance while being simple to implement.

First, although \citet{corchuelo02repairing} do not refer to it as such,
it was clear to us that the most natural way to model the search is as an instance of
Dijkstra's algorithm. However, rather than use a general queue data-structure (probably based on a
tree) to discover which element to search next, we use a similar queue data-structure to
\citet[p.~25]{cerecke03phd}. This consists of one sub-list per cost (i.e.~ the
first sub-list contains configurations
of cost 0, the second sub-list configurations of cost 1 and so on).
Since we always know what cost we are currently investigating,
finding the next todo element requires only a single \texttt{pop}
(line 8 of Figure~\ref{fig:corchuelo:algorithm}). Similarly,
adding elements requires only an \texttt{append} to the relevant sub-list
(lines 18, 21, 22). This
data-structure is a good fit because costs in our setting are always small
(double digits is unusual for real-world
grammars) and each neighbour generated from a configuration with cost $c$ has
a cost $\geq c$.

\label{cactusconfigurations}
Second, we do not use lists to represent parsing stacks and repair sequences
as Figure~\ref{fig:corchuelo:algorithm} may suggest. We found
that this representation consumes noticeably more memory, and is slightly less
efficient, than using parent pointer trees (often called `cactuses').
Every node in such a tree has a reference to a single parent (or \texttt{null} for the
root node) but no references to child nodes. Since our implementation is written
in Rust -- a language without garbage collection -- nodes
are reference counted (i.e.~a parent is only freed when it is not in a todo list and
no children point to it). When the error recovery algorithm starts, it
converts the main parsing stack (a list) into a parent pointer tree; and
repair sequences start as empty parent pointer trees. The $\crarrow$ part
of our implementation thus operates exclusively on parent pointer trees.
Although this does mean that neighbouring configurations are scattered throughout
memory, the memory sharing involved seems to more than compensate for
poor cache behaviour; it also seems to be a good
fit with modern \texttt{malloc} implementations, which are particularly
efficient when allocating and freeing objects of the same size.
However, it is quite possible that a different representation would be
better for a garbage collected language.

One seemingly obvious further improvement is to parallelise the search. However,
we found that the nature of the problem means that parallelisation is more
tricky, and less productive, than might be expected. There are two related
problems: we cannot tell in advance if a given
configuration will have huge numbers of successors or none at all; and
configurations are, in general, searched for successors extremely quickly. Thus
if we attempt to seed threads with initial sets of configurations, some threads
quickly run out of work whilst others have ever growing queues. If,
alternatively, we have a single global queue then significant amounts of time
are spent adding or removing configurations in a thread-safe manner. This
suggests that the right approach is likely to be a combination of the two
approaches: threads would have a local queue which, if it gets too full,
would be partly emptied into a global queue, from which otherwise idle threads
can find new work. As we shall see in Section~\ref{experiment}, \cpctplus runs
fast enough that the additional complexity of such an approach is not, in our
opinion, justified.


\subsection{Finding all minimum cost repair sequences}
\label{corchuelo:allminimumcost}

The algorithm as described to this point non-deterministically completes as soon
as it has found a single minimum cost repair sequence.
In this section we show that it is possible to extend such an algorithm to
efficiently find and report the complete set of minimum cost repair sequences.

The basis of a solution is simple: when a repair sequence of cost $c$ is found to
be successful, we discard all repair sequences with cost $> c$, and continue
exploring configurations in cost $c$ (including, transitively, all neighbours that are
also of cost $c$). Each successful configuration is recorded and, when all configurations
in $c$ have been explored, the set of successful configurations is returned.
This significantly slows down the search because there may
may be many remaining configurations in $c$, which may, transitively, have many neighbours.

Our solution to this performance problem is to merge together \emph{compatible}
configurations on-the-fly, preserving their distinct repair sequences while
still reducing the search space. Two configurations are compatible if:

\begin{enumerate}
\item their
parsing stacks are identical,
\item they both have an identical amount of input remaining,
\item  and their repair sequences are compatible.
\end{enumerate}

Two repair sequences are compatible:

\begin{enumerate}
   \item if they both end in the same number ($n \ge 0$) of shifts,
   \item and, if one repair sequence ends in a delete, the other repair sequence also
ends in a delete.
\end{enumerate}

The first condition is a direct consequence of the fact that a configuration
is deemed successful if it ends in $N_\textit{shifts}$ shift
repairs. When we merge configurations, one part of the merge is `dominant'
(i.e.~checked for $N_\textit{shifts}$) and the other `subsumed'. Thus we have to
maintain symmetry between the dominant and subsumed parts to prevent the
dominant part accidentally preventing the subsumed part from being recorded as
successful. In other words, if the dominant part of the merge had fewer shifts
at the end of its repair sequence than the subsumed part, then the
$N_\textit{shifts}$ check (line 10, Figure~\ref{fig:corchuelo:algorithm}) would
fail, even though reversing the dominant and subsumed
parts may have lead to success. It is therefore only safe to merge repair sequences
which end in the same number of shifts.

The second condition relates to the weak form of compatible merging inherited
from \citet[p.~8]{corchuelo02repairing}: delete repairs are never followed by an
insert (see Figure~\ref{fig:corchuelo:algorithm}) since [\textit{delete},
\textit{insert x}] always leads to the same configuration as [\textit{insert x},
\textit{delete}]. Although we get much of the same effect through
compatible configuration merging, we keep it as a separate optimisation because: it is
such a frequent case; our
use of the todo list means that we would not catch every case; the
duplicate repair sequences are uninteresting from a user perspective, so we
would have to filter them out later anyway; and each additional merge costs
memory. We thus have to make sure that merged repair sequences don't accidentally
suppress insert repairs because one part of the repair sequence ends in a delete
while the other does not. The simplest way of solving this problem is thus simply to
forbid merging repair sequences if one sequence ends in a delete and the other does not.

Fortunately, implementing compatible configuration merging is simple. We
first modify the todo data-structure to be a
list-of-ordered-hashsets\footnote{An ordered hashset preserves insertion order,
and thus allows list-like integer indexing as well as hash-based lookups.}. This has
near-identical \texttt{append} / \texttt{pop} performance to a normal list, but
filters out duplicates with near-identical performance to an unordered hashset.
We then make use of a simply property of hashsets: an object's hash behaviour
need only be a non-strict subset of its equality behaviour. In other words,
while we need to ensure that two objects that compare equal always map to the
same hash, we can allow two objects that do not compare equal to map to the
same hash. In our context, this allows us to quickly find potentially compatible
nodes using hashing, checking for definitely compatible configurations
using equality. We therefore hash configurations based solely on their parsing stack
and remaining input whereas configuration equality is based on a configurations'
parsing stacks, remaining input, and repair sequences.  Since we only record configurations
which we have yet to search, we cannot detect all possible compatible merges.
Nevertheless, this is still a powerful optimisation.
An example of compatible configuration merging can be seen in
Figure~\ref{fig:cpctplus:full}.

Conceptually, merging two configurations together is simple: each configuration
needs to store a set of repair sequences, each of which is updated as further
repairs are found. However, this is an extremely inefficient representation as
the sets involved need to be copied and extended as each new repair is found.
Instead, we reuse the idea of graph-structured stacks from GLR
parsing~\cite[p.~4]{tomita87efficient} which allows us to avoid copying whenever
possible. The basic idea is that configurations no longer reference a parent pointer tree of
repairs directly, but instead a parent pointer tree of \emph{repair merges}. A
repair merge is a pair (\textit{repair}, \textit{merged}) where
\textit{repair} is a plain repair and \textit{merged} is a (possibly null) set of
repair merge sequences. This structure has two advantages. First, the
$N_\textit{shifts}$ check can be performed solely using the first element of
repair merge pairs. Second, we avoid allocating memory for configurations which
have not yet been subject to a merge. The small
downside to this scheme is that expanding configurations into repair sequences requires
recursively expanding both the normal parent pointer tree of the first
element as well as the merged parent pointer trees of the second element.


\subsection{Putting together the \cpctplus algorithm}
\label{cpctplus}
\label{rankingrepairs}

The \textsc{CR Shift 3} rule and our ability to find the complete
set of minimum cost repair sequences are two of the key ingredients
in our new error recovery algorithm. In this subsection
we make two further additions to the algorithm, calling the result \cpctplus (in
homage to \citet{corchuelo02repairing}).

\begin{figure}[t]
\includegraphics[width=1.0\textwidth]{cpctplus}
\caption{An elided visualisation of a real run of \cpctplus with the input
`2 3 +' and the grammar from Figure~\ref{fig:exprgrammar}. The left hand side of
the tree shows the `normal' parser at work, which hits an error as soon as it
has shifted the token `\texttt{2}': at this point, \cpctplus starts operating.
As this shows, the search encounters various dead ends, as well as successful
routes. As shown in Figure~\ref{fig:crshift2:example}, this input has 6 minimum
cost repair sequences, but the search only has 5 success configurations, because
two configurations were merged together.}
\label{fig:cpctplus:full}
\end{figure}

\begin{figure}[t]
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:ranking:java}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}[language=Java]
class C {
    T x = 2 +
    T y = 3;
}
\end{lstlisting}
\end{minipage}
&
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:ranking:output}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Error at line 3 col 7. Repairs found:
  Insert "COMMA"
\end{lstlisting}
\end{minipage}
\end{tabular}
\vspace{-10pt}
\caption{An example showing how the ranking of repair sequences can lessen the
cascading error problem. The Java example (\subref{lst:ranking:java}) leads
to a parsing error on line 3 at `\texttt{y}', with three minimum cost repair
sequences found: [\textit{insert} \texttt{,}], [\textit{insert} \texttt{?}], and
[\textit{insert} \texttt{(}]. These repair sequences are then ranked by how far
they allow parsing to continue successfully. [\textit{insert} \texttt{,}] leads
to the rest of the file being parsed without further error. [\textit{insert}
\texttt{?}] causes a cascading error at `\texttt{;}' which must then be resolved
by completing the ternary expression started by `?' (e.g.~changing line 3 to
`\texttt{T ? y : this;}'). Similarly, [\textit{insert} \texttt{(}] causes a
cascading error at `\texttt{;}' which must then be resolved by inserting a
`\texttt{)}'. Since [\textit{insert} \texttt{,}] is ranked more highly than the
other repair sequences, the latter are discarded, leading to the parsing output shown
in (\subref{lst:ranking:output}). javac in contrast attempts to insert
`\texttt{;}' before `\texttt{y}' causing a cascading error on the next token.}
\label{fig:ranking}
\end{figure}

The penultimate step in our new algorithm allows us to somewhat compensate for the
small value of $N_\textit{shifts}$. This value has to be a small integer (we
use 3, the value suggested by \citet{corchuelo02repairing})
because each additional token searched exponentially increases the
search space. Thus while the repair sequences we find are all good
within the range of $N_\textit{shifts}$, some, but not others, may perform
poorly beyond that range. Fortunately, we can use the
complete set of minimum cost repair sequences to lessen this weakness.
After we have generated the set of configurations which represents the
complete set of minimum cost repair sequences, we then rank the
configurations by how far they allow parsing to continue, up to a limit of
$N_\textit{try}$ tokens (which we somewhat arbitrarily set at 250). The reason
why we rank the configurations, and not the repair sequences, is that we only
need to rank one repair sequence for each merged configuration, a small but useful
optimisation. We then expand the top ranked configurations into repair
sequences, remove shifts from the end of repair sequences, and
remove any duplicate repair sequences. Figure~\ref{fig:cpctplus:full} shows a
visualisation of \cpctplus in action.

Particularly on real-world grammars, selecting the top-ranked repair sequences
substantially decreases cascading errors (see Figure~\ref{fig:ranking} for an example).
It also does so for very little additional computational cost, as the complete set of
minimum cost repair sequences is much smaller than the number of configurations
searched. However, it cannot entirely reduce the cascading error problem. Since,
from our perspective, each member of the top-ranked set is equivalently good, we
non-deterministically select one of its members to repair the input and allow
parsing to continue. This can mean that we select a repair sequence which
performs less well beyond $N_\textit{try}$ tokens than other repair sequences in
the top-ranked set.

The final part of \cpctplus relates to the use of $N_\textit{total}$ in
\citet{corchuelo02repairing}. As with all members of the
\citet{fischer79locally} family, \cpctplus is not only unbounded in
time~\cite[p.~14]{mckenzie95error}, but
also unbounded in memory. $N_\textit{total}$ is an attempt to stop the
algorithm from running unacceptably long by limiting how much input the
algorithm will consider for modification. Unfortunately it is impossible to find
a good value for this, as `too long' is entirely dependent on the grammar and
erroneous input: Java's grammar, for example, is large with a commensurately
large search space while Lua's grammar is small with a commensurately small
search space. This can be most easily seen on inputs with unbalanced brackets
(e.g.~expressions such as `\texttt{x = f(();}'): each additional unmatched
bracket exponentially increases the search space. On a modern machine
with a Java 7 grammar, \cpctplus takes about 0.3s to find
the complete set of minimum cost repair sequences for 3 unmatched brackets, 3s
for 4 unmatched brackets, and 6 unmatched brackets caused our 32GiB test machine
to run out of RAM. We believe that the only acceptable solution is a timeout: up
to several seconds is safe in our experience. For the purposes of this paper
this problem is solved for us, because we define a timeout of 0.5s to be the
longest acceptable to users.


\section{The Kim and Yi algorithm}
\label{kimyi}

While performance problems such as those with unmatched brackets
are fairly rare, it would be better if they did not occur at
all. \citet{kimyi10astar} propose a new error recovery algorithm -- which is in the
\citet{fischer79locally} family, but by far its most radical member --  which
claims to hugely reduce such performance problems. This work has not,
to the best of our knowledge, received prior attention in the community, despite
this promise. In this section we provide a brief overview of this work: we
give enough information to understand details relevant to this paper but
we elide several details which an
implementation would need to consider. We also adjust the algorithm's style to
match this paper's and correct several minor mistakes.

We then show that the algorithm contains three serious flaws
which cause it to miss minimum cost repair sequences
(Section~\ref{kimyi:flaws}). We are unable to fix all of these flaws, but parts of the
approach serve as inspiration for our new \mf error recovery algorithm.


\subsection{An overview of the algorithm}
\label{kimyi:overview}

The \citet{kimyi10astar} algorithm takes \citet{corchuelo02repairing} as a base,
adding two significant novelties: it uses the A*
algorithm~\cite{hart68astar} to delay, and thus often to avoid, unpromising
configurations; and it can insert non-terminals, avoiding many inserts
of terminals entirely. This allows examples with
thousands of unmatched brackets to be repaired in a few seconds.

Since the A* algorithm is not a particularly common one in parsers,
we first start with a brief overview of it. The A* algorithm finds minimum cost paths through
a graph, where each edge has an associated cost $c$. The current lowest
cost to reach a node $n$ from the start node is represented by $d(n)$. A
heuristic $h(n)$ returns an estimate of the additional cost needed to reach a success node from
$n$. The heuristic must be `admissible': it must
never overestimate the cost to reach a success node (or else non-minimum
cost routes to success nodes may be found first); however, it may safely
underestimate (i.e.~the simplest admissible heuristic is $h(n) = 0$). A priority queue is
used to order nodes by their $d(n) + h(n)$. On
each iteration the node in the queue with the lowest $d(n) + h(n)$ is selected,
its neighbours explored and each entered into the priority queue. The search
terminates when the first success node is found.

\begin{figure}
\small
\[
\infer[\textsc{KY Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n], w) \kyarrow ([s_0 \ldots s_n, s'],
       [t_0 \ldots t_n], \top, [\textit{insert}~t], d)}
      {s_n \xrightarrow{\text{~~t~~}} s' \wedge t \ne \${} \wedge d = \textsf{ky\_dist}(s', t_0) < \infty)}
\]
\vspace{-3pt}
\[
\infer[\textsc{KY Reduce}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n], \bot) \kyarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'],
       [t_0 \ldots t_n], \bot, [\textit{insert}~\beta_0 \ldots \textit{insert}~\beta_n], 0)}
      {[N: \alpha \bullet \beta_0 \ldots \beta_n] \in \textsf{core}(s_n)
       \wedge \textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s'}
\]
\vspace{-4pt}
\[
\infer[\textsc{KY Delete}]
      {(S, [t_0, t_1 \ldots t_n], \bot) \kyarrow (S, [t_1 \ldots t_n], \bot, [\textit{delete}], 0)}
      {t_0 \ne \$}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{KY Shift}]
      {(S, [t_0 \ldots t_n], w) \kyarrow (S', [t_1 \ldots t_n], \bot, [\textit{shift}], 0)}
      {(S, [t_0 \ldots t_n]) \lrarrowstar (S', [t_1 \ldots t_n])}
\]
\vspace{-12pt}
\caption{The repair-creating rules for \citet{kimyi10astar} operate from (\textit{parsing
stack}, \textit{token list}, \textit{w}) to (\textit{parsing stack},
\textit{token list}, \textit{w}, \textit{repairs}, \textit{heuristic}) tuples.
\textsc{KY Insert} finds terminals in the stategraph
($\xrightarrow{\text{~~t~~}}$) which lead to a state with a finite distance to
the next input token as \textit{insert} repairs. \textsc{KY Reduce} finds items
in the core (or `kernel') state which would lead to a reduction if the
sequence of symbols (terminals and non-terminals) $\beta_0 \ldots \beta_n$ were
to be found; it then optimistically creates insert repairs for each, and
performs the corresponding reduction. \textsc{KY Delete} is virtually identical
to \textsc{CR Delete}. \textsc{KY Shift} is similar to \textsc{CR Shift 3} but
has to shift a single symbol to avoid creating repairs which
duplicate those found by \textsc{KY Reduce}.}
\label{fig:kimyi:rules}
\end{figure}

The \citet{kimyi10astar} algorithm itself comes in two main parts. First is a
relation $\kyarrow$ which defines which neighbours can be reached a
configuration (Figure~\ref{fig:kimyi:rules}). Unlike the $\crarrow$ relation, these rules are
from (\textit{parsing stack}, \textit{token list}, \textit{w}) to
(\textit{parsing stack}, \textit{token list}, \textit{w}, \textit{repairs},
\textit{heuristic}) tuples. Of these values, $w$ is the least intuitive:
since, as we will shortly see, the A* heuristic used in this approach does not
take into account reductions/gotos or deletions,
heuristic values are only valid for sequences of \textsc{KY Insert} and \textsc{KY Shift}.
The initial configuration sets $w = \bot$. Since \textsc{KY Insert} is the only
rule which uses the heuristic, it sets $w = \top$. Only \textsc{KY Shift} can
turn $w = \top$ into $\bot$.

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.63\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def mk_ky_table(sgraph, terms, eof_idx):
  table = [<!$\infty$!>] * sgraph.len_states * len(terms)
  table[sgraph.accept_state * len(terms) \
        + eof_idx] = 0
  while True:
    chgd = False
    for i in range(sgraph.len_states):
      for sym, end_st in sgraph.edges(i):
        if isinstance(sym, Nonterminal):
          d = min_sentence_cost(sym.nt_idx)
        else:
          assert isinstance(sym, Terminal)
          off = i * len(terms) + sym.term_idx
          if table[off] != 0:
            table[off] = 0
            chgd = True
          d = 1
        for j in range(len(terms)):
          this_off = i * len(terms) + j
          end_off = end_st * len(terms) + j
          if table[end_off] != <!$\infty$!>:
            other_cost = table[end_off] + d
            if other_cost < table[this_off]:
              table[this_off] = other_cost
              chgd = True
    if not chgd:
      break
\end{lstlisting}
\end{adjustbox}%
\small
\begin{adjustbox}{valign=t,minipage=.35\textwidth}
\vspace{5pt}
\begin{tabular}{rcccccc}
\toprule
      & \multicolumn{6}{c}{$t$} \\
        \cmidrule(lr){2-7}
$s$ & INT & \texttt{+} & \texttt{*} & \texttt{(} & \texttt{)} & \$ \\
\midrule
0 & 0 & 1 & 1 & 0 & 2 & 1 \\
1 & 0 & 1 & 1 & 0 & 1 & $\infty$ \\
2 & 1 & 0 & 2 & 1 & 3 & $\infty$ \\
3 & 1 & 3 & 0 & 1 & 3 & $\infty$ \\
4 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
5 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & 0 \\
6 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & 0 & $\infty$ \\
7 & 0 & 1 & 1 & 0 & 2 & $\infty$ \\
8 & 0 & 2 & 1 & 0 & 2 & $\infty$ \\
9 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
10 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
11 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-5pt}
\caption{A fixed-point algorithm for the \textit{ky\_dist} distance table (left)
and a distance table for the grammar from Figure~\ref{fig:exprgrammar} (right).
The algorithm takes in the stategraph, an ordered list of
terminals, and the index of the `\$' terminal (line 1), and returns a table with
one column per terminal and one row per state. Each entry starts
at $\infty$ (line 2) except for the `\$' terminal in the accept state (i.e.~the only state with
an \textit{accept} action) which is set
to 0 (lines 3, 4): entries monotonically reduce to a minimum of 0. For each
state $i$ in the stategraph (line 7), the algorithm explores its outgoing edges
(via the \texttt{edges} function (line 8)), each of which is labelled with a
symbol \texttt{sym} and points to another state with index \texttt{end\_st}. If
the edge's symbol is a terminal then by definition the cost of reaching that
terminal from state $i$ is 0 (lines 13-16). We then calculate the cost of the
edge: if the label's symbol is a nonterminal, we return the cost of the minimum
sentence matching that nonterminal (line 10); if a terminal we return a cost of 1 (line 17).
If the cost of $t$ in \texttt{end\_st} plus the distance to \texttt{end\_st} is
lower than the cost of $t$ in $i$ then we update the latter (lines 19--25).}
\label{fig:kimyi:dist}
\end{figure}

The A* heuristic used is \textsf{ky\_dist(s, t)} which is best
summarised as follows: if we are in parsing state $s$, what is the cost of the
minimum route through the stategraph to a state $s'$ (where $s$ and $s'$ may be
the same state), where $s'$ has the terminal $t$ as an outgoing edge? In other
words, this gives us the cost of inserting symbols to reach a state $s'$
where $t$ is a valid next terminal. If no such route
exists, \textsf{ky\_dist} returns $\infty$. \citet{kimyi10astar} provide a terse, abstract
specification of the algorithm; we provide a fixed-point algorithm which
calculates the underlying table. Note that we
assume the existence of a function \textsf{min\_sentence\_cost(N)} which returns
the length of the minimum sentence(s) which match the non-terminal $N$: this
function is a relatively simple variation of the traditional nullable
computation used in parsing.


\subsection{Problems with the algorithm}
\label{kimyi:flaws}

The combination of the $\kyarrow$ rules (in particular \textsc{KY Reduce}'s
ability to insert nonterminals) and the \textsf{ky\_dist} function lead to dramatic
performance improvements as reported by \citet{kimyi10astar}. Unfortunately the
algorithm contains three flaws which lead it to produce incorrect
results.

The first flaw is that the \textsf{ky\_dist} heuristic only tells us the cost of
inserting tokens to reach a state with the desired outgoing edge. A simple example of this can be seen for the grammar
from Figure~\ref{fig:exprgrammar} and its distance table in Figure~\ref{fig:kimyi:dist}:
\textsf{ky\_dist(3, `+')} returns 3, which can be achieved by inserting
`\texttt{*}', `\texttt{(}', and `\texttt{INT}', leaving the parser in state 2.
However, from state 3, we can also reach state 2 without inserting any symbols
at all through a reduction to state 0 and a corresponding goto state 2.
The second flaw results from trying to fix the
first: the $w$ part of the $\kyarrow$ relation is a hack to ensure that
\textsc{KY Reduce} and \textsc{KY Delete} do not interfere when the heuristic is $> 0$.
However, the hack has unfortunate effects, sometimes trapping the search in
parsing states from which it cannot escape, and sometimes stopping it from
searching states which would produce a lower cost repair sequence.
For example, given the grammar \texttt{S: T 'b' 'c'; T: 'a';} and the input `c',
\citet{kimyi10astar} can find no repairs (whereas \cpctplus finds [\textit{insert a}, \textit{insert b}]).
Given the grammar \texttt{S: 'a' 'b' 'd' | 'a' 'b' 'c'
'a' 'a' 'd';} and the input `a c d', the algorithm incorrectly returns the repair sequence
[\textit{insert b}, \textit{shift}, \textit{insert a}, \textit{insert a}]
(whereas \cpctplus finds the minimum cost repair sequence
[\textit{insert b}, \textit{delete}]).
The third and final flaw is that optimistically inserting the remainder of a
production in \textsc{KY Reduce} prevents the terminals involved from being shifted if the user's
input happens to overlap with them: since it is more expensive to insert $a$
than to shift $a$, this can cause the search to generate non-minimum cost repair
sequences.

The third flaw is easily fixed by disallowing the search from optimistically
inserting the remainder of a production. The first two flaws can then be fixed
by revisiting an old friend: if we apply the same change to
\textsc{KY Shift} as we did to \textsc{CR Shift} (see
Figure~\ref{fig:corchuelo:kimyi}), the problem disappears, because
the altered \textsc{KY Shift} can perform reductions/gotos without having to
consume input. Unfortunately applying the \textsc{CR Shift 3} fix
to \textsc{KY Shift} turns the \citet{kimyi10astar} algorithm into a slower
version of \citet{corchuelo02repairing}, since reductions/gotos are now duplicated
between \textsc{KY Reduce} and \textsc{KY Shift}. Removing \textsc{KY Reduce}
turns the algorithm into an almost literal copy of \citet{corchuelo02repairing},
with the mostly minor difference that \textsc{CR Insert} operates on the
statetable and \textsc{KY Insert} on the stategraph.\footnote{Though note that
for parsers which allow conflict resolution, such as Yacc, this means that
\textsc{KY Insert} can produce results which can't be parsed by the
statetable~\cite[p.~53, 54]{cerecke03phd}.} We have been
unable to find fixes to the algorithm that maintain its claimed performance
properties.


\section{\mf}
\label{mf}

In this section we present a new recovery algorithm \mf. As with \cpctplus,
\mf finds the complete set of minimum cost repair sequences, although it does so
using the A* algorithm. While
\cpctplus and \mf find precisely the same repair sequences, \mf does so
slightly quicker. However, \mf requires more up-front calculations that require slightly
more implementation effort: \mf is approximately 800LoC whereas \cpctplus
is approximately 450LoC.

We first provide an overview of the algorithm
(Section~\ref{mf:overview}) before describing in detail the steps needed to
calculate the new \texttt{mf\_dist} heuristic (Section~\ref{mf:mf_dist}).


\subsection{An overview of \mf}
\label{mf:overview}

\begin{figure}[t]
\small
\[
\infer[\textsc{MF Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow ([s_0 \ldots s_n, s'], [t_0 \ldots t_n], [r_0 \ldots r_n, \textit{insert}~t], d)}
      {\textsf{action}(s_n, t) = \textit{shift}~s' \wedge t \ne \${}
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n, \textit{insert}~t], s', [t_0 \ldots t_n]) < \infty}
\]
\vspace{-3pt}
\[
\infer[\textsc{MF Reduce}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'],
       [t_0 \ldots t_n], [r_0 \ldots r_n], d)}
      {N: \alpha \in \textsf{core\_reduces}(s_n)
       \wedge \textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s'
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n], s', [t_0 \ldots t_n]) < \infty}
\]
\vspace{-4pt}
\[
\infer[\textsc{MF Delete}]
      {([s_0 \ldots s_n], [t_0, t_1 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow
       ([s_0 \ldots s_n], [t_1 \ldots t_n], [r_0 \ldots r_n, \textit{delete}], d)}
      {t_0 \ne \$
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n, \textit{delete}], s_n, [t_1 \ldots t_n]) < \infty}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{MF Shift}]
      {([s_0 \ldots s_n], [t_0, t_1 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow
       ([s_0 \ldots s_n, s'], [t_1 \ldots t_n], [r_0 \ldots r_n, \textit{shift}], d)}
      {\textsf{action}(s_n, t_0) = \textit{shift}~s'
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n, \textit{shift}], s', [t_1 \ldots t_n]) < \infty}
\]
\vspace{-12pt}
\caption{The repair-creating rules for \mf from (\textit{parsing stack},
\textit{token list}, \textit{repair sequence}) configurations to (\textit{parsing stack},
\textit{token list}, \textit{repair sequence}, \textit{heuristic}) tuples.
\textit{mf\_dist} is the \mf A*
heuristic: unlike \citet{kimyi10astar}, every rule makes use of the \textit{mf\_dist} heuristic.
\textit{core\_reduces}($s_n$) returns the set of non-terminals which can be
reduced in state $s_n$.}
\label{fig:mf:rules}
\end{figure}

At a high level -- and much of the low level -- our description of \mf is
deliberately similar to \cpctplus, hopefully allowing the reader
to both easily digest \mf and pick out the differences from \cpctplus.

We introduce a new reduction relation $\mfarrow$, defining a neighbour's configurations
(Figure~\ref{fig:mf:rules}. Their most obvious features are that the relation is
from \textit{(parsing stack, token list, repair sequence)} to \textit{(parsing
stack, token list, repair sequence, heuristic)} tuples and that each rule
uses a new A* heuristic \textsf{mf\_dist} (which takes into account reductions/gotos
and deletions; see Section~\ref{mf:mf_dist}). Less obviously, the rules do not
use the $\lrarrow$ relation: \textsc{MF Shift} and \textsc{MF Reduce} subsume
the functionality of \textsc{LR Shift} and \textsc{LR Reduce} (making this aspect of \mf
closer in spirit to \citet{mckenzie95error} than either \citet{corchuelo02repairing}
or \citet{kimyi10astar}).

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.55\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def mf(pstack, toks):<!\Suppressnumber!>
    ...<!\Reactivatenumber{14}!>
    for nbr in all_mf_star(n.0, n.1, n.2):
      if len(n[2]) > 0 and n[2][-1] == <!{\textrm{\textit{delete}}}!> \
         and nbr[2][-1] == <!{\textrm{\textit{insert}}}!>:
        continue
      cst = cur_cst + rprs_cst(nbr[2]) + nbr[3]
      for _ in range(len(todo), cst):
        todo.push([])
      todo[cst].append((nbr[0], nbr[1], \
                        n[2] + nbr[2]))
  return None
\end{lstlisting}
\end{adjustbox}%
\begin{adjustbox}{valign=t,minipage=.45\textwidth}
\begin{lstlisting}[numbers=left,firstnumber=24,language=Python]
def all_mf_star(pstack, toks, rprs):
  # Exhaustively apply the <!{$\mfarrowstar$}!> relation to
  # (pstack, toks, rprs) and return the
  # resulting list of (pstack, toks, rprs,
  # heuristic) tuples.
\end{lstlisting}
\end{adjustbox}
\vspace*{-8pt}
\caption{The main \mf algorithm. Lines 2--13 are identical to those of
Figure~\ref{fig:corchuelo:algorithm} (with the exception that \mf does
not return when the first success configuration is found, but stores it until
the current cost \texttt{cur\_cst} is fully evaluated; at that point it returns all
the success configurations found). The main difference between \cpctplus
and \mf is the use of the A* heuristic, which allows configurations to be deferred
beyond their actual cost. In other words a configuration with cost $c$ and a heuristic $h$ is
placed in $\texttt{todo}[c + h]$ (lines 18, 21, 22).}
\label{fig:mf:algorithm}
\end{figure}

The main part of the \mf algorithm is deliberately similar to that of \cpctplus
(compare Figure~\ref{fig:corchuelo:algorithm} and Figure~\ref{fig:mf:algorithm}). Because
\textsf{mf\_dist} is both admissible (i.e.~it never overestimates the distance to a
success configuration) and consistent (i.e.~the total estimated cost to reach a success
configuration is monotonically non-decreasing) we know that neighbouring configurations will
always be the same, or greater, cost as the current configuration. This allows us to
reuse the todo data-structure from \cpctplus as-is. Finally, compatible
configuration merging, repair sequence simplification, and so on are kept
unchanged from \cpctplus.


\subsection{The \texttt{mf\_dist} heuristic}
\label{mf:mf_dist}

\textsf{mf\_dist} comes in static and dynamic parts: the former handles insertion
sequences and reduc\-tions/gotos; and the latter handles deletions (which
require examining the user's input). The static part
extends the distance table algorithm used in \citet{kimyi10astar} while the dynamic part
is entirely new. In this subsection we explain both parts.

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.55\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def mk_mf_table(sgraph, stable, terms, eof_idx):
  table = [<!$\infty$!>] * len(sgraph) * len(terms)
  table[sgraph.accept_state * len(terms) \
        + eof_idx] = 0
  while True:
    chgd = False
    for i in range(sgraph.len_states):
      for sym, end_st in sgraph.edges(i):<!\Suppressnumber!>
        ...<!\Reactivatenumber{26}!>
        for st_idx in goto_states(sgraph, \
                                  stable, i):
          for j in range(len(terms)):
            this_off = i * len(terms) + j
            end_off = st_idx * len(terms) + j
            if table[end_off] != <!$\infty$!> and \
               table[end_off] < table[this_off]:
              table[this_off] = table[end_off]
              chgd = True
    if not chgd:
      break
\end{lstlisting}
\end{adjustbox}
\begin{adjustbox}{valign=t,minipage=.44\textwidth}
\begin{lstlisting}[numbers=left,firstnumber=37,language=Python]
def goto_states(sgraph, stable, st_idx):
  gs = set()
  for nonterm, prod, dot in \
      sgraph.core_state(st_idx):
    if dot < len(prod.syms):
      continue
    prev = set([st_idx])
    for _ in len(prod.syms):
      next = set()
      for i in prev:
        next.union(sgraph.rev_edges(i))
      prev = next
    for i in prev:
      goto_st = stable.goto(i, nonterm)
      if goto_st is not None:
        gs.add(goto_st_idx)
\end{lstlisting}
\end{adjustbox}
\caption{A fixed-point algorithm for the static component of the
\textit{mf\_dist} function, returning a distance table which takes into account
reductions/gotos. \texttt{goto\_states} returns the set of state indexes which a state
\textit{st\_idx} may end up in after all possible reductions/gotos have
occurred. To calculate this, all core items of a state with a dot at the end are
considered (lines 39--42). We then recursively iterate backwards over the stategraph
(using \texttt{rev\_edges(i)} which returns all the states which have an
edge pointing to state \texttt{i}), exploring all possible reduction routes that
could be encountered dynamically (lines 43--48). The intuition behind this is that an
item $[N: \alpha \bullet]$ will cause $|\alpha|$ items to be popped from the
parsing stack; we thus iterate backwards $|\alpha|$ times over the stategraph
(line 44), recording at each point the states we can reach (lines 46--47).
We then take the resulting set and map each element to the state it will goto
(lines 49--52). Lines 9--25 of \texttt{mk\_mf\_table} are identical to
that of the \texttt{mk\_ky\_table} function (see Figure~\ref{fig:kimyi:dist}) (i.e.~they
follow insertion sequences). We then take into account the goto states as
part of the distance calculation (lines 26--34).}
\label{fig:mf:dist:static}
\end{figure}

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=0.38\textwidth}
\small
\begin{tabular}{rcccccc}
\toprule
      & \multicolumn{6}{c}{$t$} \\
        \cmidrule(lr){2-7}
$s$ & INT & \texttt{+} & \texttt{*} & \texttt{(} & \texttt{)} & \$ \\
\midrule
0 & 0 & 1 & 1 & 0 & 1 & 1 \\
1 & 0 & 1 & 1 & 0 & 1 & 1 \\
2 & 1 & 0 & 1 & 1 & 0 & 0 \\
3 & 1 & 0 & 0 & 1 & 0 & 0 \\
4 & 1 & 0 & 0 & 1 & 0 & 0 \\
5 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & 0 \\
6 & 2 & 1 & 1 & 2 & 0 & 1 \\
7 & 0 & 1 & 1 & 0 & 1 & 1 \\
8 & 0 & 1 & 1 & 0 & 1 & 1 \\
9 & 1 & 0 & 0 & 1 & 0 & 0 \\
10 & 2 & 1 & 1 & 2 & 0 & 0 \\
11 & 1 & 0 & 1 & 1 & 0 & 0 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\begin{adjustbox}{valign=t,minipage=0.60\textwidth}
\vspace{-13pt}
\caption{The \textit{mf\_dist} distance table for the grammar from
Figure~\ref{fig:exprgrammar}. Relative to the \textit{ky\_dist} distance
table (Figure~\ref{fig:kimyi:dist}), many fewer entries are
$\infty$, because reductions/gotos are taken into account. For example, consider state
3, terminal `\texttt{+}': in \textit{ky\_dist}'s distance table this has a cost of 3;
in \textit{mf\_dist}'s distance table the cost is 0. The reason for this can be
clearly seen from the stategraph in Figure~\ref{fig:stategraphtable}: in state
3, the terminal `\texttt{+}' causes a reduction to state 0, and then a goto to
state 2. State 2 has an outgoing edge labelled with \texttt{+} and hence a
distance of 0 to it. Notice also that every state now has a non-$\infty$ distance to the `\$'
terminal, since every (reachable) state must be able to reach the accept state.}
\label{fig:mf:dist:example}
\end{adjustbox}
\end{figure}

The A* heuristic of \citet{kimyi10astar} builds a distance table of
states and terminals: \textsf{ky\_dist(s, t)} then returns the cumulative cost of
the sequence of token insertions from state $s$ such that a state $s'$ can be
found which has an outgoing edge labelled with $t$. This is a useful base,
but we also need to take into account reductions/gotos to stop any search which
uses the heuristic from getting trapped (Section~\ref{kimyi:flaws}). Fortunately,
it is relatively easy to statically
underapproximate the effect of reductions/gotos. The basic
idea is simple: as well as taking into account the cost of a sequence of token
insertions in a distance, we also take into account all possible reduction
paths. Note that reductions/gotos and token insertions can be intertwined
(i.e.~a valid way of
reaching $s'$ might be a reduction, a token insertion, another reduction and so
on). Fortunately, not only is this a natural candidate for a fixed-point
algorithm, but we are able to follow the same structure as that used to
calculate the distance table for \textsf{ky\_dist}. Figure~\ref{fig:mf:dist:static}
shows the static part of \textsf{mf\_dist} and Figure~\ref{fig:mf:dist:example}
shows an example distance table.

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.38\textwidth}
\begin{lstlisting}[numbers=left,language=Python]
def mf_dist(rprs, s, toks):
  if ends_in_N_shifts(rprs):
    return 0
  ld = <!$\infty$!>
  dc = 0
  while dc < ld:
    d = mf_dist_table(s, toks[0])
    if d < <!$\infty$!> and dc + d < ld:
      ld = dc + d
    dc += 1
    if len(toks) == 1:
      break
    toks = toks[1:]
  if ld == <!$\infty$!>:
    return None
  else:
    return ld
\end{lstlisting}
\end{adjustbox}%
\begin{adjustbox}{valign=t,minipage=.60\textwidth}
\vspace{-9.5pt}
\caption{\textit{mf\_dist} returns the cost to a success
configuration, taking into account reductions and deletions. It takes as input a
sequence of repairs \textit{rprs}, a state \textit{s}, and the remaining tokens
from the user's input \textit{toks} and returns an integer distance or
\texttt{None} if no route can be found. The function starts with a special case:
if the repair sequence ends in $N_\textit{shifts}$ shifts then the distance to a
success configuration is by definition 0 (lines 2--3). Otherwise, the current
least distance to a success configuration is set to $\infty$ (line 4; this value
monotonically decreases as the main loop executes) and the current cost of
deleting tokens to 0 (line 5). We then continually iterate over the user's input
until either there is no input left (lines 11-12) or the cost of deleting tokens
exceeds the current least distance (line 6). On each iteration of the loop, we
lookup the static distance to the next token in the user's input (line 7). If
the distance is less than $\infty$ and the cumulative deletion cost plus the static
distance is less than the current least distance, then we update the latter
(lines 8--9). Otherwise, we increment the deletion cost (line 10) and `delete'
the next token (line 13).}
\label{fig:dyndist:dyndist}
\end{adjustbox}
\end{figure}

The dynamic part of \textsf{mf\_dist} is shown in
Figure~\ref{fig:dyndist:dyndist}. This makes use of the static table created by
\texttt{mk\_mf\_table} and also takes into account the cost of deleting user
input. The intuition is that we need to check whether the cost to a success
configuration would be reduced if one or more tokens
from the user's input were to be deleted. For example,
consider the input `\texttt{(1()}'. Parsing this against the grammar from Figure
\ref{fig:stategraphtable} causes an error in state 4 before the second
`\texttt{(}' token; after applying \textsc{MF Reduce} we end up in state 6.
Looking up the next token `\texttt{(}' in the distance table returns a cost of
2. However, the next token costs 1 to delete, while the distance to
the subsequent token `\texttt{)}' is 0. Thus the combination of the deletion
cost (1) and the cost to reach the second token from the current state (0) is
lower than the cost to reach the first token (2). \textsf{mf\_dist} must thus
return a cost of 1 in order that the heuristic remains admissible.

The remaining subtlety of \textsf{mf\_dist} comes from checking that we don't
accidentally defer success configurations. While configurations that reach an
accept state have a distance of 0 in the distance table by definition,
configurations that may end with $N_\textit{shifts}$ shifts have to be checked manually.
\textsf{mf\_dist} therefore has to consider the repair sequence of
the new configuration being created: if it ends with $N_\textit{shifts}$ shifts,
then a distance of 0 is returned to ensure that the configuration
is checked for success at the current cost.


\subsection{Skipping intermediate reduction/gotos}

Although not easily expressed in the reduction rules of Figure~\ref{fig:mf:rules},
\textsc{MF Reduce} can be further optimised. In its original form, \textsc{MF Reduce}
causes each reduction/goto to produce a new configuration whose neighbours are
then explored as normal. However, this is not always necessary.
As can be seen from \textsc{LR Reduce}, reduction/gotos are taken based on the
next token in the user's input: a sequence of configurations [\textit{reduce},
\textit{delete}, \textit{reduce}], for example, is nonsensical, as the
next token in the user's input cannot suddenly change in the middle of a
sequence of reduction/gotos.

Our implementation of \textsc{MF Reduce} therefore skips configurations whose
goto state both: only contains reductions; and
where all reductions reference the same production. For example, if, for the
grammar of Figure~\ref{fig:stategraphtable} we reduce/goto state 10 (which
contains two reduction actions, both referencing the same production), we
skip it and immediately perform the next reduction/goto. Conversely, if we
reduce/goto state 3, we must create a configuration, since it contains a shift
action (as well as several reduce actions).


\section{Experiment}
\label{experiment}

In order to understand the performance of error recovery algorithms we coducted
a large experiment on real-world Java code using three algorithms: panic mode
(Section~\ref{sec:panic mode}), \cpctplus, and \mf. In this section we outline our methodology
(Section~\ref{methodology}) and results (Section~\ref{results}). Our experiment
is fully repeatable and downloadable from \url{https://archive.org/download/error_recovery_experiment/0.1/}


\subsection{Methodology}
\label{methodology}

In order to evaluate error recovery implementations, we need a concrete implementation. We
implemented a new Yacc-compatible parsing system \emph{grmtools} in Rust. Including
associated libraries for LR table generation and so on, grmtools is around
11KLoC. Although intended as a production library, it has accidentally
played a part as a flexible test bed for experimenting with, and understanding,
error recovery algorithms. We added a simple front-end which produces the output
seen in e.g.~Figure~\ref{fig:javaerror}. We use grmtools for all experiments in
this paper.

There are two standard problems when evaluating error recovery algorithms: how
to determine if a good job has been done on an individual example; and obtaining
sufficient examples to get a wide perspective on an algorithm's performance. To
some extent, solutions to these problems are mutually exclusive: for
real-world inputs, the only way to guarantee that a good job has been done is to
manually evaluate it, which means that it is only practical to use a small set
of input programs. Most papers we are aware of use at most 200 inputs
(e.g.~\cite{corchuelo02repairing}), with one using a single input with minor
variants (\citet{kimyi10astar}). \citet{cerecke03phd} was the first to use a
large-scale corpus of approximately 60,000 Java source files. Early in the
development of our methodology, we performed some rough experiments which
suggested that statistics only start to stabilise once a corpus exceeds 10,000
programs. We therefore prefer to use a much larger corpus than most previous
studies. We are fortunate to have access to the Blackbox
project~\cite{brown14blackbox}, an opt-in data collection facility for the BlueJ
editor, which records major editing events (e.g.~compiling a file) and sends
them to a central repository. Crucially, one can see the source code associated
with each event. What makes Blackbox most appealing as a data source is its
scale and diversity: it has hundreds of thousands of users, and a huge
collection of source code.

We first obtained a Java 1.5 Yacc grammar and updated it to support Java
1.7.\footnote{Unfortunately, changes to the method calling syntax in Java 1.8
mean that it is an awkward, though not impossible, fit for an LR(1) formalism
such as Yacc, requiring substantial changes to the current Java Yacc grammar. We
consider the work involved beyond that useful for this paper.} We then
randomly selected source files from Blackbox's database (following the lead of
\citet{santos18syntax}, we selected data from Blackbox's beginning until the end
of 2017-12-31). We then ran such inputs through our Java 1.7 lexer. We immediately
rejected files which didn't lex, since such files cannot be considered
for parsing.\footnote{Happily, this also excludes outputs which can't
possibly be Java source code. Some odd things are pasted into text
editors.} We then parsed candidate files with our Java grammar and rejected any
which did parse successfully, since there is little point running an error
recovery algorithm on correct input. The final corpus consists of \corpussize source files
(collectively a total of \corpussizemb{}MiB). Since Blackbox, quite reasonably,
requires each person with access to the source files to register with them, we
cannot distribute the source files directly; instead, we distribute the
(inherently anonymised) identifiers necessary to extract the source
files for those who register with Blackbox.

The size of our corpus means that we cannot manually evaluate repairs for
quality. Instead, to evaluate this paper's most important metric (the cascading
error problem), we report the number of error locations found in the
corpus for different algorithms. We know, by definition, that the corpus
contains at least \corpussize manually created errors (i.e.~at least one per
file). Since it is likely that some files have more than one manually created
error, the minimum possible number of error locations is likely to be
bigger than this, but we have no way of knowing the true number. However, we can
compare different algorithms: the fewer error locations an algorithm
reports, the fewer cascading errors it has caused. This comes with an
important caveat: a nefarious error recovery algorithm could simply skip all
input after the first error encountered, thus reporting `only' \corpussize
error locations. Since, for other reasons, we also record the proportion of input skipped,
we can confirm that this is not a significant factor in any of the algorithms
we report on.

In order to test hypothesis H1 we ran each error recovery algorithm against
the entire Java corpus, collecting for each file: the time spent in recovery (in seconds);
whether error recovery on the file was successful (true or false); the
number of error locations; the cost of repair sequences at each
error location (only if error recovery was successful on the file as a whole);
and the number of lexemes skipped by error recovery (i.e.~how many \emph{delete}
repairs were applied). Note that there are two ways of failing to repair all errors in a file:
exceeding the timeout; or running out of plausible candidate repair sequences.
We measure the time spent in error recovery with a monotonic wall-clock timer,
covering all aspects of error recovery from when the main parser first invokes
error recovery until an updated parsing stack and parsing index are returned
along with minimum cost repair sequences. The timer is suspended when normal parsing
restarts and resumed if error recovery is needed again.

In order to test hypothesis H2, we created a variant of \mf called \mfrev,
collecting the same data as for the other error recovery algorithms.
Instead of selecting from the minimum cost repair sequences which allow
parsing to continue furthest, \mfrev selects from those which allow parsing to
continue the least far. This models the worst case for other members of the
\citet{fischer79locally} family which non-deterministically select a single minimum
cost repair sequence. In other words, it allows us to understand how many
more errors could be reported to users of other members of the
\citet{fischer79locally} family compared to \mf.

In order to understand the accuracy of the numbers we report, we provide 99\%
confidence intervals. We bootstrapped~\cite{efron79bootstrap} our results
\numbootstrap times to produce confidence intervals. However, since, as
Figure~\ref{fig:results:mf_histogram} shows, our distribution is heavy-tailed,
we cannot bootstrap naively. Instead, we ran each error recovery algorithm
\numruns times on each source file; when bootstrapping we randomly sample one of the
\numruns values collected (i.e.~our bootstapped data contains an entry for every
file in the experiment; that entry is one of the \numruns values collected for
that file). The only subtlety is when bootstrapping the mean cost
size: this value only makes sense if the file was successfully recovered from,
so we do not sample from runs where error recovery failed.

All experiments were run on an otherwise unloaded Intel Xeon E3-1240 v6 with 32GiB RAM running
Debian 9. We used Rust nightly-2018-10-28 to compile grmtools (the
\texttt{Cargo.lock} file necessary to reproduce the build is included in our
experimental repository).


\subsection{Results}
\label{results}

\begin{figure}[t]
\begin{tabular}{lcccccc}
\toprule
  & Mean     & Median   & Cost      & Failure   & Lexemes      & Error \\
  & time (s) & time (s) & size (\#) & rate (\%) & skipped (\%) & locations (\#) \\
\midrule
\input{table.tex}
\bottomrule
\end{tabular}
\caption{Summary statistics from running our error recovery algorithms over
a corpus of \corpussize Java files (for all measures, lower values are better).
Mean and median times
report how long was spent in error recovery per file: both figures
include files which exceeded the recovery timeout, so they represent the `real'
times that users would experience, whether or not all errors are repaired or
not. Cost size reports the mean cost (i.e.~the number of insert and delete
repairs) of each error location repaired (this number is meaningless for \panic,
which does not have a concept of costable repairs).
The failure rate is the percentage of files which could not be fully
repaired within the timeout (this number is semi-meaningless for \panic, which,
at worst, is always able to find a repair at the EOF token). Lexemes skipped is
the percentage of input
skipped (because of a delete repair).
The number of error locations shows how many separate points in files
are reported as errors. Since fewer files in \mf timeout, it is able
to continue repairing errors more often, hence the slightly higher
number of error locations. However \mfrev (which always selects
from the minimum cost repair sequences that allow parsing to continue the least far)
is much more prone to cascading errors, reporting
\mfreverrorlocsratioovermf more error locations than \mf to users.}
\label{fig:results:summary}
\end{figure}

\begin{figure}[t]
\vspace{-10pt}
\includegraphics[scale=.7]{mf_histogram.pdf}
\caption{A histogram of the time spent in error recovery by \mf for files in our
corpus. The $x$ axis shows time (up to the timeout of 0.5s) and the $y$ axis is
a logarithmic scale for the number of files. Error bars represent 99\% confidence
intervals. As this clearly shows, most files
are repaired extremely quickly. There is then a continual decrease
until the timeout of 0.5s, where the files that were unable to be repaired
in the timeout cause a small, but pronounced, peak.}
\label{fig:results:mf_histogram}
\end{figure}

Figure~\ref{fig:results:summary} shows a summary of the results of our
experiment. The overall conclusions are fairly clear. Both \cpctplus and \mf are
able to repair nearly all input files within the 0.5s timeout; and while
panic mode is able to repair virtually every file within the 0.5s timeout, it
reports well over twice as many error locations as \cpctplus or \mf (i.e.~panic
mode substantially worsens the cascading error problem).
The fact that the median recovery time for \cpctplus and \mf is two orders of
magnitude lower than the mean
recovery time suggests that only a small number of outliers cause error recovery to
take long enough to be perceptible to humans; this is confirmed by the
histogram in Figure~\ref{fig:results:mf_histogram}. \mf's failure rate is only
\mfcpctplusfailurerateratio that of \cpctplus's, though in absolute terms
both are already extremely low. \mf also has noticeably better median and
mean repair times (the latter, in our opinion, more important, since users
are more sensitive to worst case than best case performance) though,
again, in absolute terms both are
already fairly low. These results strongly validate Hypothesis H1.

\begin{figure}[t]
\includegraphics[scale=.7]{mf_mfrev_error_locs_histogram_full.pdf}
\includegraphics[scale=.7]{mf_mfrev_error_locs_histogram_zoomed.pdf}
\caption{Histograms of the number of error locations for \mf and \mfrev (fewer
is better). Error bars represent 99\% confidence
intervals. The top histogram shows the full spread of data (with 50 bins). Since outliers
obscure the main body of data, the bottom histogram only shows data up to 50 error
recovery locations (with 50 bins i.e.~1 bin per error location size). For both histograms, the
$x$ axis shows the number of error locations in a file and the $y$ axis is a logarithmic
scale for the number of files. Error bars represent 99\% confidence intervals.
As the bottom histogram clearly shows, the entire distribution is skewed
slightly rightwards by \mfrev, showing that \mfrev makes error recovery
slightly worse in a number of files (rather than making error recovery in a small
number of files a lot worse).}
\label{fig:results:mf_mfrev_histogram}
\end{figure}

\cpctplus and \mf rank the complete set of minimum cost repair sequences by how
far each allows parsing to continue and choose from those which allow parsing to
continue furthest. \mfrev, in contrast, selects from those which allow parsing
to continue the least far. \mfrev shows that the ranking technique used in \mf substantially
reduces the potential for cascading errors: \mfrev leads to
\mfreverrorlocsratioovermf more error locations being reported to users
relative to \mf. As the histogram in Figure~\ref{fig:results:mf_mfrev_histogram} shows,
the distribution of error locations in \mf and \mfrev is
similar, with the latter simply shifted slightly to the right.
In other words, \mfrev
makes error recovery slightly worse in a number of files
(rather than making error
recovery in a small number of files a lot worse). This strongly validates
Hypothesis H2.

Interestingly, \mfrev has a noticeably higher mean cost of repair sequences relative to
\cpctplus and \mf. In other words, \mfrev not only causes more error locations to be reported,
but the repair sequences at the additional error locations have higher numbers
of insert and delete repairs. This suggests that there is a double whammy from
cascading errors: not only are more error locations reported, but the poorer
quality repair sequences chosen make subsequent error locations
disproportionately harder for the error recovery algorithm to recover from.


\subsection{The impact of skipping input}

As well as the much higher failure rate, the number of error locations reported
by panic mode is well over twice that of \cpctplus and \mf. This led us to make
an additional hypothesis:

\begin{description}
  \item[H3] The more of the user's input that is skipped, the greater the number
of cascading parsing errors.
\end{description}

The intuition underlying this hypothesis is that, in general, the user's input
is very close to being correct: thus the more of the input that one skips, the
less likely one is to get back to a successful parse. We thus added the ability
to record how much of the user's input is skipped as the result of \emph{delete}
repairs during error recovery. The figures surprised us: \cpctplus and \mf skip
very little of the user's input; \mfrev skips a little more; and
panic mode skips an order of magnitude more. Although we do
not have enough data points to make a definitive statement, our data seems to
validate Hypothesis H3.


\section{Threats to validity}

The most obvious threat to validity is our performance comparison of \cpctplus,
and \mf. They are specific to our implementation context and it is possible that their
relative positions could change if implemented in a different fashion.
Nevertheless, the absolute performance numbers for \cpctplus and \mf are already good, and
better implementations will only improve our view of both algorithms.

A less obvious problem is that, even after repair sequence ranking, \cpctplus
and \mf are still non-deterministic. This is because, in general, multiple
repair sequences may have identical effects up to $N_\textit{try}$ tokens, but
cause different effects after that value. By running each file through each
error recovery multiple times and reporting confidence intervals, we are able
to give a good -- though inevitably imperfect -- sense of the likely variance
induced by this non-determinism.

Blackbox contains an astonishingly large amount of source code but has two
inherent limitations. First, it only contains Java source code. This means that
our main experiment is limited to one grammar: it is possible that our
techniques do not generalise beyond the Java grammar, though \citet[p.~109]{cerecke03phd}
suggests that different grammars make relatively little difference
to the performance of such error recovery algorithms. Unfortunately, we are not aware
of an equivalent repository for other language's source code. One solution is
to mutate correct source files (e.g.~randomly deleting tokens), thus
obtaining incorrect inputs which we can later test: however, it is difficult
to uncover and then emulate the numerous, sometimes surprising, ways that
humans make syntax errors, particularly as some are language specific
(though there is some early work in this area~\cite{dejonge12automated}).
Second, Blackbox's data comes largely from students,
who are more likely than average to be somewhat novice programmers. It is clear
that novice programmers make some different syntax errors -- or, at least, make
the same syntax errors more often -- relative to advanced programmers. An
extreme example is the furthest outlier elided from
Figure~\ref{fig:results:mf_mfrev_histogram}: it contains the same highly erroneous
input copy and pasted several times within the same file. It is
thus possible that a corpus consisting solely of programs from advanced programmers
would lead to slightly different results. We consider this a minor worry,
partly because a good error recovery algorithm should
aim to perform well with inputs from users of different experience levels.

Our corpus was parsed using a Java 1.7 grammar, but some members of the corpus
were almost certainly written using Java 1.8 features. Many -- though not all -- Java 1.8
features require a new keyword: such candidate source files would thus have
failed our initial lexing test and not been included in our corpus. However,
some Java 1.8 files will have made it through our checks. Arguably these are still a valid
test of our error recovery algorithms. It is even likely that they may be a little
more challenging on average, since they are likely to be further away from being valid
syntax than files intended for Java 1.7.


\section{Related work}

Error recovery techniques are so numerous that there is no
definitive reference or overview of them. However, \citet{degano95comparison}
contains an overall historical analysis and \citet{cerecke03phd} an excellent
overview of many of the approaches which build on \citet{fischer79locally}. Both
must be supplemented with more recent works, such as those we have cited in this
paper.

The biggest limitation of error recovery algorithms in the
\citet{fischer79locally} family is that they are local: they find repairs at the
point that an error is discovered, which may be later in the file than the cause
of the error. Thus even when they successfully recover from an error, the repair
sequence reported may be very different from the fix the user considers
appropriate (note that this is distinct from the cascading error problem,
which our ranking of repair sequences in Section~\ref{rankingrepairs} partly
addresses). Perhaps the most common -- and without doubt the most frustrating
-- example of this is missing a `\texttt{\}}' character within the method of a
Java-like language. Some approaches are able to backtrack from the source of
the error in order to try and more appropriate repairs. However, there are
two problems with this: first, the cost of maintaining the necessary state to
backtrack slows down normal parsing (e.g.~\cite{deJonge12natural} only store the
relevant state at each line encountered to reduce this cost), whereas we add no
overhead at all to normal parsing; second, the search-space is so hugely
increased that it can be harder to find any repairs at
all~\cite{degano95comparison}.

A more recent possibility from~\citet{santos18syntax} is to use
machine learning to train a system on syntactically correct programs: when a
syntax error is encountered, they use their model to suggest appropriate global
fixes. Although they also use data from Blackbox, their
experimental methodology is very different: they are stricter, in that they aim
to find exactly the same type of repair as the human user actually
applied themselves; but also looser, in that they only
consider errors which can be fixed by a single token (discarding 42\% of
the data \cite[p.~8]{santos18syntax}) whereas we attempt to fix errors which
span multiple tokens. It is thus difficult to directly compare their results to
ours. However, by the high bar they have set themselves, they are able to repair
52\% of single-token errors (i.e.~about 30\% of all possible errors; for
reference, MF repairs \mfsuccessrate of files). It seems likely that future machine
learning approaches will improve upon this figure, although the size of the
problem space suggests that it will be hard to get close to 100\%{}. It seems plausible
that a `perfect' system will mix both deterministic approaches (such as ours, which
has a high chance of finding a good-enough recovery) with
probabilistic approaches (which have a moderate chance of finding a perfect
recovery). There may be several shades of grey, leading to a system
with multiple error recovery sub-approaches (in similar fashion to \citet{deJonge12natural}).

Although one of our paper's aims is to find the complete set of minimum cost repair sequences,
it is unclear how best to present them to users, leading to questions such as:
should they be simplified? should a subset be presented? and so on. Although
rare, there are some surprising edge cases. For example,
the (incorrect) Java expression `\texttt{x = f(""a""b);}' leads to 23,067 minimum
cost repair sequences being found, due to the large number of Java keywords that are
valid in several parts of this expression leading to a combinatorial explosion: even the most
diligent user is unlikely to find such a volume of information valuable. There is a
body of work which has tried to understand how best to structure compiler error
messages (normally in the context of those learning to program). However, the
results are hard to interpret: some studies find that more complex error
messages are not useful \cite{nienaltowski08error}, while others suggest they
are \cite{prather17novices}. It is unclear to us what the right approach might be,
or how it could be applied in our context.

The approach of \citet{mckenzie95error} is similar to
\citet{corchuelo02repairing}, although the former cannot incorporate shift
repairs. It tries harder than \cpctplus to prune out pointless search
configurations~\cite[p.~12]{mckenzie95error}, such as cycles in the parsing stack,
although this leads to some minimum cost repairs being
skipped~\cite{bertsch99failure}. A number of interlocking, sophisticated pruning
mechanisms which build on this are described in~\citet{cerecke03phd}. These are
significantly more complex than our merging of compatible configurations: since this
gives us acceptable performance in practise, we have not investigated other
pruning mechanisms.

\cpctplus and \mf take only the grammar and token types into account. However,
it is possible to use additional information, such as nesting (e.g.~taking into
account curly brackets) and indentation when recovering from errors. This
has two aims: reducing the size of the search space (i.e.~speeding up error
recovery); and making it more likely that the repairs reported matched
the user's intentions. The most sophisticated
approach in this vein we are aware of is that of \citet{deJonge12natural}. At
its core, this approach uses GLR parsing: after a grammar is suitably
annotated by the user, it is then transformed into a `permissive' grammar which
can parse likely erroneous inputs; strings which match the permissive parts of
the grammar can then be transformed into a non-permissive counterpart. In
all practical cases, the transformed grammar will be ambiguous, hence the
need for generalised parsing. There is an intriguing similarity between
our approach and that of \citet{deJonge12natural}: our use of graph-structured
stacks in configuration merging (see Section~\ref{corchuelo:allminimumcost})
gives that part of our algorithm a similar feel to GLR parsing (even though
we never generate ambiguous strings). However, there are also major
differences: LR parsers are significantly simpler to implement than GLR parsers;
and the \citet{fischer79locally} family of algorithms do not require manually
annotating, or statically increasing the size of, the grammar. It is unlikely,
for example, that a Yacc-based parser would be able or willing to use a GLR
parser for error recovery.

The approach we have taken in this paper can only repair errors on files which
are fully lexed. Since many users are unaware of the distinction between these
two stages, this can cause confusion: a minor lexing error does not lead to any
parsing errors. Looked at another way, fixing a single lexing error can,
surprisingly, lead to a slew of parsing errors being reported. Scannerless
parsing~\cite{salomon89scannerless} is one solution to this problem, since there
is no distinction between lexing and parsing. However, scannerless parsing
introduces new trade-offs: it is inherently ambiguous (e.g.~is `if' an
identifier or a keyword?); ambiguity is, in general, undecidable
and even the best ambiguity heuristics
fail to find all ambiguous parts of a grammar~\cite{vasudevan13detecting};
and resolving those ambiguities can make the parser context
sensitive~\cite{eijck__lets_accept_rejects}. Other possibilities are to
intermingle parsing and lexing (see e.g.~\citet{wyk07context}) or to allow
`fuzzy' or partial matching of tokens (see~\citet[p.~8]{vanter00displaying}).

A very different approach is that taken by \citet{pottier16reachability}: rather
than try and recover from errors directly, it reports precisely and accurately
how the user's input caused the parser to reach an error state (e.g.~``I
read an open bracket followed by an expression, so I was expecting a close
bracket here''), and
possible routes out of the error (e.g.~``A function or variable declaration is
valid here''). This involves significant manual work, as every parser state
(1148 in the Java grammar we use) in which an error can occur needs to be
manually marked up, though the approach has
various techniques to lessen the problem of maintaining messages as a grammar
evolves. This approach seems complementary to ours: in an ideal world it would
be possible to give precise, high-level messages about the problem
encountered whilst also showing
repair sequences that allowed parsing to continue. One challenge may be to make
the top ranked repair sequences match the manually written messages.

Many compilers and editors have hand-written parsers with hand-written error
recovery. Though generally ad-hoc in their approach, it is possible, with
sufficient effort, to make them perform well. However, this comes at
a cost. For example, the hand-written error recovery routines in the Eclipse IDE
are approximately 5KLoC of code and are solely for use with Java code: \cpctplus
and \mf are approximately 450LoC and 800LoC respectively and can be applied to
any LR grammar. Furthermore, automated language-neutral approaches perform at
least as well as this hand-written approach~\cite[p.~36]{deJonge12natural}. We
are not aware of any hand-written approaches to error recovery which outperform
automated approaches.

Although error recovery approaches have, historically, been mostly LR based,
there are several non-LR approaches. A full overview is impractical, though a
couple of pointers are useful. When LL approaches encounter an error, they
generally skip input until a token in the follow set is encountered (an early
example is~\citet{turner77error}). Error recovery in PEGs is slightly more
awkward, partly because there is not always a clearly defined point at which an
error is determined. Perhaps because of this, recent approaches to PEG error
recovery have tended to require additional annotations to the grammar in order
to achieve good quality recovery (see e.g.~\cite{medeiros18syntax}).

While the programming language world has largely forgotten the approach of
\citet{aho72minimum}, there are a number of successor works, most recently that
of~\citet{rajasekaran16error}. These improve on the time complexity, though none
that we are aware of address the issue of how to present what has been done to
the user.

We are not aware of any error recovery algorithms that are formally verified.
Indeed, as shown in this paper, several have serious flaws. We are only aware of
two works which have begun to consider what correctness for such algorithms might mean:
\citet{zaytsev14formal} provides a brief philosophical justification of the need
and \citet{gomezrodriguez10error} provides an outline of an approach. Until
such time as someone verifies a full error recovery algorithm, it is difficult
to estimate the effort involved, or what issues may be uncovered.


\section{Conclusions}

In this paper we have shown that error recovery algorithms in the
\citet{fischer79locally} family can run fast enough to be usable in the real
world, and that they produce significantly better results than traditional
panic mode. Furthermore, extending such algorithms to produce the complete set of
minimum cost repair sequences allows parsers to provide better feedback to
users, as well as significantly reducing the cascading error problem.

For batch compilers, \cpctplus is probably a good match: it is simple to
implement (around 450LoC in our Rust system) and still has good performance. For
interactive environments, the extra effort involved in implementing \mf --
roughly 350LoC more than \cpctplus -- may be small enough to justify the
performance improvement.

Looking to the future, we (perhaps immodestly) suggest that \cpctplus and \mf might be `good enough'
to serve as the main representative of the \citet{fischer79locally} family.
However, we do not think that either algorithm on their own is the perfect solution. We suspect
that, in the future, multi-phase solutions will be developed. For example, one
may use a machine-learning approach such as that of \citet{santos18syntax} as a
first-step and, when it fails (which current evidence suggests it
will do fairly often), fall back to \cpctplus or \mf.

\begin{acks}
We are grateful to the Blackbox developers for allowing us access
to their data, and particularly to Neil Brown for help in extracting a relevant
corpus. We thank Edd Barrett for helping to set up our benchmarking
machine and for comments on the paper. We also thank Carl Friedrich
Bolz-Tereick, Sarah Mount, Franois Pottier, and Christian Urban for comments on the paper.
\end{acks}

\bibliography{bib}

\appendix

\section{Curated examples}
\label{app:examples}

\subsection{Java}

\noindent Example 1 input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex1.java}

\noindent Example 1 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex1.out}

\vspace{12pt}

\noindent Example 2 input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex2.java}

\noindent Example 2 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex2.out}

\vspace{12pt}

\noindent Example 3 (taken from \citet[p.~10]{deJonge12natural}) input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex3.java}

\noindent Example 3 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex3.out}

\vspace{12pt}

\noindent Example 4 (taken from \citet[p.~16]{deJonge12natural}) input:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex4.java}

\noindent Example 4 output:
\lstinputlisting[xleftmargin=2em]{examples/java_ex4.out}


\vspace{12pt}

\noindent Example 5 (taken from \citet[p.~2]{medeiros18syntax}):
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex5.java}

\noindent Example 5:
\lstinputlisting[xleftmargin=2em]{examples/java_ex5.out}


\vspace{12pt}

\noindent Example 6:
\lstinputlisting[numbers=left,language=Java,xleftmargin=2em]{examples/java_ex6.java}

\noindent Example 6 output, showing the timeout being exceeded and error recovery
unable to complete:
\lstinputlisting[xleftmargin=2em]{examples/java_ex6.out}



\subsection{Lua}

\noindent Example 1 input:
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex1.lua}

\noindent Example 1 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex1.out}

\vspace{12pt}

\noindent Example 2 input. Note that `\texttt{=}' in Lua is the assignment
operator, which is not valid in conditionals; and that if/then/else blocks must
be terminated by `\texttt{end}'.
\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex2.lua}

\noindent Example 2 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex2.out}

\vspace{12pt}

\noindent Examples 3 and 4 (both derived from the Lua 5.3 reference manual) show
that \cpctplus and \mf happily deal with an inherent ambiguity in Lua's Yacc
grammar involving function calls and assignments (which, following the Lua
specification, is resolved by Yacc in favour of function calls). First, this
example shows the `unambiguous' case (i.e.~if Lua forced users to use `;'
everywhere, the grammar would have no ambiguities):

\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex3.lua}

\noindent Example 3 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex3.out}


\vspace{12pt}

\noindent Example 4 shows what happens in the `ambiguous' case (which Lua's
grammar resolves in favour of viewing the code below as a function call to
\texttt{c}):

\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex4.lua}

\noindent Example 4 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex4.out}


\vspace{12pt}

\noindent Example 5 (taken from \citet[p.~7]{medeiros18syntax}):

\lstinputlisting[numbers=left,xleftmargin=2em]{examples/lua_ex5.lua}

\noindent Example 5 output:
\lstinputlisting[xleftmargin=2em]{examples/lua_ex5.out}

\end{document}
