\documentclass[acmsmall,small,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{multicol}
\usepackage[autolanguage]{numprint}
\usepackage{proof}
\usepackage{softdev}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xspace}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

\lstset{
    basicstyle=\tt\scriptsize,
    xleftmargin=0pt,
    numbersep=.8em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

% from https://tex.stackexchange.com/questions/264361/skipping-line-numbers-in-lstlisting#264373
\let\origthelstnumber\thelstnumber
\makeatletter
\newcommand*\Suppressnumber{%
  \lst@AddToHook{OnNewLine}{%
    \let\thelstnumber\relax%
     \advance\c@lstnumber-\@ne\relax%
    }%
}

\newcommand*\Reactivatenumber[1]{%
  \setcounter{lstnumber}{\numexpr#1-1\relax}
  \lst@AddToHook{OnNewLine}{%
   \let\thelstnumber\origthelstnumber%
   \refstepcounter{lstnumber}
  }%
}

\setcopyright{none}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}

% DOI
%\acmDOI{0000001.0000001}

% Paper history
%\received{February 2007}

\newcommand{\cpctplus}{\textrm{\textit{CPCT}}$^+$\xspace}
\newcommand{\crarrow}{\rightarrow_{\textrm{\tiny CR}}}
\newcommand{\crarrowstar}{\rightarrow_{\textrm{\tiny CR}}^*}
\newcommand{\kyarrow}{\rightarrow_{\textrm{\tiny KY}}}
\newcommand{\lrarrowstar}{\rightarrow_{\textrm{\tiny LR}}^*}
\newcommand{\lrarrow}{\rightarrow_{\textrm{\tiny LR}}}
\newcommand{\mf}{\textrm{\textit{MF}}\xspace}
\newcommand{\mfrev}{\textrm{\textit{MF$_\textrm{rev}$}}\xspace}
\newcommand{\mfarrow}{\rightarrow_{\textrm{\tiny MF}}}
\newcommand{\mfarrowstar}{\rightarrow_{\textrm{\tiny MF}}^*}

\include{experimentstats}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title[Fast Multiple Repair Error Recovery]{Fast Multiple Repair Error Recovery}

\author{Lukas Diekmann}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}}
\author{Laurence Tratt}
\orcid{0000-0002-5258-3805}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}
}
\thanks{Authors' URLs: %
    L.~Diekmann~\url{http://diekmann.co.uk/},
    L.~Tratt~\url{http://tratt.net/laurie/}.
}


\begin{abstract}
Syntax errors are generally easy to fix for humans, but not for parsers: the
latter often fail to find an effective recovery, leading to a cascading chain of
errors that drown out the original. More advanced recovery techniques
suffer much less from this problem but have seen little practical use because
their typical performance was seen as poor and their worse case unbounded.
In this paper we show that not only can an advanced approach run in acceptable time
-- which we define as spending a maximum of 0.5s in error recovery per file -- but
that we can increase the complexity of the problem to finding the complete set of minimum
cost repair sequences, and still do so within acceptable time. We first extend
Corchuelo et al.'s algorithm, before introducing a new, faster, alternative \mf. We
validate our algorithms with a corpus of \corpussize real-world
syntactically invalid Java programs: \mf is able to repair \mfsuccessrate of files
within acceptable time. We show that making use of the complete set of repair
sequences reduces cascading errors by \mfreverrorlocsratioovermf.
\end{abstract}

\keywords{Parsing, error recovery, programming languages}

\maketitle

\section{Introduction}

Programming is a humbling job, which requires acknowledging that we will make
untold errors in our quest to perfect a program. Most troubling are semantic
errors, where we intended the program to do one thing, but it does another. Less
troubling, but often no less irritating, are syntax errors, which are
(generally minor) deviances from the exacting syntax required by a compiler.
So common are syntax errors that the parsers in modern compilers expect us to make several
in a single input. Rather than stop on the first syntax error encountered, they attempt
to \emph{recover} from it. This allows them to report, and us to fix, all our
syntax errors in one go.

When error recovery works well, it is a useful productivity gain. Unfortunately,
widely used approaches are ad-hoc and simplistic, with
two weaknesses limiting their usefulness: only limited consideration is
given to the context of the error; and the only recoveries attempted are the
skipping of input until a pre-determined synchronisation
token is reached~\cite[p.~3]{degano95comparison} or the insertion of a
single synchronisation token. Inappropriate recoveries cause a
cascade of spurious syntax errors (see
Figure~\ref{fig:javaerror} for an example): programmers quickly learn that
only the position of the first error -- not the reported repair, nor the position of
subsequent errors -- can be relied upon to be accurate.

\begin{figure}[t]
\begin{minipage}{0.48\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:javaerror:input}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
class C {
  int x y;
}
\end{lstlisting}
\end{minipage}
\\
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{1}
\caption{}
\label{lst:javaerror:mf}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Error at line 2 col 9. Repairs found:
  Delete "y"
  Insert "COMMA"
  Insert "EQ"
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\vspace{-20.5pt}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{-2}
\caption{}
\label{lst:javaerror:javac}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
C.java:2: error: ';' expected
  int x y;
       ^
C.java:2: error: <identifier> expected
  int x y;
         ^
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%&
%\begin{minipage}[t]{0.47\textwidth}
%\vspace{-17pt}
\vspace{-10pt}
\caption{An example of a simple, common Java syntax error
(\subref{lst:javaerror:input}) and the problems traditional error recovery has in
dealing with it. \texttt{javac} (\subref{lst:javaerror:javac}) spots the error
when it encounters `\texttt{y}'. Its error recovery heuristic then
repairs the input by inserting a semicolon before `\texttt{y}' (i.e.~making
the input equivalent to `\texttt{int x; y;}'). This immediately leads to a spurious second parse error,
since `\texttt{y}' on its own is not a valid statement. The two new error
recovery algorithms (\cpctplus and \mf) we introduce in this paper both
produce the output shown in (\subref{lst:javaerror:mf}): after spotting an error
when parsing encounters `\texttt{y}', they then use the Java grammar to find the
complete set of minimum cost repair sequences (unlike previous approaches which
non-determinstically find one minimum cost repair sequence). In this case three
repair sequences are reported to the user: one can delete `\texttt{y}'
entirely (`\texttt{int x;}'), or insert a comma
(`\texttt{int x, y;}'), or insert an equals sign (\texttt{`int x = y;'}).}
\label{fig:javaerror}
%\end{minipage}
%\end{tabular}
\end{figure}

Most of us are so used to this state of affairs that
we assume it to be inevitable. However, there are more advanced algorithms which,
as well as being able to deal with any LR grammar, take into account the full context of the error, and have
several ways of recovering from errors. Probably the earliest such algorithm
is \citet{aho72minimum}, which, upon encountering an error, creates on-the-fly an
alternative (possibly ambiguous) grammar which allows the parser to recover.
The implementation complexity of this algorithm, and the challenge of explaining
to users what has been done, probably explain why it has fallen out of
favour in programming language circles. A simpler family of algorithms, which
trace their roots to \citet{fischer79locally}, instead try to find a single minimum cost
\emph{repair sequence} of token insertions and deletions which allow the parser to
recover. Algorithms in this family are good at recovering from errors and are
easily adapted to give human-friendly feedback. However, they
have seen little practical use because their typical
performance was seen as poor and their worse case unbounded \cite[p.~14]{mckenzie95error}.

In this paper we test the following hypothesis:

\begin{description}
  \item[H1] The complete set of minimum cost repair sequences can nearly
    always be found in acceptable time.
\end{description}

We define `acceptable time' as 0.5s for error recovery per file (i.e.~all errors
in a file must be recovered from within 0.5s) since we think that even the most
demanding user will tolerate such a delay. We strongly validate this hypothesis,
repairing \mfsuccessrate of files within this time limit. While, relative to
previous approaches in the \citet{fischer79locally} family we have quite clearly
benefited from faster modern hardware, it is important to note that we have
stated a stronger, and thus harder hypothesis than previous approaches: where
they have aimed to find only a single minimal cost repair sequence, we are able
to find the complete set.

The complete set of minimal cost repair sequences allows us to display
higher quality, and more programmer
friendly, error messages (see Figure~\ref{fig:javaerror} for an example).
Once we have one or more repair sequences, we then have to choose one
to repair the input, and allow parsing to continue as normal. Rather
than pick an arbitrary candidate, we rank repair sequences by how
far they allow parsing to continue successfully, and choose from the
subset that gets furthest. We thus also test a second hypothesis:

\begin{description}
  \item[H2] Choosing a minimum cost repair sequence $s$ which allows local
parsing to continue further than a different minimum cost repair sequence $s'$
reduces the cascading error problem.\lukas{This hypothesis is not very punchy. Maybe we can shorten it to: Choosing minimum cost repair sequences which allow local parsing to continue further reduce the cascading error problem.}
\end{description}

We also strongly validate this hypothesis. We do this by comparing `normal' \mf
with a simple variant \mfrev which always selects the worst minimal cost repair
sequence: \mfrev leads to \mfreverrorlocsratioovermf more errors being raised
(i.e.~it substantially worsens the cascading error problem).

We first use one of the more recent algorithms in this family -- that of
\citet{corchuelo02repairing} -- as a base, correcting and substantially extending it
to form a new algorithm \cpctplus (Section~\ref{corchueloplus}). We
then show that an even newer algorithm which promises better performance -- that
of \citet{kimyi10astar} -- has problems which cause it to miss many
minimum cost repair sequences (Section~\ref{kimyi}). However, we are able to use
it as partial inspiration for an entirely new error recovery algorithm \mf
(Section~\ref{mf}). We aim for both algorithms to be as simple as possible, so that
they are realistic targets for tool builders: \cpctplus is somewhat
simpler than \mf, though the latter is still less than 1,000 lines of Rust code.

We then validate \cpctplus and \mf on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}). Within the
timeout of 0.5s: \cpctplus is able to find repairs for \cpctplussuccessrate
of files, with a mean recovery time of \cpctplusmeantime; and
\mf is able to find repairs for \mfsuccessrate of files, with
a mean recovery time of \mfmeantime. As this shows, both algorithms perform well,
though \mf gets slightly closer to the ideal. We believe that this shows that such
approaches are ready for wider usage, either on their own, or as part of a multi-phase
recovery system.


\section{Background}

\begin{figure}[t]
\begin{minipage}{0.44\textwidth}
\begin{subfigure}{1.0\textwidth}
\begin{lstlisting}
%start Expr
%%
Expr: Term "+" Expr      // (I)
    | Term ;             // (II)

Term: Factor "*" Term    // (III)
    | Factor ;           // (IV)

Factor: "(" Expr ")"     // (V)
      | "INT" ;          // (VI)
\end{lstlisting}
\end{subfigure}
\begin{subfigure}{1.0\textwidth}
\vspace{10pt}
\scriptsize
\begin{tabular}{c c c c c c c}
\toprule
      & \multicolumn{6}{c}{Actions}\\
        \cmidrule{2-7}
$s$ & INT  & \texttt{+}    & \texttt{*}    & \texttt{(} & \texttt{)}    & \$   \\
\midrule
0     & S(4) &       &      & S(1) &        &      \\
1     & S(4) &       &      & S(1) &        &      \\
2     &      & S(7)  &      &      & R(II)  & R(II) \\
3     &      & R(IV) & S(8) &      & R(IV)  & R(IV) \\
4     &      & R(VI) & R(VI)&      & R(VI)  & R(VI) \\
5     &      &       &      &      &        & Accept \\
6     &      &       &      &      & S(9)   &      \\
7     & S(4) &       &      & S(1) &        &      \\
8     & S(4) &       &      & S(1) &        &      \\
9     &      & R(V)  & R(V) &      & R(V)   & R(V) \\
10    &      &       &      &      & R(I)   & R(I) \\
11    &      & R(III)&      &      & R(III) & R(III) \\
\bottomrule
\end{tabular}
\begin{subfigure}{1.0\textwidth}
\vspace{15pt}
\begin{tabular}{c c c c}
\toprule
      & \multicolumn{3}{c}{Gotos}\\
        \cmidrule{2-4}
$s$ & Term & Factor & Expr\\
\midrule
0     &  2  &   3    &  5  \\
1     &  2  &   3    &  6  \\
7     &  2  &   3    & 10  \\
8     & 11  &   3    &     \\
\bottomrule
\end{tabular}
\end{subfigure}
\end{subfigure}
\end{minipage}
\begin{minipage}{0.55\textwidth}
\begin{subfigure}{1.0\textwidth}
\vspace{-7pt}
\includegraphics[width=1.0\textwidth]{graph}
\end{subfigure}
\end{minipage}
\vspace{-5pt}
\caption{An example grammar (top left), its corresponding stategraph (right), and statetable
(split into separate action and goto tables; bottom left). Productions in
the grammar are labelled \texttt{(I)} to \texttt{(VI)}. In the stategraph: S($x$)
means `shift to state $x$'; R($x$) means `reduce production $x$ from the
grammar' (e.g.~the state 3, terminal `\texttt{+}' reduction is of production IV
i.e. the production `\texttt{Term: Factor;}').
  Each item within a state $[N \colon \alpha \bullet \beta]$ references one
  of the rule $N$'s productions; $\alpha$ and $\beta$ each
represent zero or more symbols; with the \emph{dot} ($\bullet$) representing
  how much of the production must have been matched ($\alpha$) if parsing has
  reached that state, and how much remains ($\beta$).}
\label{fig:stategraphtable}
\label{fig:exprgrammar}
\end{figure}

We assume a high-level understanding of the mechanics of parsing in this paper,
but in this section we provide a handful of definitions, and a brief refresher
of relevant low-level details, needed to understand the rest of this paper.
Although the parsing tool we created for this paper is written in Rust, we
appreciate that this is still an unfamiliar language to most readers: code examples
are therefore given in Python which, we hope, is familiar to most.

Although there are many flavours of parsing, the \citet{fischer79locally} family
of algorithms, including those presented in this paper, are based on LR
parsing~\cite{knuth65lllr}. As well as describing the largest practical set
of unambiguous grammars, LR parsing remains one of the most widely used parsing
approaches due to the ubiquity of Yacc~\cite{johnson75yacc} and its
descendants (which include the Rust parsing tool we created for this paper).
We use Yacc syntax throughout this paper so that
examples can easily be tested in Yacc-compatible parsing tools.

Yacc-like tools take in a Context-Free Grammar (CFG) and produce a parser from
it. The CFG has one or more \emph{rules}; each rule has a name and one or more
\emph{productions} (often called `alternatives'); each production contains one
or more \emph{symbols}; and a symbol is either a \emph{terminal} (i.e.~a token
type such as \texttt{INT}) or a \emph{nonterminal} (i.e.~a reference to another rule in
the grammar). One rule is designated the \emph{start rule}. The resulting parser
takes as input a stream of tokens, each of which has a \emph{type}
(e.g.~\texttt{INT}) and a \emph{value} (e.g.~\texttt{123}).\footnote{In practise, the
system we outline requires a \emph{lexer} which splits string inputs up into
tokens. In the interests of brevity, we assume the existence of a tool such as
Lex which performs this task.} Strictly speaking, parsing is the act of
determining whether a stream of tokens is correct with respect to the underlying
grammar. Since this is rarely useful on its own, Yacc-like tools allow grammars
to specify `semantic actions' which are executed when a production in the grammar is
successfully matched. In this paper, we assume that the semantic actions build
a \emph{parse tree}, ordering the tokens into a tree of nonterminal nodes (
which can have children) and terminal nodes (which cannot have children)
relative to the underlying grammar.

The beauty of LR parsing stems from the way a parser is constructed. The CFG is
first transformed into a \emph{stategraph}, which is a statemachine
where each node contains one or more \emph{items} (describing the valid
parse states at that point) and edges are labelled with terminals or
nonterminals. Since even on a modern machine, a canonical
(i.e.~unmerged) LR stategraph for a real-world grammar takes several seconds to
build, and a surprising amount of memory to store, we use the state merging
algorithm of \citet{pager77practical} to merge together compatible
states.\footnote{Unfortunately \citet{pager77practical} can over-merge states when conflict
resolution is used \cite[p.~3]{denny10ielr} (i.e.~when users give Yacc an
ambiguous grammar and use its precedence rules to disambiguate the grammar).
Since our error recovery approach is intended to be independent of the merging
approach, it should be possible to use the more sophisticated state merging
approach of \cite{denny10ielr} without problems.} The
effect of this is significant, reducing the Java grammar we use later from
8908 to 1148 states. The stategraph is then transformed into a
\emph{statetable} with one row per state. Each row has a possibly empty \emph{action} (shift, reduce,
or accept) for each terminal and a possibly empty \emph{goto state} for each
nonterminal. Figure~\ref{fig:stategraphtable} shows an example grammar, its
stategraph, and statetable.

The statetable allows us to define a simple, efficient, parsing process. We
first define two functions relative to the statetable: \textsf{action}$(s, t)$
returns the action for the state $s$ and token $t$
or \emph{error} if no such action exists; and \textsf{goto}$(s, N)$
returns the goto state for the state $s$ and the nonterminal $N$. We then define
a reduction relation $\lrarrow$ for $(\textit{parsing stack}, \textit{token
list})$ pairs with two reduction rules as shown in Figure~\ref{fig:lrreduction}.
A full LR parse $\lrarrowstar$ repeatedly applies the two $\lrarrow$ rules
until neither applies, which means that \textsf{action}$(s_n, t_0)$ is either:
$\textit{accept}$ (i.e.~the input has been fully parsed); or
$\textit{error}$ (i.e.~an error has been detected at the terminal $t_0$). A
full parse of an empty file \lukas{if the file is empty how can there be t0...tn tokens?} takes a starting pair of $([0], [t_0 \ldots t_n, \$])$,
where state $0$ is expected to represent the entry point into the stategraph, $t_0 \ldots t_n$
is the sequence of input tokens, and $\$$ is the special End-Of-File (EOF) token.

\begin{figure}[t]
\small
\[
\infer[\textsc{LR Shift}]
      {([s0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_n, s'], [t_1 \ldots t_n])}
      {\textsf{action}(s_n, t_0) = \textit{shift}\ s'}
\]
\vspace{-3pt}
\[
\infer[\textsc{LR Reduce}]
      {([s0 \ldots s_n], [t_0 \ldots t_n]) \lrarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'], [t_0 \ldots t_n])}
      {(\textsf{action}(s_n, t_0) = \textit{reduce}\ N \colon \alpha)
       \wedge (\textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s')}
\]
\vspace{-15pt}
\caption{Reduction rules for $\lrarrow$ relative to $(\textit{parsing stack},
\textit{token list})$ pairs. \textsc{LR Shift}
advances the input by one token and grows the parsing stack, while
\textsc{LR Reduce} unwinds (`reduces') the parsing stack when a production is
complete before moving to a new (`goto') state.}
\label{fig:lrreduction}
\end{figure}


\section{\cpctplus}
\label{corchueloplus}

The \citet{fischer79locally} family of error recovery algorithms has many
members --- far too many to cover them all in one paper (see
\citet{cerecke03phd} for a fairly comprehensive summary). We therefore start
with one of the more recent members -- \citet{corchuelo02repairing}. We first
explain the original algorithm (Section~\ref{corchuelo:orig}), although we use
different notation than the original, fill in several missing details, and
provide a more formal definition. We then
make two correctness fixes that ensure that the algorithm can always
find minimum cost repair sequences (Section~\ref{corchuelo:kimyi}). Since the original description
gives few details as to how the algorithm might best be implemented, we then
explain the steps we took to make a performant implementation
(Section~\ref{corchuelo:implementation}). We then show how the algorithm can
be extended to efficiently find the complete set of minimum cost repair sequences
(Section~\ref{corchuelo:allminimumcost}). This allows
us to make an algorithm less susceptible to the cascading error problem
(Section~\ref{cpctplus}): we refer to this final algorithm
as \cpctplus.


\subsection{The original algorithm}
\label{corchuelo:orig}

As with all error recovery algorithms, \citet{corchuelo02repairing} is invoked
when an error is found in main parsing i.e.~when \textsf{action}$(s_n, i) =
\textit{error}$ \lukas{earlier we used action(sn, t0). Maybe we should make it consistent?}. Intuitively, the algorithm starts at the error state and tries
to find a minimum cost repair sequence consisting of: \textit{insert T}
(`insert a token of type T'), \textit{delete} (`delete the token at the current offset'),
or \textit{shift} (`skip over \lukas{better?: parse} the token at the current offset without changing it'). The
algorithm completes successfully if it reaches an accept state or shifts
`enough' tokens ($N_\textit{shifts}$; set at 3 in \citet{corchuelo02repairing}),
or unsuccessfully if it consumes `too many' tokens ($N_\textit{total}$; set at
10 in \citet{corchuelo02repairing}).

\begin{figure}[tb]
\small
\[
\infer[\textsc{CR Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_0 \ldots t_n], [\textit{insert}~t])}
      {\textsf{action}(s_n, t) \ne error \wedge t \ne \${}
       \wedge ([s_0 \ldots s_n], [t, t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_0 \ldots t_n])}
\]
\vspace{-6pt}
\[
\infer[\textsc{CR Delete}]
      {([s_0 \ldots s_n], [t_0, t_1 \ldots t_n])
       \crarrow
       ([s_0 \ldots s_n], [t_1 \ldots t_n], [\textit{delete}])}
      {t_0 \ne \$}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 1}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n])
       \crarrow
       ([s'_0 \ldots s'_m], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_j \ldots t_n])
 \wedge 0 < j \leq N_\textit{shifts}
\\
 j = N_\textit{shifts} \vee
 \textsf{action}(s'_m, t_j) \in \{\textit{accept}, \textit{error}\} 
\end{array}}
\]
\vspace{-7pt}
\caption{The repair-creating reduction rules \cite{corchuelo02repairing}.
\textsc{CR Insert} finds all terminals reachable from the current state and
creates insert repairs for them (other than the EOF token `$\$$').
\textsc{CR Delete} creates deletion repairs if user defined input remains.
\textsc{CR Shift 1} parses at most $N_\textit{shifts}$ tokens; if it reaches an accept or error
state, or parses exactly $N_\textit{shifts}$ tokens, then a shift repair per
token shifted is created.}
\label{fig:corchuelo:reductions}
\vspace*{-10pt}
\end{figure}


\begin{figure}[tb]
\begin{multicols}{2}
\begin{lstlisting}[numbers=left]
def corchueloetal(pstack, toks):
  todo = [[(pstack, toks, [])]]
  cur_cst = 0
  while cur_cst < len(todo):
    if len(todo[cur_cst]) == 0:
      cur_cst += 1
      continue
    n = todo[cur_cst].pop()
    if action(n[0][-1], n[1][0]) == <!{\textrm{\textit{accept}}}!> \
       or ends_in_N_shifts(n[2]):
      return n
    elif len(n[1]) - len(toks) == N_total:
      continue
    for nbr in all_cr_star(n.0, n.1):
      if len(n[2]) > 0 and n[2][-1] == <!{\textrm{\textit{delete}}}!> \
         and nbr[2][-1] == <!{\textrm{\textit{insert}}}!>:
        continue
      cst = cur_cst + rprs_cst(nbr[2])
      for _ in range(len(todo), cst):
        todo.push([])
      todo[cst].append((nbr[0], nbr[1], \
                        n[2] + nbr[2]))
  return None

def rprs_cst(rprs):
  c = 0
  for r in rprs:
    if r == <!{\textrm{\textit{shift}}}!>: continue
    c += 1
  return c

def all_cr_star(pstack, toks):
  # Exhaustively apply the <!{$\crarrowstar$}!> relation to
  # (pstack, toks) and return the resulting
  # list of (pstack, toks, repair) triples.
\end{lstlisting}
\columnbreak
\vspace*{-18pt}
\caption{Our version of the \citet{corchuelo02repairing} algorithm. The main function
\texttt{corchueloetal} takes in a (\emph{parsing stack}, \emph{token list})
pair and returns: a (\emph{parsing stack}, \emph{token list}, \emph{repair
sequence}) triple where \emph{repair sequence} is guaranteed to be a minimum
cost repair sequence; or \texttt{None} if it failed to find a repair sequence.\\[9pt]
%
The algorithm maintains a todo list of lists (modelled
on \citet[p.~25]{cerecke03phd} rather than the suggestion
in \citet{corchuelo02repairing}); the first sub-list contains
configurations \lukas{first mention of this term. maybe add (i.e. repair sequence)?} of cost 0, the second sub-list contains configurations of cost 1 and
so on. The todo list is initialised with the error parsing stack, remaining
tokens, and an empty repair sequence (line 2). If there are todo items left, a
lowest cost configuration $n$ is picked (line 4--8). If $n$ represents an accept state (line
9) or if the last $N_\textit{shifts}$ repairs are shifts (line 10), then $n$
represents a minimum cost repair sequence and the algorithm terminates
successfully (line 11). If $n$ has already consumed $N_\textit{total}$ tokens,
then it is discarded (lines 12, 13). Otherwise, $n$'s neighbours
are gathered using the $\crarrow$ relation (lines 14, 32--35). To avoid
duplicate repairs, \textit{delete} repairs never follow \textit{insert} repairs
(lines 15--17). Each neighbour has its repairs costed (line 18) and is then
assigned to the correct todo sub-list (lines 21--22) after they were
added to the todo list (line 19-20).\\[9pt]
%
The \texttt{rprs\_cst} function returns the cost of a repair sequence. Inserts
and deletes cost 1, shifts 0.}
\label{fig:corchuelo:algorithm}
\end{multicols}
\vspace*{-15pt}
\end{figure}

As with the original, we explain the approach in two parts. First is a new reduction relation
$\crarrow$ which defines when individual repairs are created
(Figure~\ref{fig:corchuelo:reductions}). Second is an algorithm which
determines when to use the $\crarrow$ relation (Figure~\ref{fig:corchuelo:algorithm}).
As well as several changes for clarity, the biggest difference is that
Figure~\ref{fig:corchuelo:algorithm} captures semi-formally what
\citet{corchuelo02repairing} explain in prose (spread amongst
several topics over several pages): perhaps inevitably
we have had to fill in several missing details. For example,
\citet{corchuelo02repairing} do not define what the cost of repairs is: for
simplicities sake, we define the cost of \textit{insert} and \textit{delete} as
1, and \textit{shift} as 0.\footnote{It is trivial to extend this to variable
token costs if desired, and our implementation supports this. However, it is
unclear whether non-uniform token costs are useful in practise
\cite[p.96]{cerecke03phd}.}


\subsection{Ensuring that minimum cost repair sequences aren't missed}
\label{corchuelo:kimyi}

\textsc{CR Shift 1} has two flaws that mean that it is
unable to generate some minimum cost repair sequences.

\begin{figure}[tb]
\small
\[
\hspace{-12pt}
\infer[\textsc{CR Shift 2}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_j \ldots t_n], [\underbrace{\textit{shift} \ldots \textit{shift}}_j])}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s'_m], [t_j \ldots t_n])
 \wedge 0 \leq j \leq N_\textit{shifts}
\\
 (j = 0 \wedge [s_0 \ldots s_n] \ne [s'_0 \ldots s'_m]) \vee j = N_\textit{shifts} \vee
 \textsf{action}(s'_m, t_j) \in \{\textit{accept}, \textit{error}\}
\end{array}}
\]

\[
\infer[\textsc{CR Shift 3}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n]) \crarrow ([s'_0 \ldots s'_m], [t_j \ldots t_n], R)}
      {%
\begin{array}{c}
([s_0 \ldots s_n], [t_0 \ldots t_n]) \lrarrowstar ([s'_0 \ldots s_m], [t_j \ldots t_n])
 \wedge 0 \leq j \leq 1
\\
 (j = 0 \wedge [s_0 \ldots s_n] \ne [s'_0 \ldots s_m] \wedge R = [])
  \vee
  (j = 1 \wedge R = [\textit{shift}])
\end{array}
}
\]
\vspace{-10pt}
\caption{\textsc{CR Shift 2} fixes the problem that
\textsc{CR Shift} always forces a shift to occur, which misses repair sequences
whose final repair is a delete: it allows reductions/gotos to occur without
a corresponding shift. \textsc{CR Shift 3} fixes the problem that both
\textsc{CR Shift 1} and \textsc{CR Shift 2} generate multiple shift
repairs in one go, skipping `intermediate' repair positions. \textsc{CR
Shift 3} generates at most one shift, exploring all intermediate repair
positions.}
\label{fig:corchuelo:kimyi}
\vspace*{-8pt}
\end{figure}


\begin{figure}[t]
\begin{tabular}{llll}
\begin{subfigure}{0.02\textwidth}
\vspace{-31.5pt}
\caption{}
\label{lst:crshift2:crshift2}
\end{subfigure}
&
\begin{lstlisting}
Delete "3", Delete "+"
Delete "3", Shift "+", Insert "INT"
Insert "PLUS", Shift "3", Shift "+", Insert "INT"
Insert "MULT", Shift "3", Shift "+", Insert "INT"
\end{lstlisting}
&
\begin{subfigure}{0.02\textwidth}
\vspace{-31.5pt}
\caption{}
\label{lst:crshift2:crshift3}
\end{subfigure}
&
\begin{lstlisting}
<! \vspace{-27pt} !>
Insert "MULT", Shift "3", Delete "+"
Insert "PLUS", Shift "3", Delete "+"
\end{lstlisting}
\end{tabular}
\vspace{-10pt}
\caption{Given the input `\texttt{2 3 +}' and the grammar from
Figure~\ref{fig:exprgrammar}, \textsc{CR Shift 1} is unable to find any repair
sequences (because it does not perform the reductions/gotos necessary after
\textit{insert} repairs at the end of the input to reach an accept state). With
\textsc{CR Shift 2} it can find 4 minimum cost repair sequences
(\subref{lst:crshift2:crshift2}) and with \textsc{CR Shift 3} a further 2
minimum cost repair sequences (i.e.~6 in total)
(\subref{lst:crshift2:crshift3}).}
\label{fig:crshift2:example}
\end{figure}

First, \textsc{CR Shift 1} requires at least one token to be shifted. However,
after a non-shift repair, all that may be needed to reach a useful next configuration
(or indeed an \textit{accept} state) is one or more reductions/gotos via
\textsc{LR Reduce}. \textsc{CR Shift 2} in Figure~\ref{fig:corchuelo:kimyi}
shows the two-phase fix which addresses this problem.
We first change the condition $0 < j \leq N_\textit{shifts}$ to $0 \leq j \leq
N_\textit{shifts}$ (i.e.~we don't force the LR parser to consume any tokens).
However, this then opens the possibility of an infinite loop. We avoid this by
saying that, if the input is not advanced, the parsing stack must have changed.
Put another way, in either case, we require progress
to be made, even if that progress does not require consuming any input.

Second, \textsc{CR Shift 1} and \textsc{CR Shift 2} generate multiple shifts
at a time. This causes them to skip intermediate
positions from which minimum cost repair sequences may be found
(this problem, and the basis of a fix, derive from \cite[p.~12]{kimyi10astar},
though their suggestion suffers from the problem fixed by \textsc{CR Shift 1}\lukas{not 2 or 3?}).
The solution is simple: at most one shift can be generated at any one
time, so that intermediate positions are not missed. \textsc{CR Shift 3} in
Figure \ref{fig:corchuelo:kimyi} (as well as incorporating the fix from
\textsc{CR Shift 2}) generates at most one shift repair at a time. Relative to
\textsc{CR Shift 1}, it is simpler, though it also inevitably slows down the
search for repair sequences, as more are tried.

The problems with \textsc{CR Shift 1}, in particular, can be severe.
Figure~\ref{fig:crshift2:example} shows an example input where \textsc{CR Shift
1} is unable to find any repair sequences, \textsc{CR Shift 2} some, and
\textsc{CR Shift 3} all minimum cost repair sequences.


\subsection{Implementation considerations}
\label{corchuelo:implementation}

The definitions we have given thus far do not obviously lead to
an efficient implementation and \citet{corchuelo02repairing} give few useful
hints. We found that three techniques were both
effective at improving performance while being simple to implement.

First, rather than use a general queue data-structure (probably based on a
tree), we use a similar queue data-structure to
\citet[p.~25]{cerecke03phd}. This consists of one sub-list per cost (i.e.~ the
first sub-list contains configurations
of cost 0, the second sub-list contains configurations of cost 1 and so on).
Since we always know what cost we are currently investigating,
finding the next todo element requires only a single \texttt{pop}
(line 8 of Figure~\ref{fig:corchuelo:algorithm}). Similarly,
adding elements requires only an \texttt{append} to the relevant sub-list
(lines 18, 21, 22). This
data-structure is a good fit because costs never get
big (double digits is unusual for real-world
grammars); and each neighbour generated from a configuration with cost $c$ has
a cost $\geq c$ (i.e.~we never have to search \lukas{search how? I'm not sure what this is saying} previous cost sub-lists \lukas{if the previous sublists have a smaller cost why don't we have to search them? aren't they better?}).

\label{duplicateconfigurations}
Second, the algorithm often creates configurations which, from the search's perspective
are duplicates. Formally, we say that two configurations are duplicates if their parsing
stack and remaining input are identical (i.e.~filtering out a duplicate
simply removes its repair sequence from consideration). We
therefore modify the queue data-structure to be a
list-of-ordered-hashsets\footnote{An ordered hashset preserves insertion order,
and thus allows list-like integer indexing as well as hash-based lookups.}. This has
near-identical \texttt{append} / \texttt{pop} performance to a normal list, but
weeds out duplicates with near-identical performance to an unordered hashset.
We hash configurations, and compare them for equality, based on their parsing stack and
remaining input (i.e.~the repair sequence is not used for hashing or
equality). Although this cannot detect duplicates when the
relevant configuration has already been popped from
the todo list \lukas{just double-checking: this means that there may be configurations that we don't really have to look at since they are duplicates of other configurations. But since that other configuration has already been removed, we can't remove this one as well and have to unnecessarily process it.}, it finds the vast majority, while avoiding the impractical memory overhead of
remembering all past configurations. Depending on the example, this can lead to 20\% or
more of configurations being filtered out, with a commensurate effect on
performance.

\label{cactusconfigurations}
Third, we do not use lists to represent parsing stacks and repair sequences
as Figure~\ref{fig:corchuelo:algorithm} may suggest. We found
that this representation consumes noticeably more memory, and is slightly less
efficient, than using parent pointer trees (often called `cactuses').
Every node in such a tree has a reference to a single parent (or \texttt{null} for the
root node) but no references to child nodes. Since our implementation is written
in Rust -- a language without garbage collection -- we reference
count nodes (a parent is only freed when it is not in a todo list and
no children point to it). When the error recovery algorithm starts, it
converts the main parsing stack (a list) into a parent pointer tree; and
repair sequences start as empty parent pointer trees. The $\crarrow$ part
of our implementation thus operates exclusively on parent pointer trees.
Although this does mean that neighbouring configurations are scattered throughout
memory, the memory sharing involved seems to offset the expected
poor cache behaviour; it also seems to be a good
fit with modern \texttt{malloc} implementations, which are particularly
efficient when allocating and freeing objects of the same size.
However, it is quite possible that a different representation would be
better for a garbage collected language.


\subsection{Finding all minimum cost repair sequences}
\label{corchuelo:allminimumcost}

The algorithm as described to this point non-deterministically completes as soon
as it has found a single minimum cost repair sequence.
In this section we show how we can efficiently find
the complete set of minimum cost repair sequences.

The basis of a solution is simple: when a repair sequence of cost $c$ is found to
be successful, we discard all repair sequences with cost $> c$, and continue
exploring configurations in cost $c$ (including, transitively, all neighbours that are
also of cost $c$), recording each successful configuration we encounter. However, this
requires that we do not throw away duplicate configurations, as they may contain distinct
repair sequences. Unfortunately, turning off duplicate detection is a big
performance loss, not least because there can be
many remaining configurations in $c$, which may, transitively, have many neighbours.

We therefore need a scheme which gives us most of the performance benefit of
duplicate detection but which still records all possible repair sequences.
Our solution is to remove duplicate detection and instead merge together
\emph{compatible} configurations, preserving their distinct repair sequences while still
reducing the search space. Two configurations are compatible if: their
parsing stacks are identical; they both have an identical amount of input
remaining; and their repair sequences are compatible. Two repair sequences
are compatible:

\begin{enumerate}
   \item if they both end in the same number ($n \ge 0$) of shifts.
   \item if one repair sequence ends in a delete, the other repair sequence also
ends in a delete.
\end{enumerate}

The first condition is a result of having to take into account
that a configuration can be deemed successful in two ways: if it reaches an accept state (line 9,
Figure~\ref{fig:corchuelo:algorithm}) or if it ends in $N_\textit{shifts}$ shift
repairs (line 10). The former case is only affected by the parsing stack (and
thus doesn't affect compatible configurations which have identical parsing stacks),
but the latter requires care with the number of shift repairs at
the end of the sequence.\lukas{I feel like this can be expanded a bit. It doesn't really explain WHY these conditions mean we can merge the configurations. Also this seems to suggest that in Fig9 there should be more merges as there are other configurations that fulfill those conditions, e.g. all success configs except the one ending with a delete! Maybe we need an additional example here for two or more configurations that can be merged or we need to forward reference Fig9}

The second condition relates to the weak form of compatible merging inherited
from \citet[p.~8]{corchuelo02repairing}: delete repairs are never followed by an
insert (see Figure~\ref{fig:corchuelo:algorithm}) since [\textit{delete},
\textit{insert x}] always leads to the same configuration as [\textit{insert x},
\textit{delete}]. Although this is largely subsumed
by compatible configuration merging, we keep it as a separate optimisation because: it is
such a frequent case; our
use of the todo list means that we would not catch every case; the
duplicate repair sequences are uninteresting from a user perspective, so we
would have to weed them out later anyway; and each additional merge costs
memory.

Our implementation of compatible configurations subsumes our detection of duplicate configurations
(Section~\ref{duplicateconfigurations}), relying on the fact that our todo data-structure
is a list-of-ordered-hashsets. We need only loosen configuration equality to allow compatible
repair sequences to be considered as equal. This leads to efficient
detection of compatible configurations.

Conceptually, merging two configurations together is simple: each configuration simply
needs to store a set of repair sequences, each of which is updated as further
repairs are found. However, this is an extremely inefficient representation as
the sets involved need to be copied and extended as each new repair is found.
Instead, we use a representation which relies on parent pointer trees (for the
same reasons as in Section~\ref{cactusconfigurations}).
The basic idea is that configurations no longer reference a parent pointer tree of
repairs directly, but instead a parent pointer tree of \emph{repair merges}. A
repair merge is a pair (\textit{repair}, \textit{merged}) where
\textit{repair} is a plain repair and \textit{merged} is a (possibly null) set of
repair merge sequences \lukas{This sentence has so many `pairs' that it made me hungry for fruit. Maybe we can rewrite this so it's easier to read?}. The reason for using a pair apart is twofold \lukas{what is this saying? what pair is apart? I thought we are separating configurations}: we can quickly
perform the compatibility checks on \textit{repair} alone; and we avoid allocating
additional memory for configurations which have not yet been subject to a merge. The small
downside to this scheme is that expanding configurations into repair sequences requires
recursively expanding both the normal parent pointer tree of \textit{repair}s \lukas{why is this plural and merged is not?}
and the merged parent pointer trees of \textit{merged}.


\subsection{Putting together the \cpctplus algorithm}
\label{cpctplus}
\label{rankingrepairs}

The \textsc{CR Shift 3} rule and our ability to find the complete
set of minimum cost repair sequences are two of the three key ingredients
in our our new error recovery algorithm. In this subsection
we make one further addition to the algorithm, calling the result \cpctplus (in
homage to its roots in \citet{corchuelo02repairing}).

\begin{figure}[t]
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:ranking:java}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
class C {
    T x = 2 +
    T y = 3;
}
\end{lstlisting}
\end{minipage}
&
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:ranking:output}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Error at line 3 col 7. Repairs found:
  Insert "COMMA"
\end{lstlisting}
\end{minipage}
\end{tabular}
\vspace{-10pt}
\caption{An example showing how the ranking of repair sequences can lessen the
cascading error problem. The Java example (\subref{lst:ranking:java}) leads
to a parsing error on line 3 at `\texttt{y}', with three minimum cost repair
sequences found: [\textit{insert} \texttt{,}], [\textit{insert} \texttt{?}], and
[\textit{insert} \texttt{(}]. These repair sequences are then ranked by how far
they allow parsing to continue successfully. [\textit{insert} \texttt{,}] leads
to the rest of the file being parsed without further error. [\textit{insert}
\texttt{?}] causes a cascading error at `\texttt{;}` which must then be resolved
by completing the ternary expression started by `?' (e.g.~changing line 3 to
\texttt{T ? y : this;}). Similarly, [\textit{insert} \texttt{(}] causes a
cascading error at `\texttt{;}' which must then be resolved by inserting a
`\texttt{)}'. Since [\textit{insert} \texttt{,}] is ranked more highly than the
other repair sequences, the latter are discarded, leading to the parsing output shown
in (\subref{lst:ranking:output}). javac in contrast attempts to insert
`\texttt{;}' before `\texttt{y}' causing a cascading error on the next token.}
\label{fig:ranking}
\end{figure}

The final step in our new algorithm allows us to somewhat compensate for the
small value of $N_\textit{shifts}$. This value has to be a small integer (we
use 3, the value suggested by \citet{corchuelo02repairing})
because each additional token searched exponentially increases the
search space. Thus the repair sequences we find can be less than ideal when further
user input is considered. Fortunately, we can use the
complete set of minimum cost repair sequences to lessen this weakness.
After we have generated the set of configurations which represents the
complete set of minimal cost repair sequences, we then rank the
configurations by how far they allow parsing to continue, up to a limit of
$N_\textit{try}$ tokens (which we somewhat arbitrarily set at 250). The reason
why we rank the configurations, and not the repair sequences, is that we only
need to rank one repair sequence for each merged configuration, a useful
optimisation. We then expand the top ranked configurations into repair
sequences, remove shifts from the end of repair sequences, and
remove duplicates.

Particularly on real-world grammars, selecting the top-ranked sequences
substantially decreases cascading errors sequence causing cascading errors (see
Figure~\ref{fig:ranking} for an example).
It also does so for very little additional computational cost as the complete set of
minimum cost repair sequences is much smaller than the number of configurations
searched.

\begin{figure}[t]
\includegraphics[width=1.0\textwidth]{cpctplus}
\caption{An elided visualisation of the search tree for \cpctplus with the input
`2 3 +' and the grammar from Figure~\ref{fig:exprgrammar}. The left hand side of
the tree shows the `normal' parser at work, which hits an error as soon as it
has shifted the token `\texttt{2}': at this point, \cpctplus starts operating.
As this shows, the search encounters various dead ends, as well as successful
routes. As shown in Figure~\ref{fig:crshift2:example}, this input has 6 minimal
cost repair sequences, but the search only has 5 success configurations, because
two routes are merged together.}
\label{fig:cpctplus:full}
\end{figure}

Figure~\ref{fig:cpctplus:full} shows a visualisation of \cpctplus
operating on the input `2 3 +' and the grammar from Figure~\ref{fig:exprgrammar}.


\section{The Kim and Yi algorithm}
\label{kimyi}

Algorithms in the \citet{fischer79locally} family have unbounded worse case
performance, which is easily seen on inputs with unbalanced brackets
(e.g.~expressions such as `\texttt{x = f(();}'): each additional unmatched
bracket exponentially increases the search space. On a modern machine
with a Java grammar, the \cpctplus algorithm takes about 0.3s to find
the complete set of minimum cost repair sequences for 3 unmatched brackets, 3s
for 4 unmatched brackets, and 6 unmatched brackets caused our 32GiB test machine
to run out of RAM.

While such cases are fairly rare, it would be better if they did not occur at
all. \citet{kimyi10astar} propose a new error recovery -- which is in the
\citet{fischer79locally} family, but by far its most radical member --  which
claims to hugely reduce such performance problems. This work has not,
to the best of our knowledge, received prior attention in the community, despite
this promise. In this section we provide a brief overview of this work: we
give enough information to understand relevant details but
we elide several details which an
implementation would need to consider. We also adjust the algorithm's style to
match this paper's and correct several minor mistakes.

We then show that the algorithm contains two serious flaws
which cause it to miss minimum cost repair sequences
(Section~\ref{kimyi:flaws}). These flaws are not easily fixed, but parts of the
approach serve as inspiration for our new \mf error recovery algorithm.


\subsection{An overview of the algorithm}
\label{kimyi:overview}

The \citet{kimyi10astar} algorithm takes \citet{corchuelo02repairing} as a base,
adding two significant novelties: it uses the A*
algorithm~\cite{hart68astar} to delay, and thus often to avoid, unpromising
configurations; and it can insert non-terminals, avoiding many inserts
of terminals entirely. This allows examples with
thousands of unmatched brackets to be repaired in a few seconds.

Since the A* algorithm is not a particularly common one in parsers,
we first start with a brief recap. The A* algorithm finds minimum cost paths through
a graph where each edge has an associated cost $c$. The current lowest
cost to reach a node $n$ from the start node is represented by $d(n)$. A
heuristic $h(n)$ returns an estimate of the additional cost needed to reach a success node from
$n$. The heuristic must be `admissible': it must
never overestimate the cost to reach a success node (or else non-minimal
cost routes to success nodes may be found first); however, it may safely
underestimate (i.e.~the simplest safe heuristic is $h(n) = 0$). A priority queue is
used to order nodes by their $d(n) + h(n)$. On
each iteration the node in the queue with the lowest $d(n) + h(n)$ is selected,
its neighbours explored and each entered into the priority queue. The search
terminates when the first success node is found.

\begin{figure}
\small
\[
\infer[\textsc{KY Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n], w) \kyarrow ([s_0 \ldots s_n, s'],
       [t_0 \ldots t_n], \top, [\textit{insert}~t], d)}
      {s_n \xrightarrow{\text{~~t~~}} s' \wedge t \ne \${} \wedge d = \textsf{ky\_dist}(s', t_0) < \infty)}
\]
\vspace{-3pt}
\[
\infer[\textsc{KY Reduce}]
      {([s0 \ldots s_n], [t_0 \ldots t_n], \bot) \kyarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'],
       [t_0 \ldots t_n], \bot, [\textit{insert}~\beta_0 \ldots \textit{insert}~\beta_n], 0)}
      {[N: \alpha \bullet \beta_0 \ldots \beta_n] \in \textsf{core}(s_n)
       \wedge \textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s'}
\]
\vspace{-4pt}
\[
\infer[\textsc{KY Delete}]
      {(S, [t_0, t_1 \ldots t_n], \bot) \kyarrow (S, [t_1 \ldots t_n], \bot, [\textit{delete}], 0)}
      {t_0 \ne \$}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{KY Shift}]
      {(S, [t_0 \ldots t_n], w) \kyarrow (S', [t_1 \ldots t_n], \bot, [\textit{shift}], 0)}
      {(S, [t_0 \ldots t_n]) \lrarrowstar (S', [t_1 \ldots t_n])}
\]
\vspace{-12pt}
\caption{The repair-creating rules for \citet{kimyi10astar} operate from (\textit{parsing
stack}, \textit{token list}, \textit{w}) to (\textit{parsing stack},
\textit{token list}, \textit{w}, \textit{repairs}, \textit{heuristic}) tuples.
\textsc{KY Insert} finds terminals in the stategraph
($\xrightarrow{\text{~~t~~}}$) which lead to a state with a finite distance to
the next input token as \textit{insert} repairs. \textsc{KY Reduce} finds items
in the core (or `kernel') state which would lead to a reduction if the
sequence of symbols (terminals and non-terminals) $\beta_0 \ldots \beta_n$ were
to be found; it then optimistically creates insert repairs for each, and
performs the corresponding reduction. \textsc{KY Delete} is virtually identical
to \textsc{CR Delete}. \textsc{KY Shift} is similar to \textsc{CR Shift} but
has to always shift a single symbol to avoid creating repairs which
duplicate those found by \textsc{KY Reduce}.}
\label{fig:kimyi:rules}
\end{figure}

The \citet{kimyi10astar} algorithm itself comes in two main parts. First is a
relation $\kyarrow$ which defines when repairs are created
(Figure~\ref{fig:kimyi:rules}). Unlike the $\crarrow$ relation, these rules are
from (\textit{parsing stack}, \textit{token list}, \textit{w}) to
(\textit{parsing stack}, \textit{token list}, \textit{w}, \textit{repairs},
\textit{heuristic}) tuples. Of these values, $w$ is the least intuitive:
since the A* heuristic used doesn't take into account reductions/gotos or deletions,
it is only valid for sequences of \textsc{KY Insert} and \textsc{KY Shift};
were \textsc{KY Reduce} or \textsc{KY Delete} to be used, the heuristic
would be inadmissible. The initial configuration sets $w = \bot$. Since \textsc{KY Insert} is the only
rule which uses the heuristic, it sets $w = \top$. Only \textsc{KY Shift} can
turn $w = \top$ into $\bot$.

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.63\textwidth}
\begin{lstlisting}[numbers=left]
def mk_ky_table(sgraph, terms, eof_idx):
  table = [<!$\infty$!>] * sgraph.len_states * len(terms)
  table[sgraph.accept_state * len(terms) \
        + eof_idx] = 0
  while True:
    chgd = False
    for i in range(sgraph.len_states):
      for sym, end_st in sgraph.edges(i):
        if isinstance(sym, Nonterminal):
          d = min_sentence_cost(sym.nt_idx)
        else:
          assert isinstance(sym, Terminal)
          off = i * len(terms) + sym.term_idx
          if table[off] != 0:
            table[off] = 0
            chgd = True
          d = 1
        for j in range(len(terms)):
          this_off = i * len(terms) + j
          end_off = end_st * len(terms) + j
          if table[end_off] != <!$\infty$!>:
            other_cost = table[end_off] + d
            if other_cost < table[this_off]:
              table[this_off] = other_cost
              chgd = True
    if not chgd:
      break
\end{lstlisting}
\end{adjustbox}%
\small
\begin{adjustbox}{valign=t,minipage=.35\textwidth}
\begin{tabular}{rcccccc}
\toprule
      & \multicolumn{6}{c}{$t$} \\
        \cmidrule(lr){2-7}
$s$ & INT & \texttt{+} & \texttt{*} & \texttt{(} & \texttt{)} & \$ \\
\midrule
0 & 0 & 1 & 1 & 0 & 2 & 1 \\
1 & 0 & 1 & 1 & 0 & 1 & $\infty$ \\
2 & 1 & 0 & 2 & 1 & 3 & $\infty$ \\
3 & 1 & 3 & 0 & 1 & 3 & $\infty$ \\
4 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
5 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & 0 \\
6 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & 0 & $\infty$ \\
7 & 0 & 1 & 1 & 0 & 2 & $\infty$ \\
8 & 0 & 2 & 1 & 0 & 2 & $\infty$ \\
9 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
10 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
11 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-5pt}
\caption{A fixed-point algorithm for the \textit{ky\_dist} distance table (left)
and a distance table for the grammar from Figure~\ref{fig:exprgrammar} (right).
The algorithm takes in the stategraph, an ordered list of
terminals, and the index of the \$ terminal (line 1), and returns a table with
one column per terminal and one row per state. Each entry starts
at $\infty$ (line 2) except for the accept state's (i.e.~the only state with
an \textit{accept} action) `\$' terminal which is set
to 0 (lines 3, 4): entries monotonically reduce to a minimum of 0. For each
state $i$ in the stategraph (line 7), the algorithm explores its outgoing edges
(via the \texttt{edges} function (line 8)), each of which is labelled with a
symbol \texttt{sym} and points to another state with index \texttt{end\_st}. If
the edge's symbol is a terminal then by definition the cost of reaching that
terminal from state $i$ is 0 (lines 13-16). We then calculate the cost of the
edge: if the label's symbol is a nonterminal, we return the cost of the minimal
sentence matching that nonterminal (line 10); if a terminal we return a cost of 1 (line 17).
If the cost of $t$ in \texttt{end\_st} plus the distance to \texttt{end\_st} is
lower than the cost of $t$ in $i$ then we update the latter (lines 19--25).}
\label{fig:kimyi:dist}
\end{figure}

The A* heuristic used is \textsf{dist(s, t)} which is best
summarised as follows: if we are in parsing state $s$, what is the cost of the
minimum route through the stategraph to a state $s'$ (where $s$ and $s'$ may be
the same state), where $s'$ has the terminal $t$ as an outgoing edge? In other
words, this gives us the cost of inserting symbols to reach a state $s'$
where $t$ is a valid next token. If no such route
exists, \textsf{ky\_dist} returns $\infty$. While \citet{kimyi10astar} provide a
specification of \textsf{ky\_dist}, we provide a fixed-point algorithm
(Figure~\ref{fig:kimyi:dist}). The result is cached in a table at the point
of grammar generation with entries looked up during the search. Note that we
assume the existence of a function \textsf{min\_sentence\_cost(N)} which returns
the length of the minimum sentence(s) which match the non-terminal $N$: this
function is a relatively simple variation of the traditional nullable
computation used in parsing.


\subsection{Problems with the algorithm}
\label{kimyi:flaws}

The combination of the $\kyarrow$ rules (in particular \textsc{KY Reduce}'s
ability to insert nonterminals) and the \textsf{ky\_dist} function lead to dramatic
performance improvements as reported by \citet{kimyi10astar}. Unfortunately the
algorithm contains three flaws which lead it to produce incorrect
results.

The first flaw is that the \textsf{ky\_dist} heuristic does not take into
account reductions/gotos or deletions: hence the heuristic is only valid for sequences of
inserts via \textsc{KY Insert}. A simple example of this can be seen for the grammar
from Figure~\ref{fig:exprgrammar} and its distance table in Figure~\ref{fig:kimyi:dist}:
\textsf{dist(3, `+')} returns 3 (which can be achieved by inserting
`\texttt{*}', `\texttt{(}', and `\texttt{INT}'): however, from
state 3, we could reduce to state 0 and goto state 2, which has an outgoing edge labelled
`\texttt{+}' (i.e.~without inserting any terminals). As this shows,
the heuristic would be inadmissible in the face of reductions.
The second flaw results from trying to fix the
first: the $w$ part of the $\kyarrow$ relation is a hack to ensure that
\textsc{KY Reduce} and \textsc{KY Delete} do not interfere when the heuristic is $> 0$.
However, the hack has unfortunate effects, sometimes trapping the search in
parsing states from which it cannot escape, and sometimes stopping it from
searching states which would produce a lower cost repair sequence.
For example, given the grammar \texttt{S: T 'b' 'c'; T: 'a';} and the input `c',
\citet{kimyi10astar} can find no repairs (whereas \cpctplus finds [\textit{insert a}, \textit{insert b}]).
Given the grammar \texttt{S: 'a' 'b' 'd' | 'a' 'b' 'c'
'a' 'a' 'd';} and the input `a c d', the algorithm incorrectly returns the repair sequence
[\textit{insert b}, \textit{shift}, \textit{insert a}, \textit{insert a}]
(whereas \cpctplus returns the minimum cost repair sequence
[\textit{insert b}, \textit{delete}].
The third and final flaw is that optimistically inserting the remainder of a
production prevents the terminals involved from being shifted if the user's
input happens to overlap with them: since it is more expensive to insert $a$
than to shift $a$, this can cause the search to produce non-minimal cost repair
sequences.

The third flaw is easily fixed by disallowing the search from optimistically
inserting the remainder of a production. The first two flaws can then be fixed
by revisiting an old friend: if we apply the same change to
\textsc{KY Shift} as we did to \textsc{CR Shift} (see
Figure~\ref{fig:corchuelo:kimyi}), the problem disappears, because
the altered \textsc{KY Shift} can perform reductions/gotos without having to
consume input. Unfortunately applying the \textsc{CR Shift 3} fix
to \textsc{KY Shift} turns the \citet{kimyi10astar} algorithm into a slower
version of \citet{corchuelo02repairing}, since reductions/gotos are now duplicated
between \textsc{KY Reduce} and \textsc{KY Shift}. Removing \textsc{KY Reduce}
turns the algorithm into an almost literal copy of \citet{corchuelo02repairing},
with the mostly minor difference that \textsc{CR Insert} operates on the
statetable and \textsc{KY Insert} on the stategraph.\footnote{Though note that
for parsers which allow conflict resolution, such as Yacc, this means that
\textsc{KY Insert} can produce results which can't be parsed by the
statetable~\cite[p.~53, 54]{cerecke03phd}.} We have been
unable to find fixes to the algorithm that maintain its claimed performance
properties.


\section{\mf}
\label{mf}

In this section we present a new recovery algorithm \mf. As with \cpctplus,
\mf finds the complete set of minimum cost repair sequences, although it does so
using the A* algorithm. While
\cpctplus and \mf find precisely the same repair sequences, \mf does so
in less time. However, \mf requires more up-front calculations that require slightly
more implementation effort: \mf is approximately 950LoC whereas \cpctplus
is approximately 700LoC.

We first provide an overview of the algorithm
(Section~\ref{mf:overview}) before describing in detail the steps needed to
calculate the new \texttt{mf\_dist} heuristic (Section~\ref{mf:mf_dist}).


\subsection{An overview of \mf}
\label{mf:overview}

\begin{figure}[t]
\small
\[
\infer[\textsc{MF Insert}]
      {([s_0 \ldots s_n], [t_0 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow ([s_0 \ldots s_n, s'], [t_0 \ldots t_n], [r_0 \ldots r_n, \textit{insert}~t], d)}
      {\textsf{action}(s_n, t) = \textit{shift}~s' \wedge t \ne \${}
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n, \textit{insert}~t], s', [t_0 \ldots t_n]) < \infty}
\]
\vspace{-3pt}
\[
\infer[\textsc{MF Reduce}]
      {([s0 \ldots s_n], [t_0 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow ([s_0 \ldots s_{n - \lvert \alpha \rvert}, s'],
       [t_0 \ldots t_n], [r_0 \ldots r_n], d)}
      {N: \alpha \in \textsf{core\_reduces}(s_n)
       \wedge \textsf{goto}(s_{n - \lvert \alpha \rvert}, N) = s'
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n], s', [t_0 \ldots t_n]) < \infty}
\]
\vspace{-4pt}
\[
\infer[\textsc{MF Delete}]
      {(S, [t_0, t_1 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow
       (S, [t_1 \ldots t_n], [r_0 \ldots r_n, \textit{delete}], d)}
      {t_0 \ne \$
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n, \textit{delete}], s_n, [t_1 \ldots t_n]) < \infty}
\]
\vspace{-3pt}
\[
\hspace{-12pt}
\infer[\textsc{MF Shift}]
      {([s_0 \ldots s_n], [t_0, t_1 \ldots t_n], [r_0 \ldots r_n])
       \mfarrow
       ([s_0 \ldots s_n, s'], [t_1 \ldots t_n], [r_0 \ldots r_n, \textit{shift}], d)}
      {\textsf{action}(s_n, t_0) = \textit{shift}~s'
       \wedge d = \textsf{mf\_dist}([r_0 \ldots r_n, \textit{shift}], s', [t_1 \ldots t_n]) < \infty}
\]
\vspace{-12pt}
\caption{The repair-creating rules for \mf from (\textit{parsing stack},
\textit{token list}, \textit{repair sequence}) to (\textit{parsing stack},
\textit{token list}, \textit{repair sequence}, \textit{heuristic}) tuples.
\textit{mf\_dist} is the \mf A*
heuristic: unlike \citet{kimyi10astar}, every rule makes use of the \textit{mf\_dist} heuristic.
\textit{core\_reduces}($s_n$) returns the set of non-terminals which can be
reduced in state $s_n$.}
\label{fig:mf:rules}
\end{figure}

At a high level -- and much of the low level -- our description of \mf is
deliberately similar to \cpctplus, hopefully allowing the reader
to both easily digest \mf and pick out the differences from \cpctplus.

We introduce a new reduction relation $\mfarrow$, whose rules are shown in
Figure~\ref{fig:mf:rules}. Their most obvious features are that the relation is
from \textit{(parsing stack, token list, repair sequence)} to \textit{(parsing
stack, token list, repair sequence, heuristic)} tuples and that each rule
uses a new A* heuristic \textsf{mf\_dist} (which takes into account reductions/gotos
and deletions; see Section~\ref{mf:mf_dist}). Less obviously, the rules do not
use the $\lrarrow$ relation: \textsc{MF Shift} and \textsc{MF Reduce} subsume
the functionality of \textsc{LR Shift} and \textsc{LR Reduce} (making this aspect of \mf
closer in spirit to \citet{mckenzie95error} than either \citet{corchuelo02repairing}
or \citet{kimyi10astar}).

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.55\textwidth}
\begin{lstlisting}[numbers=left]
def mf(pstack, toks):<!\Suppressnumber!>
    ...<!\Reactivatenumber{14}!>
    for nbr in all_mf_star(n.0, n.1, n.2):
      if len(n[2]) > 0 and n[2][-1] == <!{\textrm{\textit{delete}}}!> \
         and nbr[2][-1] == <!{\textrm{\textit{insert}}}!>:
        continue
      cst = cur_cst + rprs_cst(nbr[2]) + nbr[3]
      for _ in range(len(todo), cst):
        todo.push([])
      todo[cst].append((nbr[0], nbr[1], \
                        n[2] + nbr[2]))
  return None
\end{lstlisting}
\end{adjustbox}%
\begin{adjustbox}{valign=t,minipage=.45\textwidth}
\begin{lstlisting}[numbers=left,firstnumber=24]
def all_mf_star(pstack, toks, rprs):
  # Exhaustively apply the <!{$\mfarrowstar$}!> relation to
  # (pstack, toks, rprs) and return the
  # resulting list of (pstack, toks, rprs,
  # heuristic) tuples.
\end{lstlisting}
\end{adjustbox}
\vspace*{-8pt}
\caption{The main \mf algorithm. Lines 2--13 are identical to those of
Figure~\ref{fig:corchuelo:algorithm} (with the exception that \mf does
not return when the first success configuration is found, but stores it until
the current cost $c$ is fully evaluated; at that point it returns all
the success configurations found). The main difference between \cpctplus
and \mf is the use of the A* heuristic, which allows configurations to be deferred
beyond their actual cost. In other words a configuration $c$ with a heuristic $h$ is
placed in $\text{todo}[c + h]$ (lines 18, 21, 22).}
\label{fig:mf:algorithm}
\end{figure}

The main part of the \mf algorithm is deliberately similar to that of \cpctplus
(compare Figure~\ref{fig:corchuelo:algorithm} and Figure~\ref{fig:mf:algorithm}). Because
\textsf{mf\_dist} is both admissible (i.e.~it never overestimates the distance to a
success configuration) and consistent (i.e.~the total estimated cost to reach a success
configuration is monotonically non-decreasing) we know that neighbouring configurations will
always be the same, or greater, cost as the current configuration. This allows us to
use a simple, and efficient, variant of the A* algorithm that is similar
to the algorithm of \cpctplus with the notable difference that
\textsf{mf\_dist} is used to defer configurations later in the todo list than
\cpctplus (often not needing to explore them if less costly successful
configurations are found).
Finally, compatible configuration merging, repair sequence simplification, and so on are identical
to that in \cpctplus.


\subsection{The \texttt{mf\_dist} heuristic}
\label{mf:mf_dist}

\textsf{mf\_dist} comes in static and dynamic parts: the former handles insertion
sequences and reductions/gotos; and the latter handles deletions (which
require examining the user's input). The static part
extends the distance table algorithm used in \citet{kimyi10astar} while the dynamic part
is entirely new. In this subsection we explain both parts.

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.55\textwidth}
\begin{lstlisting}[numbers=left]
def mk_mf_table(sgraph, stable, terms, eof_idx):
  table = [<!$\infty$!>] * len(sgraph) * len(terms)
  table[sgraph.accept_state * len(terms) \
        + eof_idx] = 0
  while True:
    chgd = False
    for i in range(sgraph.len_states):
      for sym, end_st in sgraph.edges(i):<!\Suppressnumber!>
        ...<!\Reactivatenumber{26}!>
        for st_idx in goto_states(sgraph, \
                                  stable, i):
          for j in range(len(terms)):
            this_off = i * len(terms) + j
            end_off = st_idx * len(terms) + j
            if table[end_off] != <!$\infty$!> and \
               table[end_off] < table[this_off]:
              table[this_off] = table[end_off]
              chgd = True
    if not chgd:
      break
\end{lstlisting}
\end{adjustbox}
\begin{adjustbox}{valign=t,minipage=.44\textwidth}
\begin{lstlisting}[numbers=left,firstnumber=37]
def goto_states(sgraph, stable, st_idx):
  gs = set()
  for nonterm, prod, dot in \
      sgraph.core_state(st_idx):
    if dot < len(prod.syms):
      continue
    prev = set([st_idx])
    for _ in len(prod.syms):
      next = set()
      for i in prev:
        next.union(sgraph.rev_edges(i))
      prev = next
    for i in prev:
      goto_st = stable.goto(i, nonterm)
      if goto_st is not None:
        gs.add(goto_st_idx)
\end{lstlisting}
\end{adjustbox}
\caption{A fixed-point algorithm for the static component of the
\textit{mf\_dist} function, returning a distance table which takes into account
reductions/gotos. \texttt{goto\_states} returns the set of state indexes which a state
\textit{st\_idx} may end up in after all possible reductions/gotos have
occurred. To calculate this, all core items of a state with a dot at the end are
considered (lines 39--42). We then recursively iterate backwards over the graph
(using \texttt{rev\_edges(i)} which returns all the states which have an
edge pointing to state \texttt{i}), exploring all possible reduction routes that
could be encountered dynamically (lines 43--48). The underlying idea is that an
item $[N: \alpha \bullet]$ will cause $|\alpha|$ items to be popped from the
parsing stack; we thus iterate backwards $|\alpha|$ times over the stategraph
(line 44), recording at each point the states we can reach (lines 46--47).
We then take the resulting set and map each element to the state it will goto
(lines 49--52). Lines 9--25 of \texttt{mk\_mf\_table} are identical to
that of the \texttt{mk\_ky\_table} function (see Figure~\ref{fig:kimyi:dist}) (i.e.~they
follow insertion sequences). We then take into account the goto states as
part of the distance calculation (lines 26--34).}
\label{fig:mf:dist:static}
\end{figure}

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=0.38\textwidth}
\small
\begin{tabular}{rcccccc}
\toprule
      & \multicolumn{6}{c}{$t$} \\
        \cmidrule(lr){2-7}
$s$ & INT & \texttt{+} & \texttt{*} & \texttt{(} & \texttt{)} & \$ \\
\midrule
0 & 0 & 1 & 1 & 0 & 1 & 1 \\
1 & 0 & 1 & 1 & 0 & 1 & 1 \\
2 & 1 & 0 & 1 & 1 & 0 & 0 \\
3 & 1 & 0 & 0 & 1 & 0 & 0 \\
4 & 1 & 0 & 0 & 1 & 0 & 0 \\
5 & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & 0 \\
6 & 2 & 1 & 1 & 2 & 0 & 1 \\
7 & 0 & 1 & 1 & 0 & 1 & 1 \\
8 & 0 & 1 & 1 & 0 & 1 & 1 \\
9 & 1 & 0 & 0 & 1 & 0 & 0 \\
10 & 2 & 1 & 1 & 2 & 0 & 0 \\
11 & 1 & 0 & 1 & 1 & 0 & 0 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\begin{adjustbox}{valign=t,minipage=0.60\textwidth}
\vspace{-13pt}
\caption{The \textit{mf\_dist} distance table for the grammar from
Figure~\ref{fig:exprgrammar}. Relative to the \textit{ky\_dist} distance
table (Figure~\ref{fig:kimyi:dist}), many fewer entries are
$\infty$, because reductions/gotos are taken into account. For example, consider state
3, terminal `\texttt{+}': in \textit{ky\_dist}'s distance table this has a cost of 3;
in \textit{mf\_dist}'s distance table the cost is 0. The reason for this can be
clearly seen from the stategraph in Figure~\ref{fig:stategraphtable}: in state
3, the terminal `\texttt{+}' causes a reduction to state 0, and then a goto to
state 2. State 2 has an outgoing edge labelled with \texttt{+} and hence a
distance of 0 to it. Notice also that every state now has a distance to the \$
terminal, since every (reachable) state must via reductions/gotos and insertions
be able to reach the accept state.}
\label{fig:mf:dist:example}
\end{adjustbox}
\end{figure}

The A* heuristic of \citet{kimyi10astar} builds a distance table of
states and tokens: \textsf{dist(s, t)} then returns the cumulative cost of
the sequence of token insertions from state $s$ such that a state $s'$ can be
found which has an outgoing edge labelled with $t$. This is a useful base,
but we also need to take into account reductions/gotos to stop any search which
uses the heuristic from getting trapped (Section~\ref{kimyi:flaws}). Fortunately,
it is relatively easy to statically
underapproximate the effect of reductions/gotos. The basic
idea is simple: as well as taking into account the cost of a sequence of token
insertions in a distance, we also take into account all possible reduction
paths. Note that reductions/gotos and token insertions can be intertwined
(i.e.~a valid way of
reaching $s'$ might be a reduction, a token insertion, another reduction and so
on). Fortunately, not only is this a natural candidate for a fixed-point
algorithm, but we are able to follow the same structure as that used to
calculate the distance table for \textsf{ky\_dist}. Figure~\ref{fig:mf:dist:static}
shows the static part of \textsf{mf\_dist} and Figure~\ref{fig:mf:dist:example}
shows an example distance table.

\begin{figure}[t]
\begin{adjustbox}{valign=t,minipage=.38\textwidth}
\begin{lstlisting}[numbers=left]
def mf_dist(rprs, s, toks):
  if ends_in_N_shifts(rprs):
    return 0
  ld = <!$\infty$!>
  dc = 0
  while dc < ld:
    d = mf_dist_table(s, toks[0])
    if d < <!$\infty$!> and dc + d < ld:
      ld = dc + d
    dc += 1
    if len(toks) == 1:
      break
    toks = toks[1:]
  if ld == <!$\infty$!>: return None
  else: return ld
\end{lstlisting}
\end{adjustbox}%
\begin{adjustbox}{valign=t,minipage=.60\textwidth}
\vspace{-9.5pt}
\caption{\textit{mf\_dist} returns the cost to a success
configuration, taking into account reductions and deletions. It takes as input a
sequence of repairs \textit{rprs}, a state \textit{s}, and the remaining tokens
from the user's input \textit{toks} and returns an integer distance or
\texttt{None} if no route can be found. The function starts with a special case:
if the repair sequence ends in $N_\textit{shifts}$ shifts then the distance to a
success confguration is by definition 0 (lines 2--3). Otherwise the current
least distance to a success configuration is set to $\infty$ (line 4; this value
monotonically decreases as the main loop executes) and the current cost of
deleting tokens to 0 (line 5). We then continually iterate over the user's input
until either there is no input left (lines 11-12) or the cost of deleting tokens
exceeds the current least distance (line 6). On each iteration of the loop, we
lookup the static distance to the next token in the user's input (line 7). If
the distance is less than $\infty$ and the cumulative deletion cost plus the static
distance is less than the current least distance, then we update the latter
(lines 8--9). Otherwise, we increment the deletion cost (line 10) and `delete'
the next token (line 13).}
\label{fig:dyndist:dyndist}
\end{adjustbox}
\end{figure}

The dynamic part of \textsf{mf\_dist} is shown in
Figure~\ref{fig:dyndist:dyndist}. This makes use of the static table created by
\texttt{mk\_mf\_table} and also takes into account the cost of deleting user
input. The basic idea is to discover if, were a token in the user's input to
be deleted, the cost to a success configuration would be reduced. For example,
consider the input `\texttt{(1()}'. Parsing this against the grammar from Figure
\ref{fig:stategraphtable} causes an error in state 4 when before the
`\texttt{(}' token; after applying \textsc{MF Reduce} we end up in state 6.
Looking up the next token `\texttt{(}' in the distance table returns a cost of
2. However, if we were to delete that symbol (at a cost of 1), the distance to
the subsequent token `\texttt{)}' is 0. Thus the combination of the deletion
cost (1) and the cost to reach the second token from the current state (0) is
less than the cost to reach the first token (2). \textsf{mf\_dist} must thus
return a cost of 1 in this case in order that the heuristic remains admissible.

The remaining subtlety of \textsf{mf\_dist}
comes from the fact that we have to ensure that we don't accidentally
defer success configurations. While configurations that reach an accept state
have a distance of 0 in the statetable, we have to deal with the other success
route, which is when a repair sequence ends with $N_\textit{shifts}$ shifts.
Because of this \textsf{mf\_dist} has to take the repair sequence of
the new configuration being created: if it ends with $N_\textit{shifts}$ shifts,
then \textsf{mf\_dist} returns a distance of 0. This ensures that the new
configuration will be checked for success at the current cost, and is not
accidentally deferred until later in the todo list.


\section{Experiment}
\label{experiment}

In order to understand the performance of \cpctplus and \mf we conducted a large
experiment on real-world Java code. In this section we outline our methodology
(Section~\ref{methodology}) and results (Section~\ref{results}). Our experiment
is fully repeatable and downloadable from \laurie{XXX}.


\subsection{Methodolgy}
\label{methodology}

In order to evaluate \cpctplus and \mf, we need a concrete implementation. We
implemented a new Yacc-compatible parsing library \emph{lrpar} (including
associated libraries for LR table generation and so on, consisting of just under
10KLoC) in Rust. Although intended as a production library, it has accidentally
played a part as a flexible test bed for experimenting with, and understanding,
error recovery algorithms. We added a simple front-end which produces the output
seen in e.g.~Figure~\ref{fig:javaerror}. We use lrpar for all experiments in
this paper.

A standard problem when evaluating error recovery algorithms has
been obtaining sufficient example programs to test them on. Most papers we are
aware of use around 200 inputs (e.g.~\cite{corchuelo02repairing}) or even a
single input with minor variants (\citet{kimyi10astar}). \citet{cerecke03phd}
was the first to use a large-scale test suite (approximately 60,000 Java
source files). Fortunately, the situation has been made rather easier for us
through the existence of the Blackbox project~\cite{brown14blackbox}. This
is an opt-in data collection facility for the BlueJ editor, which records
major editing events (e.g.~compiling a file) and sends them to a central
repository. Crucially, one can see the source code associated with each event.
What makes Blackbox most appealing as a
data source is its scale and diversity: it has hundreds of thousands of users, and a
correspondingly huge collection of source code.

We first obtained a Java 1.5 Yacc grammar and modified it to support Java
1.7.\footnote{Unfortunately, changes to the method calling syntax in Java 1.8
mean that it is an awkward, though not impossible, fit for an LR(1) formalism
such as Yacc, requiring substantial changes to the current Java Yacc grammar. We
consider the work involved beyond that useful for this paper.} We then
randomly selected source files from Blackbox's database (following the lead of
\citet{santos18syntax}, we selected data from Blackbox's beginning until the end
of 2017-12-31) which resulted from explicit compilation events. In other
words, we selected inputs which a user had thought worthwhile enough to try
compiling. We then ran such inputs through our Java 1.7 lexer. We immediately
rejected files which didn't lex, since such files cannot be considered
for parsing.\footnote{Happily, this also excludes outputs which can't
possibly be Java source code. Some odd things are pasted into text
editors.} We then parsed candidate files with our Java grammar and rejected any
which did parse successfully, since there is little point running an error
recovery algorithm on correct input. We collected \corpussize source files
(collectively a total of \corpussizemb{}MiB). Although we cannot distribute the
source files directly (Blackbox, quite reasonably, requires each person with
access to the source files to register with them), we do distribute the
identifiers necessary to extract the source files from Blackbox in our
repeatable experiment.

In order to test hypothesis H1 we ran \cpctplus and \mf against our
corpus, collecting for each file: the time spent in recovery (in seconds); whether it
succeeded in repairing all errors (true or false); and the number of errors
found. Note that there are two ways of failing to repair all errors in a file:
exceeding the timeout; or running out of plausible candidate repair sequences.
The latter is rare, but does occur on occasion. Unlike
\citet{corchuelo02repairing} lrpar does not implement traditional `panic mode'
as a backup recovery mechanism: in our experience, if \cpctplus or \mf cannot
repair an error, then using panic mode simply causes cascading errors.

In order to test hypothesis H2, we created a variant of \mf called \mfrev
which reverses the ranking order and collected the same data as for \cpctplus
and \mf. Instead of selecting from the minimal cost repair sequences which allow
parsing to continue furthest, \mfrev selects from those which allow parsing to
continue the least far. This models the worse case for other members of the
\citet{fischer79locally} family which non-deterministically select a single minimal
cost repair sequence.

All experiments were run on an otherwise unloaded Intel Xeon E3-1240 v6 with 32GiB RAM running
Debian 9. We used Rust nightly-2018-03-25 to compile lrpar (the full
\texttt{Cargo.lock} file necessary to reproduce the build is included in our
experimental repository).


\subsection{Results}
\label{results}

\begin{figure}[t]
\begin{tabular}{lcccc}
\toprule
  & Mean time (s) & Median time (s) & Failure rate (\%) & Error locations (\#) \\
\midrule
\input{table.tex}
\bottomrule
\end{tabular}
\caption{Summary statistics from running our error recovery algorithms over
a corpus of \corpussize Java files. Mean and median times
report how long was spent in error recovery per file: both figures
include files which exceeded the recovery timeout, so they represent the `real'
times that users would experience, whether or not all errors are repaired or
not. The failure rate is the percentage of files which could not be fully
repaired within the timeout. The number of error locations shows how many separate points in files
are reported as errors. Since both \cpctplus and \mf are non-deterministic, we
expect some variation in this figure. However \mfrev (which always selects
from the minimal cost repair sequences that allow parsing to continue the least far)
suffers significantly from the cascading error problem, reporting
\mfreverrorlocsratioovermf more error locations than \mf to users.}
\label{fig:results:summary}
\end{figure}

\begin{figure}[t]
\includegraphics[scale=.7]{mf_histogram.pdf}
\caption{A histogram of the time spent in error recovery by \mf for files in our
corpus. The $x$ axis shows time (up to the timeout of 0.5s) and the $y$ axis is
a logarithmic scale for the number of files. As this clearly shows, most files
are repaired extremely quickly. There is then a continual decrease
until the timeout.}
\label{fig:results:mf_histogram}
\end{figure}

Figure~\ref{fig:results:summary} shows a summary of the results of our
experiment. The overall conclusions are fairly clear: both \cpctplus and \mf are
able to repair nearly all input files within the 0.5s timeout. The fact
that the median recovery time is two orders of magnitude lower than the mean
recovery time suggests that only a small number of outliers cause error recovery to
take long enough to be perceptible to humans; this is confirmed by the
histogram in Figure~\ref{fig:results:mf_histogram}. \mf's failure rate is only
\mfcpctplusfailurerateratio that of \cpctplus's, though in absolute terms
both are already extremely low. \mf also has noticeably better median and (more
importantly) mean repair times, though, again, in absolute terms both are
already fairly low. These results strongly validate Hypothesis H1.

\begin{figure}[t]
\includegraphics[scale=.7]{mf_mfrev_error_locs_histogram.pdf}
\caption{A histogram of the number of error locations for \mf and \mfrev. The
$x$ axis shows the number of error locations and the $y$ axis is a logarithmic
scale for the number of files. In essence, the entire distribution is skewed
slightly rightwards by \mfrev: many files have one or two more errors found in
them. Perhaps surprisingly, there are only a handful of outliers.}
\label{fig:results:mf_mfrev_histogram}
\end{figure}

\cpctplus and \mf rank the complete set of minimum cost repair sequences by how
far each allows parsing to continue and choose from those which allow parsing to
continue furthest. \mfrev, in contrast, selects from those which allow parsing
to continue the least far. \mfrev shows that the ranking technique used in \mf substantially
reduces the potential for cascading errors: \mfrev leads to
\mfreverrorlocsratioovermf more error locations being reported to users than
with \mf. As Figure~\ref{fig:results:mf_mfrev_histogram} shows, most files
have one or two more error locations with \mfrev: in other words, \mfrev
makes error recovery in a lot of files slightly worse (rather than making error
recovery in a small number of files a lot worse). This strongly validates
Hypothesis H2.


\section{Threats to validity}

The most obvious threat to validity is our results comparing \cpctplus and \mf.
They are specific to our implementation context and it is possible that their
relative positions could change if implemented in a different fashion.
Nevertheless, the absolute performance numbers for both are already good, and
better implementations will only improve our view of both algorithms.

A less obvious problem is that, even after repair sequence ranking, \cpctplus
and \mf are still non-deterministic. This is because, in general, multiple
repair sequences may have identical effects up to $N_\textit{try}$ tokens, but
cause different effects after that value. Thus our results inevitably vary a
little from run to run. In future versions of this paper we will produce
confidence intervals from multiple runs of the experiment, but we lacked
the significant time (approximately 15 days) necessary to run them.

Blackbox contains an astonishingly large amount of source code but has two
inherent limitations. First, it only contains Java source code. This means that
our main experiment is limited to one grammar: it is possible that our
techniques do not generalise beyond the Java grammar (though previous
work in the \citet{fischer79locally} family suggests that different grammars
make relatively little difference~\cite[p.~109]{cerecke03phd}). However, we are not aware
of an equivalent repository for other language's source code. One solution is
to mutate correct source files (e.g.~randomly deleting tokens), thus
obtaining incorrect inputs which we can later test: however, it is difficult
to uncover and then emulate the numerous, sometimes surprising, ways that
humans make syntax errors, particularly as some are language specific
(though there is some early work in this area~\cite{dejonge12automated}).
Second, Blackbox's data comes largely from students,
who are more likely than average to be somewhat novice programmers. It is likely
that novice programmers make some different syntax errors -- or, at least, make
the same syntax errors more often -- relative to advanced programmers. It is
thus possible that a corpus consisting of programs from advanced programmers
would lead to different results.

Our corpus was parsed using a Java 1.7 grammar, but some members of the corpus
were almost certainly written using Java 1.8 features. Many -- though not all -- Java 1.8
features require a new keyword: such candidate source files would thus have
failed our initial lexing test and not been included in our corpus. However,
some Java 1.8 files will have made it through our checks. Arguably these are still a valid
test of our error recovery algorithms. It is even likely that they may be a little
more challenging on average, since they are likely to be further away from being valid
syntax than files intended for Java 1.7.


\section{Related work}

Error recovery techniques are so numerous that there is no
definitive reference or overview of them. However, \citet{degano95comparison}
contains an overall historical analysis and \citet{cerecke03phd} an excellent
overview of many of the approaches which build on \citet{fischer79locally}. Both
must be supplemented with more recent works, such as those we have cited in this
paper.

The biggest limitation of error recovery algorithms in the
\citet{fischer79locally} family is that they are local: they find repairs at the
point that an error is discovered, which may be later in the file than the cause
of the error. Thus even when they successfully recover from an error, the repair
sequence reported may be very different from the fix the user considers
appropriate (note that this is distinct from the cascading error problem,
which our ranking of repair sequences in Section~\ref{rankingrepairs} partly
addresses). Perhaps the most common -- and without doubt the most frustrating
-- example of this is missing a \texttt{\}} character within the method of a
Java-like languages. \citet{santos18syntax} use
machine learning to train a system on syntactically correct programs: when a
syntax error is encountered, they use their model to suggest appropriate global
fixes. Although they also use data from Blackbox, their
experimental methodology is very different: they are stricter, in that they aim
to find exactly the same type of repair as the human user actually
applied themselves; but also looser, in that they only
consider errors which can be fixed by a single token (discarding 42\% of
the data \cite[p.~8]{santos18syntax}) whereas we attempt to fix errors which
span multiple tokens. It is thus difficult to directly compare their results to
ours. However, by the high bar they have set themselves, they are able to repair
52\% of single-token errors (i.e.~about 30\% of all possible errors; for
reference, we repair \mfsuccessrate). It seems likely that future machine
learning approaches will improve upon this figure, although the size of the
problem space suggests that it will be hard to get close to 100\%{}. It seems plausible
that a `perfect' system will mix both deterministic approaches (such as ours, which
has a high chance of finding a good-enough recovery) with
probabilistic approaches (which have a moderate chance of finding a perfect
recovery). There may be several shades of grey, leading to a system
with multiple error recovery sub-approaches (in similar fashion to \citet{deJonge12natural}).

Although one of our paper's aims is to find the complete set of minimum cost repair sequences,
it is unclear how best to present them to users, leading to questions such as:
should they be simplified? should a subset be presented? and so on. Although
rare, there are some surprising edge cases. For example,
the (incorrect) Java expression `\texttt{x = f(""a""b);}' leads to 23,067 minimum
cost repair sequences being found, due to the large number of Java keywords that are
valid in several parts of this expression leading to a combinatorial explosion: even the most
diligent user is unlikely to find such a volume of information valuable. There is a
body of work which has tried to understand how best to structure compiler error
messages (normally in the context of those learning to program). However, the
results are hard to interpret: some studies find that more complex error
messages are not useful \cite{nienaltowski08error}, while others suggest they
are \cite{prather17novices}. It is unclear to us what the right approach might be,
or how it could be applied in our context.

The approach of \citet{mckenzie95error} is similar to
\citet{corchuelo02repairing}, although the former cannot incorporate shift
repairs. It tries harder than \cpctplus to prune out pointless search
configurations~\cite[p.~12]{mckenzie95error}, such as cycles in the parsing stack,
although this leads to some minimum cost repairs being
skipped~\cite{bertsch99failure}. A number of interlocking, sophisticated pruning
mechanisms which build on this are described in~\citet{cerecke03phd}. These are
significantly more complex than our merging of compatible configurations; since this
gives us acceptable performance in practise, we have not investigated other
pruning mechanisms.

\cpctplus and \mf take only the grammar and user's input into account. However,
it is possible to take further information, such as nesting (e.g.~taking into
account curly brackets) and indentation, when undertaking error recovery. The
advantage of such information is that it can reduce the search space by making
educated guesses about the user's true intentions. The most sophisticated
approach in this vein we are aware of is that of \citet{deJonge12natural} which
uses factors such as regionality and indentation to improve the quality of error
recovery. Although judging performance across time is difficult, it appears to
be somewhat slower than \mf, failing to recover around 20\% of files within 1s.
It would be interesting to combine these with \mf.

A very different approach is that taken by \cite{pottier16reachability}: rather
than try and recover from errors directly, it reports precisely and accurately
how the user's input caused the parser got into an error state (e.g.~`I was
parsing an expression but I would have expected a close bracket here'), and
possible routes out of the error (e.g.~`A function or variable declaration is
valid here'). This involves significant manual work, though the approach has
various techniques to lessen the problem of maintaining messages as a grammar
involves. This approach seems complementary to ours: in an ideal world it would
be possible to give precise, human-orientated, messages whilst also showing
repair sequences that allowed parsing to continue. One challenge may be to make
the top ranked repair sequences match the manually written messages.

While the programming language world has largely forgotten the approach of
\citet{aho72minimum}, there are a number of successor works, most recently that
of~\citet{rajasekaran16error}. These improve on the time complexity, though none
that we are aware of address the issue of how to present what has been done to
the user.

We are not aware of any error recovery algorithms that are formally verified.
Indeed, as shown in this paper, several have serious flaws. We are only aware of
two works which have begun to consider what correctness for such algorithms might mean:
\citet{zaytsev14formal} provides a brief philosophical justification of the need
and \citet{gomezrodriguez10error} provides an outlines of an approach. Until
such time as someone verifies a full error recovery algorithm, it is difficult
to estimate the effort involved, or what issues may be uncovered.


\section{Conclusions}

In this paper we have shown that it is possible to do a better job of error
recovery than is currently the case, and that this can be done fast enough to be
practical in existing compilers. In particular, creating the complete set of
minimal cost repair sequences significantly reduces the cascading error problem.
We believe that both \cpctplus and \mf give good enough results to be usable in
practise. If time is scarce, we suggest implementing \cpctplus, since it avoids
having to calculate and store distance tables. However, the extra effort
involved in implementing \mf -- roughly 250LoC more than \cpctplus -- is small
enough to justify the performance improvement. In particular, users who require
the best performance -- e.g.~when error recovery is used in an interactive
environment -- are likely to prefer \mf over \cpctplus.

\begin{acks}
We are particularly grateful to the Blackbox developers for allowing us access
to their data, and particularly to Neil Brown for help in extracting a relevant
corpus. We also thank Edd Barrett for helping to set up our benchmarking
machine.
\end{acks}

\bibliography{bib}

\end{document}
