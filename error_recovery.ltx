\documentclass[acmsmall,small,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{listings}
\usepackage{booktabs} % For formal tables
\usepackage{softdev}
\usepackage{subcaption}
\usepackage{xspace}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

\lstset{
    basicstyle=\tt\scriptsize,
    xleftmargin=0pt,
    framexleftmargin=1.5em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

\setcopyright{none}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}

% DOI
%\acmDOI{0000001.0000001}

% Paper history
%\received{February 2007}

\newcommand{\corchueloplus}{\textit{CPCT}$^+$\xspace}
\newcommand{\mf}{\textit{MF}\xspace}
\include{experimentstats}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title[Fast Multiple Repair Error Recovery]{Fast Multiple Repair Error Recovery}

\author{Lukas Diekmann}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}}
\author{Laurence Tratt}
\orcid{0000-0002-5258-3805}
\affiliation{%
  \department{Software Development Team}
  \institution{King's College London}
  \country{United Kingdom}
}
\thanks{Authors' URLs: %
    L.~Diekmann~\url{http://diekmann.co.uk/},
    L.~Tratt~\url{http://tratt.net/laurie/}.
}


\begin{abstract}
Syntax errors are generally easy to fix for humans, but not for compilers: the
latter often fail to find an effective recovery, leading to a cascading chain of
errors that drown out the original. More advanced recovery techniques
suffer much less from this problem but have seen little practical use because
their typical performance was seen as poor and their worse case unbounded.
In this paper we show that not only can an advanced approach run in acceptable time
(which we define as spending a maximum of 0.5s in error recovery per file), but
that we can increase the complexity of the problem to finding \emph{all} minimal
cost recovery options, and still do so within acceptable time. We first extend
an existing algorithm, before introducing a new, faster, alternative \mf. We
validate our algorithms with a corpus of \corpussize real-world
syntactically invalid of Java programs: \mf is able to find recovery options for
\mfsuccessrate of these in acceptable time.
\end{abstract}

\keywords{Parsing, error recovery, programming languages}

\maketitle

\section{Introduction}

Programming is a humbling job, which involves acknowledging that we will make
untold errors in our quest to perfect a program. Most troubling are semantic
errors, where we intended the program to do one thing, but it does another. Less
troubling, but often no less irritating, are syntax errors, which are
(generally minor) deviances from the exacting syntax required by a compiler.
So common are syntax errors that modern compilers expect us to make several
in a single input. Rather than stop on the first syntax error encountered, they attempt
to \emph{recover} from it. This allows them to report, and us to fix, all our
syntax errors in one go.

When error recovery works well, it is a useful productivity gain. Unfortunately,
widely used approaches are ad-hoc and simplistic, with
two weaknesses limiting their usefulness: only limited consideration is
given to the context of the error; and the only recoveries attempted are the
skipping of input until a pre-determined synchronisation
token is reached~\cite[p.~3]{degano95comparison} or the insertion of a
single synchronisation token. Inappropriate recoveries tend to cause a
cascade of spurious syntax errors (see
Figure~\ref{fig:javaerror} for an example): programmers quickly learn that
only the position of the first error -- not its repair, nor the position of
subsequent errors -- can really be relied upon to be accurate.

\begin{figure}[t]
\begin{minipage}{0.48\textwidth}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\caption{}
\label{lst:javaerror:input}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
class C {
  int x y;
}
\end{lstlisting}
\end{minipage}
\\
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{1}
\caption{}
\label{lst:javaerror:mf}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
Error at line 2 col 9. Repairs found:
  Delete "y"
  Insert "COMMA"
  Insert "EQ"
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\vspace{-20.5pt}
\begin{tabular}{p{0.02\textwidth}p{0.45\textwidth}}
\begin{subfigure}{0.02\textwidth}
\addtocounter{subfigure}{-2}
\caption{}
\label{lst:javaerror:javac}
\end{subfigure}
&
\begin{minipage}[t]{0.45\textwidth}
\vspace{-7pt}
\begin{lstlisting}
C.java:2: error: ';' expected
  int x y;
       ^
C.java:2: error: <identifier> expected
  int x y;
         ^
\end{lstlisting}
\end{minipage}
\end{tabular}
\end{minipage}
%&
%\begin{minipage}[t]{0.47\textwidth}
%\vspace{-17pt}
\vspace{-10pt}
\caption{An example of a simple, common Java syntax error
(\subref{lst:javaerror:input}) and the problems traditional error recovery has in
dealing with it. \texttt{javac} (\subref{lst:javaerror:javac}) spots the error
when it encounters `\texttt{y}'. Its error recovery heuristic then
repairs the input by inserting a semicolon before `\texttt{y}' (i.e.~making
the input equivalent to `\texttt{int x; y;}'). This immediately leads to a spurious second parse error,
since `\texttt{y}' on its own is not a valid statement. The \corchueloplus and \mf
recovery algorithms (both produce the output shown in \subref{lst:javaerror:mf}) also spot the
error when it encounters `\texttt{y}', and then use the
Java grammar to find minimal cost repair sequences. These two algorithms find
and report to the user three separate repair sequences: one can delete `\texttt{y}'
entirely (`\texttt{int x;}'), or insert a comma
(`\texttt{int x, y;}'), or insert an equals sign (\texttt{`int x = y;'}). In
this case, all three repair sequences have the same rank, and are thus presented
in an arbitrary order. However, it is important to note that the first repair
sequence is then used to repair the input for subsequent parsing. Presenting all
three minimal cost repair sequences to the user gives a much greater chance that one
matches their original intention.}
\label{fig:javaerror}
%\end{minipage}
%\end{tabular}
\end{figure}

Most of us are so used to this state of affairs that
we assume it to be inevitable. However, there are more advanced algorithms which,
as well as being able to deal with any LR grammar, take into account the full context of the error, and have
several ways of recovering from errors. Probably the earliest such algorithm
is \citet{aho72minimum}, which, upon encountering an error, creates on-the-fly an
alternative (possibly ambiguous) grammar which allows the parser to recover.
The implementation complexity of this algorithm, and the challenge of explaining
to users what has been done, probably explain why it has fallen out of
favour in programming language circles. A simpler family of algorithms, which
trace their roots to \citet{fischer79locally}, instead try to find a single minimal cost
\emph{repair sequence} of token insertions and deletions which allow the parser to
recover. Algorithms in this family are good at recovering from errors and are
easily adapted to give human-friendly feedback. However, they
have seen little practical use because their typical
performance was seen as poor and their worse case unbounded \cite[p.~14]{mckenzie95error}.

In this paper we test the following hypothesis:

\begin{description}
  \item[H1] Error recovery algorithms from the \citet{fischer79locally} family
    can repair nearly all errors in acceptable time.
\end{description}

We (somewhat arbitrarily) define `acceptable time' as 0.5s for error recovery
per file (i.e.~all errors in a file must be
recovered from within this time) since we think that even the most demanding user
tolerate that delay in order to get better quality results. We
strongly validate this hypothesis. To a
certain extent, our access to faster hardware made it more likely that we
can validate this hypothesis than past researchers. However, while existing
algorithms in the \citet{fischer79locally} family aim to find a single
minimal cost recovery, we then made, and test, a second hypothesis:

\begin{description}
  \item[H2] Nearly all minimal cost repair sequences can be found in acceptable time.
\end{description}

We also strongly validate this hypothesis. In other words, we are able to tackle a
harder version of the problem than any previous algorithm, and do so in
acceptable time. This not only leads to higher quality, and more programmer
friendly, error messages (see Figure~\ref{fig:javaerror} for an example), but
allows us to reduce the cascading error problem still further.

We first use one of the more recent algorithms in this family -- that of
\citet{corchuelo02repairing} -- as a base, slightly correcting and extending it
to form an algorithm \corchueloplus (Section~\ref{corchueloplus}). We
then show that an even newer algorithm which promises better performance -- that
of \citet{kimyi10astar} -- has several weaknesses which cause it to miss many
minimal cost repair sequences (Section~\ref{kimyi}). However, we are able to use
this as partial inspiration for an entirely new error recovery algorithm \mf
(Section~\ref{mf}) which is approximately \mfratioovercorchuelo faster than
\corchueloplus. We aim for both algorithms to be as simple as possible, so that
they are realistic targets for tool builders: \corchueloplus is somewhat
simpler than \mf, though the latter is still less than 1,000 lines of Rust code.

We then validate \corchueloplus and \mf on a corpus of \corpussize real,
syntactically incorrect, Java programs (Section~\ref{experiment}) -- approximately
15x bigger than the biggest previous experiment \cite[p.~84]{cerecke03phd}. Within the
time budget of 0.5s: \corchueloplus is able to find repairs for \corchuelosuccessrate
of files within this time, with a mean recovery time of \corchuelomeantime; and
\mf is able to find repairs for \mfsuccessrate of files within this time, with
a mean recovery time of \mfmeantime. We believe that this shows that such
approaches are ready for wider usage.


\section{Background}

We assume a high-level understanding of the mechanics of parsing in this paper,
but in this section we provide a handful of definitions, and a brief refresher
of relevant low-level details, needed to understand the rest of this paper. Although
there are many flavours of parsing, we exclusively consider LR
parsing~\cite{knuth65lllr}, which is the basis of all algorithms in the
\citet{fischer79locally} family. As well as describing the largest practical set
of unambiguous grammars, LR parsing remains one of the most widely used parsing
approaches, largely thanks to the ubiquity of Yacc~\cite{johnson75yacc} and its
descendants. In acknowledgement of that, we use Yacc syntax throughout this
paper, so that examples can easily be tested in Yacc-compatible parsing tools.

Yacc-like tools take in a Context-Free Grammar (CFG) and produce a parser from
it. The CFG has one or more \emph{rules}; each rule has a name and one or more
\emph{productions} (often called `alternatives'); each production contains one
or more \emph{symbols}; and a symbol is either a \emph{terminal} (i.e.~a token
type such as `INT`) or a \emph{nonterminal} (i.e.~a reference to another rule in
the grammar). One rule is designated the \emph{start rule}. The resulting parser
takes as input a stream of tokens, each of which has a \emph{type}
(e.g.~\texttt{INT}) and a value (e.g.~\texttt{123}).\footnote{In practise, the
system we outline requires a \emph{lexer} which splits string inputs up into
tokens. In the interests of brevity, we assume the existence of a tool such as
Lex which does performs this task.} Strictly speaking, parsing is the act of
determining whether a stream of tokens is correct with respect to the underlying
grammar. Since this is rarely useful on its own, Yacc-like tools allow grammars
to specify `semantic actions' which are executed when a grammar rule is
successfully matched. In this paper, we assume that the semantic actions build
up a \emph{parse tree}, ordering the tokens into a tree of nonterminals (nodes
which can have children) and terminals (nodes which cannot have children)
relative to the underlying grammar. This makes it easy to visualise the result
of parsing and error recovery (indeed, the Rust parser we built for this paper
automatically prints parse trees as its default output).

The beauty of LR parsing stems from the way a parser is constructed. The CFG is
first transformed into a \emph{stategraph}, which is a fairly traditional
description of a statemachine. Since even on a modern machine, a canonical
(i.e.~unmerged) stategraph for a real-world grammar takes several seconds to
build, and a surprising amount of memory to store, we use the state merging
algorithm of \citet{pager77practical} to merge together compatible states. The
effect of this is significant, reducing the Java grammar we use later from
\laurie{8000ish} to \laurie{900ish} states. The stategraph is then transformed into a
\emph{statetable}, which allows the run-time parser to operate efficiently.
\section{\corchueloplus}
\label{corchueloplus}


\section{\cite{kimyi10astar}}
\label{kimyi}


\section{\mf}
\label{mf}


\section{Experiment}
\label{experiment}

\subsection{Methodolgy}

\subsection{Results}
\section{Related work}

\cite{corchuelo02repairing}

\cite{deJonge12natural}

\cite{pottier16reachability}

\cite{gomezrodriguez10error}


\section{Future work}

Our current parsing tool uses the merging algorithm from
\citet{pager77practical}, but this can over-merge states when conflict
resolution is used \cite[p.~3]{denny10ielr}. Since our error recovery approach
is intended to be independent of the merging approach, it should be possible to
use the more sophisticated state merging approach of \cite{denny10ielr} without
problems.

\bibliography{bib}

\end{document}
